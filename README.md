<div align="center">
  <h1>InnoEval: An Automated Framework for Research Idea Evaluation</h1>
</div>
<p align="center">
  <a href="https://github.com/zjunlp/InnoEval">
  	<img src="https://awesome.re/badge.svg" alt="Awesome">
  </a>
  <a href="https://github.com/zjunlp/InnoEval/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT">
  </a>
  <img src="https://img.shields.io/github/last-commit/zjunlp/InnoEval?color=blue" alt="Last Commit">
  <img src="https://img.shields.io/badge/PRs-Welcome-red" alt="PRs Welcome">
</p>


<h5 align="center"> If you like our project, please give us a star on GitHub for the latest updates!</h5>

---

![method](./asset/method.png)

**InnoEval** is an automated evaluation framework designed for assessing research ideas and innovation proposals. It leverages multi-agent systems and LLMs to comprehensively evaluate the novelty, feasibility, and significance of research contributions.

* **Multi-Agent Pipeline**
  <br> A chain of specialized agents (Extraction, Research, Grounding, Evaluation, Report) working together

* **Multi-Source Grounding**
  <br> Gathers evidence from web pages, code repositories, and academic papers to validate claims

* **Persona-Based Evaluation**
  <br> Simulates multiple reviewer perspectives for balanced and comprehensive assessment

* **Flexible Input Modes**
  <br> Supports both PDF URLs and direct text input for research ideas

* **Batch Processing**
  <br> Point-wise and group-wise evaluation for large-scale dataset analysis


## Table of Contents

* [ğŸ“¥ Installation](#ğŸ“¥-installation)
* [ğŸ¬ Quick Start](#ğŸ¬-quick-start)
* [ğŸ“‚ Architecture](#ğŸ“‚-architecture)
* [ğŸ”¬ Examples](#ğŸ”¬-examples)
* [ğŸ›  Configuration](#ğŸ› -configuration)
* [ğŸ“„ Acknowledgement](#ğŸ“„-acknowledgement)
* [âœï¸ Citation](#âœï¸-citation)



## ğŸ“¥ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/InnoEval.git
cd InnoEval
```

### 2. Create Virtual Environment

```bash
conda create -n innoeval python=3.10 -y
conda activate innoeval
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure API Keys

Copy the example configuration file and fill in your API keys:

```bash
cd config/
cp LLM.env.example LLM.env
# Edit LLM.env with your API keys
```

Required API keys:
| Key | Description |
|-----|-------------|
| `DS_API_KEY` | DeepSeek API key (primary LLM) |
| `DS_API_BASE_URL` | DeepSeek API base URL |
| `OPENAI_API_KEY` | OpenAI API key (optional) |
| `GOOGLE_API_KEY` | Google Search API key |
| `SERPER_API_KEY` | Serper API key for web search |
| `JINA_API_KEY` | Jina API key for content extraction |
| `S2_API_KEY` | Semantic Scholar API key |
| `GH_TOKEN` | GitHub token for repository analysis |



## ğŸ¬ Quick Start

### 1. Single Idea Evaluation

Run the complete pipeline for a single research idea:

```bash
cd InnoEval
python3 -m innoeval.pipeline.single_idea_pipeline
```

This executes the full 6-step pipeline:
1. **ExtractionAgent**: Extract structured idea from PDF/text
2. **ResearchAgent**: Search for related works (web, code, papers)
3. **Report Extraction**: Build evidence reports from search results
4. **GroundingAgent**: Map claims to supporting evidence
5. **EvaluationAgent**: Multi-perspective quality assessment
6. **ReportAgent**: Generate final evaluation report

### 2. Point-wise Dataset Evaluation

Evaluate an entire dataset of research papers:

```bash
python3 -m innoeval.pipeline.batch_pipeline
```

Results are saved to `cache/dataset_conference_points/`.

### 3. Group Dataset Evaluation

Process papers organized in groups:

```bash
python3 -m innoeval.pipeline.group_pipeline
```

Results are saved to `cache/dataset_conference_groups/`.

### 4. Group/Pair Evaluation

Run comparison evaluation on cached group results:

```bash
# Group-wise comparison and ranking
python3 -m innoeval.pipeline.group_evaluation

# Pair-wise comparison
python3 -m innoeval.pipeline.pair_evaluation
```

These scripts read from `cache/dataset_conference_groups/` and do not re-run the pipeline.



## ğŸ“‚ Architecture

### Directory Structure

```
InnoEval/
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ LLM.env                 # API keys (not tracked)
â”‚   â”œâ”€â”€ LLM.env.example         # Example configuration
â”‚   â””â”€â”€ kaggle.json             # Kaggle API config
â”œâ”€â”€ dataset/                    # Evaluation datasets
â”‚   â”œâ”€â”€ conference_points.jsonl # Point-wise dataset
â”‚   â”œâ”€â”€ conference_groups.json  # Group-wise dataset
â”‚   â””â”€â”€ conference_pairs_*.json # Pair datasets
â”œâ”€â”€ cache/                      # Pipeline results cache
â”‚   â””â”€â”€ reviewer_personas.json  # Reviewer personas
â””â”€â”€ innoeval/                   # Main package
    â”œâ”€â”€ mas/                    # Multi-Agent System
    â”‚   â”œâ”€â”€ agents/             # Agent implementations
    â”‚   â”‚   â”œâ”€â”€ extraction_agent.py
    â”‚   â”‚   â”œâ”€â”€ research_agent.py
    â”‚   â”‚   â”œâ”€â”€ grounding_agent.py
    â”‚   â”‚   â”œâ”€â”€ evaluation_agent.py
    â”‚   â”‚   â””â”€â”€ report_agent.py
    â”‚   â”œâ”€â”€ models/             # LLM and model interfaces
    â”‚   â”‚   â”œâ”€â”€ model_factory.py
    â”‚   â”‚   â””â”€â”€ bge_singleton.py
    â”‚   â””â”€â”€ tools/              # Utility tools
    â”‚       â”œâ”€â”€ searchers/      # Web/code/paper search
    â”‚       â”œâ”€â”€ querygen/       # Query generation
    â”‚       â”œâ”€â”€ enricher/       # Content enrichment
    â”‚       â”œâ”€â”€ grobid_refs/    # Reference extraction
    â”‚       â””â”€â”€ repo_analysis/  # GitHub repo analysis
    â””â”€â”€ pipeline/               # Pipeline implementations
        â”œâ”€â”€ single_idea_pipeline.py
        â”œâ”€â”€ batch_pipeline.py
        â”œâ”€â”€ group_pipeline.py
        â”œâ”€â”€ group_evaluation.py
        â””â”€â”€ pair_evaluation.py
```

### Pipeline Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: PDF URL â”‚â”€â”€â”€â–¶â”‚ ExtractionAgent â”‚â”€â”€â”€â–¶â”‚   Idea Object   â”‚
â”‚  or Text Input  â”‚    â”‚   (Extract)     â”‚    â”‚  (structured)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Pages     â”‚    â”‚  ResearchAgent  â”‚â”€â”€â”€â–¶â”‚  SearchResults  â”‚
â”‚   Code Repos    â”‚â—€â”€â”€â”€â”‚    (Search)     â”‚    â”‚   (enriched)    â”‚
â”‚   Papers        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Claims Map    â”‚â—€â”€â”€â”€â”‚ GroundingAgent  â”‚â—€â”€â”€â”€â”‚  Reports Data   â”‚
â”‚  (evidence)     â”‚    â”‚   (Grounding)   â”‚    â”‚  (extracted)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Personas      â”‚â”€â”€â”€â–¶â”‚EvaluationAgent  â”‚â”€â”€â”€â–¶â”‚ EvaluationResultâ”‚
â”‚  (reviewers)    â”‚    â”‚   (Evaluate)    â”‚    â”‚   (per-persona) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  ReportAgent    â”‚
                                              â”‚  (Synthesize)   â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  Final Report   â”‚
                                              â”‚  (Markdown)     â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evaluation Dimensions

The framework evaluates research ideas across five core dimensions:

| Dimension | Description |
|-----------|-------------|
| **Clarity** | How clearly the idea is presented and explained |
| **Novelty** | Originality and innovation compared to existing work |
| **Validity** | Soundness of methodology and theoretical foundations |
| **Feasibility** | Practical implementability with available resources |
| **Significance** | Potential impact and contribution to the field |

Custom evaluation metrics can be added through the `user_metric` parameter.



## ğŸ”¬ Examples

### Example 1: Evaluate from PDF URL

```python
import asyncio
from pathlib import Path
from innoeval.pipeline.single_idea_pipeline import SingleIdeaPipeline

async def evaluate_paper():
    pipeline = SingleIdeaPipeline(
        input_type="pdf",
        pdf_url="https://openreview.net/pdf?id=YOUR_PAPER_ID",
        cache_path=Path("cache/my_paper.json"),
        persona_path=Path("cache/reviewer_personas.json"),
        research_params={
            "title": "Your Paper Title",
            "after": "2022-01-01",
            "before": "2024-01-01",
            "depth": 3,
        },
        num_personas=5,
        get_future_paper=True,
    )
    result = await pipeline.run()
    print(result["final_report"])

asyncio.run(evaluate_paper())
```

### Example 2: Evaluate from Text

```python
import asyncio
from pathlib import Path
from innoeval.pipeline.single_idea_pipeline import SingleIdeaPipeline

async def evaluate_idea():
    idea_text = """
    This paper introduces a novel approach to automated code review
    using large language models with retrieval-augmented generation...
    """

    pipeline = SingleIdeaPipeline(
        input_type="text",
        idea_text=idea_text,
        cache_path=Path("cache/my_idea.json"),
        research_params={
            "title": "LLM-based Code Review",
            "after": "2023-01-01",
            "before": "2024-12-01",
        },
        num_personas=3,
    )
    result = await pipeline.run()
    print(result["final_decision"])

asyncio.run(evaluate_idea())
```

### Example 3: Custom Evaluation Metrics

```python
# The evaluation agent supports custom metrics
eval_params = {
    "temperature": 0.7,
    "user_metric": [
        {
            "metric": "Reproducibility",
            "description": "Evaluate whether sufficient detail is provided for reproduction"
        },
        {
            "metric": "EthicalConsiderations",
            "description": "Assess potential ethical implications and mitigation strategies"
        }
    ]
}
```

### Example 4: Batch Processing with Custom Dataset

```python
# Create a JSONL file with format:
# {"paper_id": "xxx", "title": "...", "decision": "accept"}
# Then run:
# python3 -m innoeval.pipeline.batch_pipeline

# Or programmatically:
from innoeval.pipeline.batch_pipeline import load_dataset, process_paper

items = load_dataset(Path("dataset/my_papers.jsonl"), num=10)
for item in items:
    print(f"Processing: {item.title}")
```



## ğŸ›  Configuration

### LLM Configuration

The `config/LLM.env` file controls all API settings:

```bash
# Primary LLM (DeepSeek)
DS_API_KEY=your_deepseek_key
DS_API_BASE_URL=https://api.deepseek.com/v1

# OpenAI (alternative)
OPENAI_API_KEY=your_openai_key
OPENAI_API_BASE_URL=https://api.openai.com/v1

# Search APIs
GOOGLE_API_KEY=your_google_key
SERPER_API_KEY=your_serper_key
JINA_API_KEY=your_jina_key
S2_API_KEY=your_semantic_scholar_key

# GitHub
GH_TOKEN=your_github_token

# Kaggle (optional)
KAGGLE_CONFIG_DIR=./config
```

### Model Configuration

The default model configuration in `SingleIdeaPipeline`:

```python
model_config = {
    "models": {
        "default_provider": "dsr1",
        "dsr1": {
            "model_name": "deepseek-v3.2",
            "api_key": os.getenv("DS_API_KEY"),
            "base_url": os.getenv("DS_API_BASE_URL"),
            "max_tokens": 4096,
            "temperature": 0.7,
        },
    }
}
```

### Agent Parameters

| Agent | Key Parameters |
|-------|----------------|
| **ExtractionAgent** | `extract_temperature: 0.3` |
| **ResearchAgent** | `top_k: 10`, `max_results_per_query: 5`, `web_max_results: 5`, `github_max_results: 5` |
| **GroundingAgent** | `extract_temperature: 0.0` |
| **EvaluationAgent** | `temperature: 0.7`, `num_personas: 5` |
| **ReportAgent** | `temperature: 0.4` |

### Research Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `title` | str | Paper title for search optimization |
| `after` | str | Search papers after this date (YYYY-MM-DD) |
| `before` | str | Search papers before this date (YYYY-MM-DD) |
| `depth` | int | Search depth (1-5) |
| `web_temperature` | float | Temperature for web search queries |
| `code_temperature` | float | Temperature for code search queries |

### Cache Structure

Pipeline results are cached in JSON format:

```json
{
  "extraction_result": {...},
  "search_results_dict": {...},
  "reports_data": {...},
  "grounding_result": {...},
  "evaluation_result": {...},
  "final_report": "...",
  "final_decision": "accept/reject",
  "total_time": 123.45,
  "total_token": 50000
}
```



## ğŸ“„ Acknowledgement

This project builds upon and draws inspiration from the following open-source projects:

### InternAgent
We thank the [InternAgent](https://github.com/InternScience/InternAgent) project for providing foundational multi-agent architecture patterns and evaluation methodologies that influenced our pipeline design.

### RepoMaster
We thank [RepoMaster](https://github.com/QuantaAlpha/RepoMaster) for the repository analysis toolkit that enables comprehensive code repository evaluation in our grounding process.

---



## âœï¸ Citation

If you find our work helpful, please use the following citations.

```

```

### License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
