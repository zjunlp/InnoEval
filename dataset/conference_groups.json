[
  {
    "query": {
      "paper_id": "WhEPg4mUs6",
      "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "abstract": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses.  While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions.  We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.",
      "keywords": "['Analog AI; in-memory computing; stochastic gradient descent; stochastic optimization']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "WhEPg4mUs6",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "FQbkBcpcvA",
          "title": "Rethinking cross entropy for continual fine-tuning: policy gradient with entropy annealing",
          "abstract": "While large pretrained vision models have achieved widespread success, their post-training adaptation in continual learning remains vulnerable to catastrophic forgetting. We challenge the conventional use of cross-entropy (CE) loss, a surrogate for 0-1 loss, by reformulating classification through reinforcement learning. Our approach frames classification as a one-step Markov Decision Process (MDP), where input samples serve as states, class labels as actions, and a fully observable reward model is derived from ground-truth labels.  From this formulation, we derive Expected Policy Gradient (EPG), a gradient-based method that directly minimizes the 0-1 loss (i.e., misclassification error). Theoretical and empirical analyses reveal a critical distinction between EPG and CE: while CE encourages exploration via high-entropy outputs, EPG adopts an exploitation-centric approach, prioritizing high-confidence samples through implicit sample weighting. Building on this insight, we propose an adaptive entropy annealing strategy (aEPG) that transitions from exploratory to exploitative learning during continual adaptation of a pre-trained model. Our method outperforms CE-based optimization across diverse benchmarks (Split-ImageNet-R, Split-Food101, Split-CUB100, CLRS) and parameter-efficient modules (LoRA, Adapter, Prefix). More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models. These findings suggest that excessive exploration may disrupt pretrained knowledge and establish exploitative learning as a crucial principle for adapting foundation vision models to evolving classification tasks.",
          "keywords": [
            "Continual learning",
            "reinforcement learning",
            "cross-entropy",
            "class-incremental learning"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "We propose Expected Policy Gradient (EPG), a reinforcement learning method that directly optimizes classification accuracy (0-1 loss)  outperforming cross-entropy for continual fine-tuning pretrained vision models.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=FQbkBcpcvA",
          "pdf_link": "https://openreview.net/pdf?id=FQbkBcpcvA"
        },
        "paper_internal_id": "FQbkBcpcvA",
        "category": "reject",
        "embedding_score": 0.6969096064567566,
        "final_score": 0.023993918672204018
      },
      "poster": {
        "paper": {
          "id": "tDG6bY48ch",
          "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation",
          "abstract": "Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
          "keywords": [
            "large vision-language models",
            "KV cache compression"
          ],
          "primary_area": "deep_learning",
          "TLDR": "A kv cache compression method for large vision-language models",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=tDG6bY48ch",
          "pdf_link": "https://openreview.net/pdf?id=tDG6bY48ch"
        },
        "paper_internal_id": "tDG6bY48ch",
        "category": "poster",
        "embedding_score": 0.7056117057800293,
        "final_score": 0.1495734602212906
      },
      "spotlight": {
        "paper": {
          "id": "TDFSKAspoQ",
          "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
          "abstract": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP.",
          "keywords": [
            "Stochastic Optimization; Convergence Analysis; Adaptive Optimizers ; Nonconvex Optimization; Large Language Models; Momentum-Gradient Alignment"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=TDFSKAspoQ",
          "pdf_link": "https://openreview.net/pdf?id=TDFSKAspoQ"
        },
        "paper_internal_id": "TDFSKAspoQ",
        "category": "spotlight",
        "embedding_score": 0.6981012225151062,
        "final_score": 0.038930926471948624
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "rMhQBlhh4c",
      "title": "Adjoint Schrödinger Bridge Sampler",
      "abstract": "Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers",
      "keywords": "['Boltzmann distribution', 'diffusion sampler', 'Schrödinger bridge']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "rMhQBlhh4c",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "LPUr2CexmX",
          "title": "DO-EM: Density Operator Expectation Maximization",
          "abstract": "Density operators, quantum generalizations of probability distributions, are gaining prominence in machine learning due to their foundational role in quantum computing. Generative modeling based on density operator models (**DOMs**) is an emerging field, but existing training algorithms - such as those for the Quantum Boltzmann Machine - do not scale to real-world data, such as the MNIST dataset. The Expectation-Maximization algorithm has played a fundamental role in enabling scalable training of probabilistic latent variable models on real-world datasets. *In this paper, we develop an Expectation-Maximization framework to learn latent variable models defined through **DOMs** on classical hardware, with resources comparable to those used for probabilistic models, while scaling to real-world data.* However, designing such an algorithm is nontrivial due to the absence of a well-defined quantum analogue to conditional probability, which complicates the Expectation step. To overcome this, we reformulate the Expectation step as a quantum information projection (QIP) problem and show that the Petz Recovery Map provides a solution under sufficient conditions. Using this formulation, we introduce the Density Operator Expectation Maximization (DO-EM) algorithm - an iterative Minorant-Maximization procedure that optimizes a quantum evidence lower bound. We show that the **DO-EM** algorithm ensures non-decreasing log-likelihood across iterations for a broad class of models. Finally, we present Quantum Interleaved Deep Boltzmann Machines (**QiDBMs**), a **DOM** that can be trained with the same resources as a DBM. When trained with **DO-EM** under Contrastive Divergence, a **QiDBM** outperforms larger classical DBMs in image generation on the MNIST dataset, achieving a 40–60% reduction in the Fréchet Inception Distance.",
          "keywords": [
            "Density Operators",
            "Expectation-Maximization",
            "Quantum Unsupervised Learning",
            "Latent Variable Models"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "Training density operator latent variable models using DO-EM outperforms corresponding classical latent variable models.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=LPUr2CexmX",
          "pdf_link": "https://openreview.net/pdf?id=LPUr2CexmX"
        },
        "paper_internal_id": "LPUr2CexmX",
        "category": "reject",
        "embedding_score": 0.6960470676422119,
        "final_score": 0.39379411935806274
      },
      "poster": {
        "paper": {
          "id": "WFujqJ5UBV",
          "title": "Path Gradients after Flow Matching",
          "abstract": "Boltzmann Generators have emerged as a promising machine learning tool for generating samples from equilibrium distributions of molecular systems using Normalizing Flows and importance weighting.\nRecently, Flow Matching has helped speed up Continuous Normalizing Flows (CNFs), scale them to more complex molecular systems, and minimize the length of the flow integration trajectories.\n We investigate the benefits of using path gradients to fine-tune CNFs  initially trained by Flow Matching, in the setting where a target energy is known. \n Our experiments show that this hybrid approach yields up to a threefold increase in sampling efficiency for molecular systems,\n all while using the same model, a similar computational budget and without the need for additional sampling.\n Furthermore, by measuring the length of the flow trajectories \n during fine-tuning, we show that path gradients largely preserve the learned structure of the flow.",
          "keywords": [
            "Path Gradients",
            "Boltzmann Generators",
            "Continuous Normalizing Flows",
            "Equivariance",
            "Optimal Transport",
            "Flow Matching",
            "Variational Inference",
            "Molecular Dynamics"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "We investigate the benefits of using path gradients to fine-tune CNFs initially trained by Flow Matching, in the setting where a target energy is known.",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=WFujqJ5UBV",
          "pdf_link": "https://openreview.net/pdf?id=WFujqJ5UBV"
        },
        "paper_internal_id": "WFujqJ5UBV",
        "category": "poster",
        "embedding_score": 0.7600991725921631,
        "final_score": 0.8304428458213806
      },
      "spotlight": {
        "paper": {
          "id": "U87XyMPrZp",
          "title": "Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time",
          "abstract": "The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or conformations. Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models---whether trained for static structure prediction or conformational generation---to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.",
          "keywords": [
            "protein conformations",
            "biomolecular systems",
            "enhanced sampling",
            "diffusion",
            "importance sampling",
            "protein flexibility"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "ConforMix, a new enhanced sampling method applied to biomolecular diffusion models, improves conformational sampling",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=U87XyMPrZp",
          "pdf_link": "https://openreview.net/pdf?id=U87XyMPrZp"
        },
        "paper_internal_id": "U87XyMPrZp",
        "category": "spotlight",
        "embedding_score": 0.741614580154419,
        "final_score": 0.8163195252418518
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "XoN10bZtR9",
      "title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "abstract": "In domain adaption (DA), joint maximum mean discrepancy (JMMD), as a famous distribution-distance metric, aims to measure joint probability distribution difference between the source domain and target domain, while it is still not fully explored and especially hard to be applied into a subspace-learning framework as its empirical estimation involves a tensor-product operator whose partial derivative is difficult to obtain. To solve this issue, we deduce a concise JMMD based on the Representer theorem that avoids the tensor-product operator and obtains two essential findings. First, we reveal the uniformity of JMMD by proving that previous marginal, class conditional, and weighted class conditional probability distribution distances are three special cases of JMMD with different label reproducing kernels. Second, inspired by graph embedding, we observe that the similarity weights, which strengthen the intra-class compactness in the graph of Hilbert Schmidt independence criterion (HSIC), take opposite signs in the graph of JMMD, revealing why JMMD degrades the feature discrimination. This motivates us to propose a novel loss JMMD-HSIC by jointly considering JMMD and HSIC to promote discrimination of JMMD. Extensive experiments on several cross-domain datasets could demonstrate the validity of our revealed theoretical results and the effectiveness of our proposed JMMD-HSIC.",
      "keywords": "['domain adaptation', 'JMMD', 'HSIC', 'feature discrimination', 'graph embedding']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "XoN10bZtR9",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "np5NmBQL4F",
          "title": "Isometry pursuit",
          "abstract": "Isometry pursuit is a convex algorithm for identifying orthonormal column-submatrices of wide matrices.\nIt consists of a vector normalization followed by multitask basis pursuit.\nApplied to Jacobians of putative coordinate functions, it helps identify locally isometric embeddings from within interpretable dictionaries.\nWe provide theoretical and experimental results justifying this method, including a proof with realistic assumptions that such isometric submatrices, should they exist, are contained within the obtained support.\nFor problems involving coordinate selection and diversification, it offers a synergistic alternative to greedy and brute force search.",
          "keywords": [
            "Manifold learning",
            "interpretability",
            "sparse coding"
          ],
          "primary_area": "optimization",
          "TLDR": "Isometry pursuit is a convex algorithm for identifying orthonormal column-submatrices of wide matrices.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=np5NmBQL4F",
          "pdf_link": "https://openreview.net/pdf?id=np5NmBQL4F"
        },
        "paper_internal_id": "np5NmBQL4F",
        "category": "reject",
        "embedding_score": 0.6892173290252686,
        "final_score": 0.040615301579236984
      },
      "poster": {
        "paper": {
          "id": "e2WesV6Voe",
          "title": "Sequence Modeling with Spectral Mean Flows",
          "abstract": "A key question in sequence modeling with neural networks is how to represent and learn highly nonlinear and probabilistic state dynamics. Operator theory views such dynamics as linear maps on Hilbert spaces containing mean embedding vectors of distributions, offering an appealing but currently overlooked perspective. We propose a new approach to sequence modeling based on an operator-theoretic view of a hidden Markov model (HMM). Instead of materializing stochastic recurrence, we embed the full sequence distribution as a tensor in the product Hilbert space. A generative process is then defined as maximum mean discrepancy (MMD) gradient flow in the space of sequences. To overcome challenges with large tensors and slow sampling convergence, we introduce spectral mean flows, a novel tractable algorithm integrating two core concepts. First, we propose a new neural architecture by leveraging spectral decomposition of linear operators to derive a scalable tensor network decomposition of sequence mean embeddings. Second, we extend MMD gradient flows to time-dependent Hilbert spaces and connect them to flow matching via the continuity equation, enabling simulation-free learning and faster sampling. We demonstrate competitive results on a range of time-series modeling datasets.",
          "keywords": [
            "sequence modeling",
            "time series",
            "hidden Markov models",
            "mean embeddings",
            "linear operators",
            "maximum mean discrepancy",
            "gradient flows"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose a new algorithm for deep generative modeling of sequence data in continuous spaces based on a novel adaptation of operator theory for probabilistic dynamical systems.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=e2WesV6Voe",
          "pdf_link": "https://openreview.net/pdf?id=e2WesV6Voe"
        },
        "paper_internal_id": "e2WesV6Voe",
        "category": "poster",
        "embedding_score": 0.7335132956504822,
        "final_score": 0.6567044258117676
      },
      "spotlight": {
        "paper": {
          "id": "z2SGaPIhLT",
          "title": "SGCD: Stain-Guided CycleDiffusion for Unsupervised Domain Adaptation of Histopathology Image Classification",
          "abstract": "The effectiveness of domain translation in addressing image-based problems of Unsupervised Domain Adaptation (UDA) depends on the quality of the translated images and the preservation of crucial discriminative features. However, achieving high-quality and stable translations typically requires paired data, which poses a challenge in scenarios with limited annotations in the target domain. To address this issue, this paper proposes a novel method termed Stain-Guided Cycle Diffusion (SGCD), employing a dual diffusion model with bidirectional generative constraints to synthesize highly realistic data for downstream task fine-tuning. The bidirectional generative constraints ensure that the translated images retain the features critical to the downstream model in properly controlling the generation process. Additionally, a stain-guided consistency loss is introduced to enhance the denoising capability of the dual diffusion model, thereby improving the quality of images translated between different domains using latents from one domain and a diffusion model trained on another. Experiments conducted on four public datasets demonstrate that SGCD can effectively enhance the performance of downstream task models on the target domain.",
          "keywords": [
            "Transfer Learning",
            "Domain Adaptation",
            "Generative Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=z2SGaPIhLT",
          "pdf_link": "https://openreview.net/pdf?id=z2SGaPIhLT"
        },
        "paper_internal_id": "z2SGaPIhLT",
        "category": "spotlight",
        "embedding_score": 0.6948760151863098,
        "final_score": 0.36703407764434814
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "BSZqpqgqM0",
      "title": "Why Diffusion Models Don’t Memorize:  The Role of Implicit Dynamical Regularization in Training",
      "abstract": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size $n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times.\nThese findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic  datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.",
      "keywords": "['Diffusion Models', 'Deep Learning', 'Probabilistic Methods']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "BSZqpqgqM0",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "9NHd6Z4aIi",
          "title": "Single-Step Diffusion via Direct Models",
          "abstract": "We introduce Direct Models, a generative modeling framework that enables single-step diffusion by learning a direct mapping from initial noise $x_0$ to all intermediate latent states along the generative trajectory. Unlike traditional diffusion models that rely on iterative denoising or integration, Direct Models leverages a progressive learning scheme where the mapping from $x_0$ to $x_{t + \\delta t}$ is composed as an update from $x_0$ to $x_t$ plus the velocity at time $t$. This formulation allows the model to learn the entire trajectory in a recursive, data-consistent manner while maintaining computational efficiency. At inference, the full generative path can be obtained in a single forward pass. Experimentally, we show that Direct Models achieves state-of-the-art sample quality among single-step diffusion methods while significantly reducing inference time.",
          "keywords": [
            "Efficient generative models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=9NHd6Z4aIi",
          "pdf_link": "https://openreview.net/pdf?id=9NHd6Z4aIi"
        },
        "paper_internal_id": "9NHd6Z4aIi",
        "category": "reject",
        "embedding_score": 0.7721430063247681,
        "final_score": 0.17225846648216248
      },
      "poster": {
        "paper": {
          "id": "X8C20fJXtx",
          "title": "Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory",
          "abstract": "Diffusion models have achieved impressive success in high-fidelity image generation but suffer from slow sampling due to their inherently iterative denoising process. While recent one-step methods accelerate inference by learning direct noise-to-image mappings, they sacrifice the interpretability and fine-grained control intrinsic to diffusion dynamics, key advantages that enable applications like editable generation. To resolve this dichotomy, we introduce **Hierarchical Koopman Diffusion**, a novel framework that achieves both one-step sampling and interpretable generative trajectories.  Grounded in Koopman operator theory, our method lifts the nonlinear diffusion dynamics into a latent space where evolution is governed by globally linear operators, enabling closed-form trajectory solutions. This formulation not only eliminates iterative sampling but also provides full access to intermediate states, allowing manual intervention during generation. To model the multi-scale nature of images, we design a hierarchical architecture that disentangles generative dynamics across spatial resolutions via scale-specific Koopman subspaces, capturing coarse-to-fine details systematically. We empirically show that the Hierarchical Koopman Diffusion not only achieves competitive one-step generation performance but also provides a principled mechanism for interpreting and manipulating the generative process through spectral analysis. Our framework bridges the gap between fast sampling and interpretability in diffusion models, paving the way for explainable image synthesis in generative modeling.",
          "keywords": [
            "One-step Generation",
            "Diffusion Models",
            "Koopman Operators",
            "Interpretable Image Synthesis"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose explicit and interpretable one-step generation framework that retains the advantages of traditional diffusion models, such as access to intermediate states and fine-grained control, while enabling fast sampling.",
          "creation_date": "2025-04-21",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=X8C20fJXtx",
          "pdf_link": "https://openreview.net/pdf?id=X8C20fJXtx"
        },
        "paper_internal_id": "X8C20fJXtx",
        "category": "poster",
        "embedding_score": 0.7624284029006958,
        "final_score": 0.9992595314979553
      },
      "spotlight": {
        "paper": {
          "id": "f6AYwCvynr",
          "title": "Neural Entropy",
          "abstract": "We explore the connection between deep learning and information theory through the paradigm of diffusion models. A diffusion model converts noise into structured data by reinstating, imperfectly, information that is erased when data was diffused to noise. This information is stored in a neural network during training. We quantify this information by introducing a measure called \\textit{neural entropy}, which is related to the total entropy produced by diffusion. Neural entropy is a function of not just the data distribution, but also the diffusive process itself. Measurements of neural entropy on a few simple image diffusion models reveal that they are extremely efficient at compressing large ensembles of structured data.",
          "keywords": [
            "Diffusion models",
            "thermodynamics",
            "information theory",
            "statistical mechanics"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce a measure of the information content stored in a neural network in the context of diffusion models.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=f6AYwCvynr",
          "pdf_link": "https://openreview.net/pdf?id=f6AYwCvynr"
        },
        "paper_internal_id": "f6AYwCvynr",
        "category": "spotlight",
        "embedding_score": 0.7537661790847778,
        "final_score": 0.7099247574806213
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "SDhOClkyqC",
      "title": "An Analytical Theory of Spectral Bias in the Learning Dynamics of Diffusion Models",
      "abstract": "We develop an analytical framework for understanding how the learned distribution evolves during diffusion model training. \nLeveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one or two layer linear or linear convolutional denoiser settings with arbitrary data, where linear networks converge along principal components, and convolutional networks converge along Fourier modes. \nRemarkably, these solutions allow us to derive the generated distribution in closed-form and its KL-divergence through training. \nThese analytical results expose a pronounced \\emph{spectral bias}, i.e. for both weights and generated distributions, the convergence time of a mode follows an inverse power law of its variance.  \nEmpirical experiments on both Gaussian and natural image datasets demonstrate that the power-law spectral bias—remain robust even when using deeper or convolutional architectures. \nOur results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations of why earlier stopping could lead to incorrect details in image generative model.",
      "keywords": "['Spectral bias', 'analytical theory', 'learning dynamics', 'diffusion', 'flow matching', 'deep linear network']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "SDhOClkyqC",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "QJtanJS4T9",
          "title": "Irreducible Loss Floors in Gradient Descent Convergence and Energy Footprint",
          "abstract": "Despite their central role, convergence analyses of the dynamics of loss functions\nduring training require strong assumptions (e.g convexity and smoothness) which\nare non-trivial to prove. In this work, we introduce a framework for deriving\nnecessary convergence conditions that hold without restrictive assumptions on\nthe dataset or the model architecture. By linking microscopic properties such as\nindividual sample losses and their gradient to macroscopic training dynamics, we\nderive tight lower bounds for loss functions, applicable to both full-batch and mini-\nbatch gradient systems. These bounds reveal the presence of irreducible floors\nthat optimizers cannot surpass and beyond theoretical guarantees, this framework offers a practical tool for anticipating convergence speed, and estimating\nminimum training time and energy requirements. Thus, this framework can be\nused to ensure the sustainability and feasibility of large-scale training regimes.",
          "keywords": [
            "gradient descent",
            "convergence",
            "loss bounds",
            "optimization",
            "training dynamics",
            "sustainability",
            "efficiency",
            "feasibility",
            "computational cost",
            "irreducible loss",
            "non-convex optimization",
            "lower bounds"
          ],
          "primary_area": "theory",
          "TLDR": "We derive theoretical lower bounds for loss functions, revealing convergence limits without relying on standard convexity assumptions.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=QJtanJS4T9",
          "pdf_link": "https://openreview.net/pdf?id=QJtanJS4T9"
        },
        "paper_internal_id": "QJtanJS4T9",
        "category": "reject",
        "embedding_score": 0.728600800037384,
        "final_score": 0.49978622794151306
      },
      "poster": {
        "paper": {
          "id": "cIGfKdfy3N",
          "title": "Learning Diffusion Models with Flexible Representation Guidance",
          "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA.",
          "keywords": [
            "Diffusion Models",
            "Representation Learning",
            "Image Generation",
            "Biomolecule Generation",
            "Theoretical Analysis of Generative Model"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce a systematic approach to flexibly incorporating representation guidance into diffusion models, resulting in both accelerated training and better performance across image, protein, and molecule generation tasks.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=cIGfKdfy3N",
          "pdf_link": "https://openreview.net/pdf?id=cIGfKdfy3N"
        },
        "paper_internal_id": "cIGfKdfy3N",
        "category": "poster",
        "embedding_score": 0.7544918656349182,
        "final_score": 0.8875080347061157
      },
      "oral": {
        "paper": {
          "id": "BSZqpqgqM0",
          "title": "Why Diffusion Models Don’t Memorize:  The Role of Implicit Dynamical Regularization in Training",
          "abstract": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size $n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times.\nThese findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic  datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.",
          "keywords": [
            "Diffusion Models",
            "Deep Learning",
            "Probabilistic Methods"
          ],
          "primary_area": "deep_learning",
          "TLDR": "Implicit dynamical regularization during training gives diffusion models a generalization window that widens with the training set size, so stopping within this window prevents memorization.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=BSZqpqgqM0",
          "pdf_link": "https://openreview.net/pdf?id=BSZqpqgqM0"
        },
        "paper_internal_id": "BSZqpqgqM0",
        "category": "oral",
        "embedding_score": 0.779463529586792,
        "final_score": 0.8291453719139099
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "QwXpn5IPKk",
      "title": "RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation",
      "abstract": "While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one.\n\nInstead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for high-resolution (HR) image generation; however, existing methods often result in poor image quality and long inference time.\n\nWe introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel parameter-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency.\n\nExtensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications.\nCodes: https://github.com/kmittle/RepLDM.",
      "keywords": "['Diffusion Model', 'High-Resolution Image', 'Attention Guidance', 'progressive pixel space upsampling']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "QwXpn5IPKk",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "9NHd6Z4aIi",
          "title": "Single-Step Diffusion via Direct Models",
          "abstract": "We introduce Direct Models, a generative modeling framework that enables single-step diffusion by learning a direct mapping from initial noise $x_0$ to all intermediate latent states along the generative trajectory. Unlike traditional diffusion models that rely on iterative denoising or integration, Direct Models leverages a progressive learning scheme where the mapping from $x_0$ to $x_{t + \\delta t}$ is composed as an update from $x_0$ to $x_t$ plus the velocity at time $t$. This formulation allows the model to learn the entire trajectory in a recursive, data-consistent manner while maintaining computational efficiency. At inference, the full generative path can be obtained in a single forward pass. Experimentally, we show that Direct Models achieves state-of-the-art sample quality among single-step diffusion methods while significantly reducing inference time.",
          "keywords": [
            "Efficient generative models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=9NHd6Z4aIi",
          "pdf_link": "https://openreview.net/pdf?id=9NHd6Z4aIi"
        },
        "paper_internal_id": "9NHd6Z4aIi",
        "category": "reject",
        "embedding_score": 0.7275325059890747,
        "final_score": 0.836446225643158
      },
      "poster": {
        "paper": {
          "id": "hlpRj222RG",
          "title": "PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement",
          "abstract": "Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel‐level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.",
          "keywords": [
            "image editing",
            "image refinement",
            "diffusion model"
          ],
          "primary_area": "applications",
          "TLDR": "We introduce PixPerfect, a general pixel refinement framework that removes seams, color shifts, and texture artifacts from latent diffusion outputs, achieving seamless, high-quality image inpainting and local editing.",
          "creation_date": "2025-05-04",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=hlpRj222RG",
          "pdf_link": "https://openreview.net/pdf?id=hlpRj222RG"
        },
        "paper_internal_id": "hlpRj222RG",
        "category": "poster",
        "embedding_score": 0.7677091956138611,
        "final_score": 0.9889006614685059
      },
      "oral": {
        "paper": {
          "id": "BSZqpqgqM0",
          "title": "Why Diffusion Models Don’t Memorize:  The Role of Implicit Dynamical Regularization in Training",
          "abstract": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size $n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times.\nThese findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic  datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.",
          "keywords": [
            "Diffusion Models",
            "Deep Learning",
            "Probabilistic Methods"
          ],
          "primary_area": "deep_learning",
          "TLDR": "Implicit dynamical regularization during training gives diffusion models a generalization window that widens with the training set size, so stopping within this window prevents memorization.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=BSZqpqgqM0",
          "pdf_link": "https://openreview.net/pdf?id=BSZqpqgqM0"
        },
        "paper_internal_id": "BSZqpqgqM0",
        "category": "oral",
        "embedding_score": 0.7335197925567627,
        "final_score": 0.7592641711235046
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "WrYWolqKh3",
      "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations",
      "abstract": "Modern tokenizers employ deterministic algorithms to map text into a single ``canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the language model vocabulary, including tokenizing by character. In this paper, we investigate the robustness of LMs to input encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4\\% of their original performance when given a randomly sampled tokenization, and 90.8\\% with character-level tokenization.  We find that overall stronger models tend to be more robust, and that robustness diminishes as the tokenization departs farther from the canonical form.  Motivated by these results, we identify settings where non-canonical tokenization schemes can \\textit{improve} performance, finding that character‑level segmentation improves string manipulation and code understanding tasks by up to 15\\%, and right‑aligned digit grouping enhances large‑number arithmetic by over 33\\%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We provide evidence that both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings). However, base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less committed to their tokenizer than previously believed, and highlight the promise of intervening on tokenization at inference time to boost language model performance.",
      "keywords": "['tokenization', 'language models', 'robustness']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "WrYWolqKh3",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "eR8raBLZW7",
          "title": "BriLLM: Brain-inspired Large Language Model",
          "abstract": "This paper reports the brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of \"least resistance\" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM versions in Chinese and English, with 4000 tokens, 32-dimensional node size, 32-token sequence prediction ability, model sizes around 2B and 1B respectively, bringing language model prediction performance comparable to GPT-1.",
          "keywords": [
            "LLM"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=eR8raBLZW7",
          "pdf_link": "https://openreview.net/pdf?id=eR8raBLZW7"
        },
        "paper_internal_id": "eR8raBLZW7",
        "category": "reject",
        "embedding_score": 0.7692906856536865,
        "final_score": 0.05832792446017265
      },
      "poster": {
        "paper": {
          "id": "stpe7UeETz",
          "title": "Corrector Sampling in Language Models",
          "abstract": "Autoregressive language models accumulate errors due to their fixed, irrevocable left-to-right token generation. To address this, we propose a new sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error accumulation by iteratively revisiting and potentially replacing tokens in a window of previously generated text. Fine-tuning a pretrained 8B parameter model with RPT for only 100B resulted in ~10% relative improvements on reasoning and coding benchmarks compared to the standard sampling.",
          "keywords": [
            "Language modeling",
            "Sampling"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce Resample-Previous-Tokens (RPT), a sampling method that allows models to revisit and replace previously generated tokens, leading to ~10% relative improvements in reasoning and coding after a short fine-tuning.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=stpe7UeETz",
          "pdf_link": "https://openreview.net/pdf?id=stpe7UeETz"
        },
        "paper_internal_id": "stpe7UeETz",
        "category": "poster",
        "embedding_score": 0.7579489946365356,
        "final_score": 0.8554340600967407
      },
      "oral": {
        "paper": {
          "id": "NM8Apk61NA",
          "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
          "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Code is available at \\url{https://github.com/godlin-sjtu/HyperET}.",
          "keywords": [
            "Efficient Training",
            "Multi-modal Large Language Models",
            "Granularity Levels",
            "Hyperbolic Space"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-18",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=NM8Apk61NA",
          "pdf_link": "https://openreview.net/pdf?id=NM8Apk61NA"
        },
        "paper_internal_id": "NM8Apk61NA",
        "category": "oral",
        "embedding_score": 0.7496964335441589,
        "final_score": 0.060394495725631714
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "hTbimOuFPM",
      "title": "An efficient implementation for solving the all pairs minimax path problem in an undirected dense graph",
      "abstract": "We provide an efficient $ O(n^2) $ implementation for solving the all pairs minimax path problem or  widest path problem in an undirected dense graph. The distance matrix is also called the all points path distance (APPD). We conducted experiments to test the implementation and algorithm, compared it with several other algorithms for solving the APPD matrix.  Result shows Algorithm 4 works good for solving the widest path or minimax path APPD matrix.  It can drastically improve the efficiency for computing the APPD matrix.  There are several theoretical outcomes which claim the APPD matrix can be solved accurately in $ O(n^2) $ . However, they are impractical because there is no code implementation of these algorithms. Algorithm 4 is the first algorithm that has an actual code implementation for solving the APPD matrix of minimax path or widest path problem in $ O(n^2) $, in an undirected dense graph.",
      "keywords": "['Minimax path problem', 'Longest-leg path distance', 'Min-Max-Jump distance', 'Widest path problem', 'Maximum capacity path problem', 'Bottleneck edge query problem', 'All points path distance', 'Floyd-Warshall algorithm', 'Minimum spanning tree']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "hTbimOuFPM",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "drZzzGUlbG",
          "title": "Quasi-Self-Concordant Optimization with $\\ell_{\\infty}$ Lewis Weights",
          "abstract": "In this paper, we study the problem $\\min_{x\\in R^{d},Nx=v}\\sum\\_{i=1}^{n}f((Ax-b)_{i})$\nfor a quasi-self-concordant function $f: R\\to R$, where $A,N$ are\n$n\\times d$ and $m\\times d$ matrices, $b,v$ are vectors of length\n$n$ and $m$ with $n\\ge d.$ We show an algorithm based on a trust-region\nmethod with an oracle  that can be implemented using $\\widetilde{O}(d^{1/3})$\nlinear system solves, improving the $\\widetilde{O}(n^{1/3})$ oracle\nby [Adil-Bullins-Sachdeva, NeurIPS 2021]. Our implementation of\nthe oracle relies on solving the overdetermined $\\ell\\_{\\infty}$-regression\nproblem $\\min\\_{x\\in R^{d},Nx=v}\\|Ax-b\\|\\_{\\infty}$. We provide an\nalgorithm that finds a $(1+\\epsilon)$-approximate solution to this\nproblem using $O((d^{1/3}/\\epsilon+1/\\epsilon^{2})\\log(n/\\epsilon))$\nlinear system solves. This algorithm leverages $\\ell\\_{\\infty}$ Lewis\nweight overestimates and achieves this iteration complexity via a\nsimple lightweight IRLS approach, inspired by the work of [Ene-Vladu,\nICML 2019]. Experimentally, we demonstrate that our algorithm significantly\nimproves the runtime of the standard CVX solver.",
          "keywords": [
            "Convex optimization",
            "Quasi-Self-Concordant Optimization"
          ],
          "primary_area": "optimization",
          "TLDR": "We give a practical algorithm for optimizing over-parametrized quasi-self-concordant functions to high precision via  $\\ell_{\\infty}$ Lewis weights",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=drZzzGUlbG",
          "pdf_link": "https://openreview.net/pdf?id=drZzzGUlbG"
        },
        "paper_internal_id": "drZzzGUlbG",
        "category": "poster",
        "embedding_score": 0.6972919702529907,
        "final_score": 0.9888297915458679
      },
      "spotlight": {
        "paper": {
          "id": "idjZKbf78s",
          "title": "Product Distribution Learning with Imperfect Advice",
          "abstract": "Given i.i.d.~samples from an unknown distribution $P$, the goal of distribution learning is to recover the parameters of a distribution that is close to $P$. When $P$ belongs to the class of product distributions on the Boolean hypercube $\\{0,1\\}^d$, it is known that $\\Omega(d/\\epsilon^2)$ samples are necessary to learn $P$ within total variation (TV) distance $\\epsilon$. We revisit this problem when the learner is also given as advice the parameters of a product distribution $Q$. We show that there is an efficient algorithm to learn $P$ within TV distance $\\epsilon$ that has sample complexity $\\tilde{O}(d^{1-\\eta}/\\epsilon^2)$, if $\\|\\mathbf{p} - \\mathbf{q}\\|_1<\\epsilon d^{0.5 - \\Omega(\\eta)}$. Here, $\\mathbf{p}$ and $\\mathbf{q}$ are the mean vectors of $P$ and $Q$ respectively, and no bound on $\\|\\mathbf{p} - \\mathbf{q}\\|_1$ is known to the algorithm a priori.",
          "keywords": [
            "distribution learning",
            "statistics",
            "algorithms",
            "sample complexity"
          ],
          "primary_area": "theory",
          "TLDR": "Product distributions on n dimensions can be learned with sublinear samples if a sufficiently close distribution is provided as advice.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=idjZKbf78s",
          "pdf_link": "https://openreview.net/pdf?id=idjZKbf78s"
        },
        "paper_internal_id": "idjZKbf78s",
        "category": "spotlight",
        "embedding_score": 0.6479485034942627,
        "final_score": 0.9340095520019531
      },
      "oral": {
        "paper": {
          "id": "8P3QNSckMp",
          "title": "A Clean Slate for Offline Reinforcement Learning",
          "abstract": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral.",
          "keywords": [
            "Offline Reinforcement Learning",
            "Evaluation",
            "Open-Source"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "We propose a principled taxonomy, evaluation procedure, and unified algorithm space for offline RL.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=8P3QNSckMp",
          "pdf_link": "https://openreview.net/pdf?id=8P3QNSckMp"
        },
        "paper_internal_id": "8P3QNSckMp",
        "category": "oral",
        "embedding_score": 0.6512719988822937,
        "final_score": 0.8090198636054993
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "2Ri68h7bD1",
      "title": "Dale's Law Meets Geometric Brownian Motion: Multiplicative Updates for Sampling",
      "abstract": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Proceeding further, we propose a new formalism for multiplicative denoising score-matching, which subsumes the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.",
      "keywords": "[\"Dale's Law\", 'Geometric Brownian Motion', 'Stochastic Differential Equations', 'Score-matching']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "2Ri68h7bD1",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "5YMZfufpfY",
          "title": "Improving Energy Natural Gradient Descent through Woodbury, Momentum, and Randomization",
          "abstract": "Natural gradient methods significantly accelerate the training of Physics-Informed Neural Networks (PINNs), but are often prohibitively costly. We introduce a suite of techniques to improve the accuracy and efficiency of energy natural gradient descent (ENGD) for PINNs. First, we leverage the Woodbury formula to dramatically reduce the computational complexity of ENGD. Second, we adapt the Subsampled Projected-Increment Natural Gradient Descent algorithm from the variational Monte Carlo literature to accelerate the convergence. Third, we explore the use of randomized algorithms to further reduce the computational cost in the case of large batch sizes. We find that randomization accelerates progress in the early stages of training for low-dimensional problems, and we identify key barriers to attaining acceleration in other scenarios. Our numerical experiments demonstrate that our methods outperform previous approaches, achieving the same $L^2$ error as the original ENGD up to $75\\times$ faster.",
          "keywords": [
            "Physics-Informed Neural Networks",
            "Natural Gradient Descent",
            "Woodbury Matrix Identity",
            "Momentum",
            "Nyström Approximation"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "We introduce Woodbury's matrix identity, momentum-like SPRING and randomization to make energy natural gradient descent 75 times faster for PINNs.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=5YMZfufpfY",
          "pdf_link": "https://openreview.net/pdf?id=5YMZfufpfY"
        },
        "paper_internal_id": "5YMZfufpfY",
        "category": "poster",
        "embedding_score": 0.7475711703300476,
        "final_score": 0.8974791169166565
      },
      "spotlight": {
        "paper": {
          "id": "D6aCr4RRdt",
          "title": "Any-stepsize Gradient Descent for Separable Data under Fenchel–Young Losses",
          "abstract": "The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \\emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\\Omega(\\epsilon^{-1/2})$, and the R{\\'e}nyi entropy achieves the far better rate $T=\\Omega(\\epsilon^{-1/3})$. We argue that these better rate is possible because of \\emph{separation margin} of loss functions, instead of the self-bounding property.",
          "keywords": [
            "gradient descent",
            "loss function",
            "implicit bias"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2025-05-06",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=D6aCr4RRdt",
          "pdf_link": "https://openreview.net/pdf?id=D6aCr4RRdt"
        },
        "paper_internal_id": "D6aCr4RRdt",
        "category": "spotlight",
        "embedding_score": 0.7678166627883911,
        "final_score": 0.9083248376846313
      },
      "oral": {
        "paper": {
          "id": "uWj4s7rMnR",
          "title": "Mean Flows for One-step Generative Modeling",
          "abstract": "We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \\textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256$\\times$256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.",
          "keywords": [
            "Generative Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-04-06",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=uWj4s7rMnR",
          "pdf_link": "https://openreview.net/pdf?id=uWj4s7rMnR"
        },
        "paper_internal_id": "uWj4s7rMnR",
        "category": "oral",
        "embedding_score": 0.7321690320968628,
        "final_score": 0.75505131483078
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "ZqwyrPXbV9",
      "title": "Concept Attractors in LLMs and their Applications",
      "abstract": "Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be generalized and explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these attractors to solve a wide range of practical tasks, including **language translation**, **hallucination reduction**, **guardrailing**, and **synthetic data generation**. Despite their simplicity, these attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.",
      "keywords": "['Large Language Models', 'Dynamic Systems', 'Attractors', 'Guardrails', 'Transpiler', 'Steering', 'Hallucinations', 'Synthetic Data']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "ZqwyrPXbV9",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "qTXlFwlggv",
          "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations",
          "abstract": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, yet at other times seem unable to recognize those strategies that govern their behavior. This suggests a limited degree of metacognition --- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognition enhances LLMs' capabilities in solving complex tasks but also raises safety concerns, as models may obfuscate their internal processes to evade neural-activation-based oversight (e.g., safety detector). Given society's increased reliance on these models, it is critical that we understand their metacognitive abilities. To address this, we introduce a neuroscience-inspired \\emph{neurofeedback} paradigm that uses in-context learning to quantify metacognitive abilities of LLMs to \\textit{report} and \\textit{control} their activation patterns. \nWe demonstrate that their abilities depend on several factors: the number of in-context examples provided, the semantic interpretability of the neural activation direction (to be reported/controlled), and the variance explained by that direction. These directions span a ``metacognitive space'' with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a small subset of their neural activations. Our paradigm provides empirical evidence to quantify metacognition in LLMs, with significant implications for AI safety (e.g., adversarial attack and defense).",
          "keywords": [
            "large language model",
            "metacognition",
            "safety",
            "neurofeedback",
            "in-context learning"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "This paper demonstrates that language models possess metacognitive-like abilities to monitor and control their internal neural activations.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=qTXlFwlggv",
          "pdf_link": "https://openreview.net/pdf?id=qTXlFwlggv"
        },
        "paper_internal_id": "qTXlFwlggv",
        "category": "poster",
        "embedding_score": 0.7720111012458801,
        "final_score": 0.9940086603164673
      },
      "spotlight": {
        "paper": {
          "id": "M0U8wUow8c",
          "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning",
          "abstract": "Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.",
          "keywords": [
            "Mechanistic interpretability",
            "circuit analysis",
            "logical reasoning",
            "propositional logic",
            "large language model",
            "transformer"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We mechanistically analyze how large language models solve synthetic propositional logic problems.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=M0U8wUow8c",
          "pdf_link": "https://openreview.net/pdf?id=M0U8wUow8c"
        },
        "paper_internal_id": "M0U8wUow8c",
        "category": "spotlight",
        "embedding_score": 0.7659810781478882,
        "final_score": 0.904895544052124
      },
      "oral": {
        "paper": {
          "id": "KnqiC0znVF",
          "title": "Large Language Diffusion Models",
          "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}.",
          "keywords": [
            "diffusion language models",
            "large language models",
            "masked diffusion models",
            "discrete diffusion models",
            "diffusion models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We present LLaDA, a diffusion  language model trained from scratch that is competitive to LLaMA 3 in performance.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=KnqiC0znVF",
          "pdf_link": "https://openreview.net/pdf?id=KnqiC0znVF"
        },
        "paper_internal_id": "KnqiC0znVF",
        "category": "oral",
        "embedding_score": 0.752939760684967,
        "final_score": 0.8304495215415955
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "yThwhNCaZN",
      "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models",
      "abstract": "Recent advances in knowledge representation learning (KRL) highlight the urgent necessity to unify symbolic knowledge graphs (KGs) with language models (LMs) for richer semantic understanding. However, existing approaches typically prioritize either graph structure or textual semantics, leaving a gap: a unified framework that simultaneously captures global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics. To bridge this gap, we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers. KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations. Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations—validating its effectiveness in unifying structural information and textual semantics.",
      "keywords": "['Knowledge Graph Embedding', 'Language Model', 'Transformer', 'Attention Mechanism']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "yThwhNCaZN",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "5h9mS87Pyt",
          "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation",
          "abstract": "Large language models are able to exploit in-context learning to access external knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain unclear. In this work, we shed light on the mechanism of in-context retrieval augmentation for question answering by viewing a prompt as a composition of informational components. We propose an attribution-based method to identify specialized attention heads, revealing in-context heads that comprehend instructions and retrieve relevant contextual information, and parametric heads that store entities' relational knowledge. To better understand their roles, we extract function vectors and modify their attention weights to show how they can influence the answer generation process. Finally, we leverage the gained insights to trace the sources of knowledge used during inference, paving the way towards more safe and transparent language models.",
          "keywords": [
            "feature attribution",
            "in-context learning",
            "function vectors",
            "retrieval-augmented language models",
            "transformers",
            "explainable AI",
            "mechanistic Interpretability"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "A method to identify specialized attention heads for in-context retrieval augmented LMs, with analyses of their roles and an application use case to trace knowledge sources.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=5h9mS87Pyt",
          "pdf_link": "https://openreview.net/pdf?id=5h9mS87Pyt"
        },
        "paper_internal_id": "5h9mS87Pyt",
        "category": "poster",
        "embedding_score": 0.7237913012504578,
        "final_score": 0.22591648995876312
      },
      "spotlight": {
        "paper": {
          "id": "OkVQJZWGfn",
          "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision",
          "abstract": "Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which augments training data with intermediate reasoning steps to provide a richer learning signal, has driven recent advances in large language model reasoning. This paper develops a statistical theory of learning under CoT supervision. Central to the theory is the *CoT information*, which measures the additional discriminative power offered by the chain-of-thought for distinguishing hypotheses with different end-to-end behaviors. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard end-to-end supervision, with both upper bounds and information-theoretic lower bounds characterized by the CoT information.",
          "keywords": [
            "chain-of-thought",
            "learning theory",
            "statistical learning theory",
            "PAC learning",
            "sample complexity"
          ],
          "primary_area": "theory",
          "TLDR": "This paper develops a statistical theory for learning under chain-of-thought supervision.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=OkVQJZWGfn",
          "pdf_link": "https://openreview.net/pdf?id=OkVQJZWGfn"
        },
        "paper_internal_id": "OkVQJZWGfn",
        "category": "spotlight",
        "embedding_score": 0.729320228099823,
        "final_score": 0.06214001774787903
      },
      "oral": {
        "paper": {
          "id": "NM8Apk61NA",
          "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
          "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Code is available at \\url{https://github.com/godlin-sjtu/HyperET}.",
          "keywords": [
            "Efficient Training",
            "Multi-modal Large Language Models",
            "Granularity Levels",
            "Hyperbolic Space"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-18",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=NM8Apk61NA",
          "pdf_link": "https://openreview.net/pdf?id=NM8Apk61NA"
        },
        "paper_internal_id": "NM8Apk61NA",
        "category": "oral",
        "embedding_score": 0.7567522525787354,
        "final_score": 0.004520601127296686
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "pv964N1RYb",
      "title": "DynaPhArM: Adaptive and Physics-Constrained Modeling for Target-Drug Complexes with Drug-Specific Adaptations",
      "abstract": "Accurately modeling the target-drug complex at atom level presents a significant challenge in the computer-aided drug design. Traditional methods that rely solely on rigid transformations often fail to capture the adaptive interactions between targets and drugs, particularly during substantial conformational changes in targets upon ligand binding, which becomes especially critical when learning target-drug interactions in drug design. Accurately modeling these changes is crucial for understanding target-drug interactions and improving drug efficacy. To address these challenges, we introduce DynaPhArM, an SE(3)-Equivariant Transformer model specifically designed to capture adaptive alterations occurring within target-drug interactions. DynaPhArM utilizes the cooperative scalar-vector representation, drug-specific embeddings, and a diffusion process to effectively model the evolving dynamics of interactions between targets and drugs. Furthermore, we integrate physical information and energetic principles that maintain essential geometric constraints, such as bond lengths, bond angles, van der Waals forces (vdW), within a multi-task learning (MTL) framework to enhance accuracy. Experimental results demonstrate that DynaPhArM achieves state-of-the-art performance with an overall root mean square deviation (RMSD) of 2.01 Å and a sc-RMSD of 0.29 Å while exhibiting higher success rates compared to existing methodologies. Additionally, DynaPhArM shows promise in enhancing drug specificity, thereby simulating how targets adapt to various drugs through precise modeling of atomic-level interactions and conformational flexibility.",
      "keywords": "['Target-drug complex dynamic modeling', 'Physics constraints', 'Denoising diffusion probabilistic model', 'Flexible docking']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "pv964N1RYb",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "8EWo0gX5TW",
          "title": "Neural SDEs as a Unified Approach to Continuous-Domain Sequence Modeling",
          "abstract": "Inspired by the ubiquitous use of differential equations to model continuous dynamics across diverse scientific and engineering domains, we propose a novel and intuitive approach to continuous sequence modeling. Our method interprets timeseries data as discrete samples from an underlying continuous dynamical system, and models its time evolution using Neural Stochastic Differential Equation (Neural SDE), where both the flow (drift) and diffusion terms are parameterized by neural networks. We derive a principled maximum likelihood objective and a simulationfree scheme for efficient training of our Neural SDE model. We demonstrate the versatility of our approach through experiments on sequence modeling tasks across both embodied and generative AI. Notably, to the best of our knowledge, this is the first work to show that SDEbased continuous-time modeling also excels in such complex scenarios, and we hope that our\nwork opens up new avenues for research of SDE models in high-dimensional and temporally intricate domains.",
          "keywords": [
            "Neural Stochastic Differential Equations",
            "Flow Matching",
            "Diffusion Models",
            "Continuous-Time Sequence Modeling."
          ],
          "primary_area": "deep_learning",
          "TLDR": "We proposed a new sequence modeling approach tailored to continuous domain, as a unified framework for embodied and generative AI tasks.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=8EWo0gX5TW",
          "pdf_link": "https://openreview.net/pdf?id=8EWo0gX5TW"
        },
        "paper_internal_id": "8EWo0gX5TW",
        "category": "reject",
        "embedding_score": 0.5994383096694946,
        "final_score": 0.8402640223503113
      },
      "spotlight": {
        "paper": {
          "id": "SvopaNxYWt",
          "title": "UMA: A Family of Universal Models for Atoms",
          "abstract": "The ability to quickly and accurately compute properties from atomic simulations is critical for advancing a large number of applications in chemistry and materials science including drug discovery, energy storage, and semiconductor manufacturing. To address this need, we present a family of Universal Models for Atoms (UMA), designed to push the frontier of speed, accuracy, and generalization. UMA models are trained on half a billion unique 3D atomic structures (the largest training runs to date) by compiling data across multiple chemical domains, e.g. molecules, materials, and catalysts. We develop empirical scaling laws to help understand how to increase model capacity alongside dataset size to achieve the best accuracy. The UMA small and medium models utilize a novel architectural design we refer to as mixture of linear experts that enables increasing model capacity without sacrificing speed. For example, UMA-medium has 1.4B parameters but only $\\sim$50M active parameters per atomic structure. We evaluate UMA models on a diverse set of applications across multiple domains and find that, remarkably, a single model without any fine-tuning can perform similarly or better than specialized models. We are releasing the UMA code, weights, and associated data to accelerate computational workflows and enable the community to build increasingly capable AI models.",
          "keywords": [
            "Chemistry",
            "Materials Science",
            "Machine Learning Interatomic Potential",
            "Density Functional Theory",
            "Graph Neural Networks"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "We introduce a Universal Model for Atoms (UMA), trained on nearly half a billion 3D atomic structures, designed to push accuracy, speed, and generalization in atomic simulations across chemistry using a single model without fine-tuning.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=SvopaNxYWt",
          "pdf_link": "https://openreview.net/pdf?id=SvopaNxYWt"
        },
        "paper_internal_id": "SvopaNxYWt",
        "category": "spotlight",
        "embedding_score": 0.7294424772262573,
        "final_score": 0.8861337304115295
      },
      "oral": {
        "paper": {
          "id": "ImpizBSKcu",
          "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
          "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm.  We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$,\nand large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies:\n$(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network;\n$(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity;\n$(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated  `feature unlearning' regime at large times.",
          "keywords": [
            "Overfitting; feature learning; dynamical mean field theory; generalization;"
          ],
          "primary_area": "theory",
          "TLDR": "Large neural networks first learn low dimensional feature representation then overfit the data and revert to a kernel regime.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=ImpizBSKcu",
          "pdf_link": "https://openreview.net/pdf?id=ImpizBSKcu"
        },
        "paper_internal_id": "ImpizBSKcu",
        "category": "oral",
        "embedding_score": 0.597667932510376,
        "final_score": 0.819999635219574
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "8YniJnJQ0P",
      "title": "Detecting High-Stakes Interactions with Activation Probes",
      "abstract": "Monitoring is an important aspect of safely deploying Large Language Models (LLMs).\nThis paper examines activation probes for detecting ``high-stakes'' interactions---where the text indicates that the interaction might lead to significant harm---as a critical, yet underexplored, target for such monitoring.\nWe evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data.\nProbes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude.\nThese savings are enabled by reusing activations of the model that is being monitored.\nOur experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis.\nWe release our novel synthetic dataset and the codebase at\n\\url{https://github.com/arrrlex/models-under-pressure}.",
      "keywords": "['linear probes', 'monitoring', 'mechanistic interpretability', 'large language models']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "8YniJnJQ0P",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "obXGSmmG70",
          "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06% on APP, while maintaining high performance on complex tasks. This substantial token decrease directly translates to a significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering a practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications.",
          "keywords": [
            "Adaptive Reasoning",
            "Chain-of-Thought",
            "Large Language Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "LLMs using Chain-of-Thought (CoT) for everything is wasteful. We built AdaCoT, a smart system that teaches LLMs when to use CoT based on clear principles, saving compute and improving user experience without sacrificing performance on hard tasks.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=obXGSmmG70",
          "pdf_link": "https://openreview.net/pdf?id=obXGSmmG70"
        },
        "paper_internal_id": "obXGSmmG70",
        "category": "reject",
        "embedding_score": 0.7265040874481201,
        "final_score": 0.09629775583744049
      },
      "spotlight": {
        "paper": {
          "id": "DwXX8c7xst",
          "title": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs",
          "abstract": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property $T$ that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed.  \nExamples of target properties $T$ include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions.  \nThe LLM reveals an \\emph{observable signal} $Z$ that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits.\nYet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off.\nWe fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit.\nTreating the mutual information $I(Z;T)$ between the observation $Z$ and the target property $T$ as the leaked bits per query, we show that achieving error $\\varepsilon$ requires at least $\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy.\nThus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy.\nExperiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen.\nOur results provide the first principled yardstick for balancing transparency and security when deploying LLMs.",
          "keywords": [
            "Large Language Models",
            "Jailbreak attack",
            "Security"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-15",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=DwXX8c7xst",
          "pdf_link": "https://openreview.net/pdf?id=DwXX8c7xst"
        },
        "paper_internal_id": "DwXX8c7xst",
        "category": "spotlight",
        "embedding_score": 0.7654931545257568,
        "final_score": 0.19158823788166046
      },
      "oral": {
        "paper": {
          "id": "Q3qAsZAEZw",
          "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
          "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
          "keywords": [
            "Large Language Models (LLMs)",
            "Reproducibility",
            "Numerical precision",
            "Deterministic inference"
          ],
          "primary_area": "deep_learning",
          "TLDR": "This paper demonstrates that low precision causes non-reproducible LLM inference across different setups, proposing a hybrid-precision method, LayerCast, that computes in FP32 to achieve determinism while saving memory.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=Q3qAsZAEZw",
          "pdf_link": "https://openreview.net/pdf?id=Q3qAsZAEZw"
        },
        "paper_internal_id": "Q3qAsZAEZw",
        "category": "oral",
        "embedding_score": 0.7333893179893494,
        "final_score": 0.017719300463795662
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Ceb788Uigr",
      "title": "On the Convergence of Stochastic Smoothed Multi-Level Compositional Gradient Descent Ascent",
      "abstract": "Multi-level compositional optimization is a fundamental framework in machine learning with broad applications. While recent advances have addressed compositional minimization problems, the stochastic multi-level compositional minimax problem introduces significant new challenges—most notably, the biased nature of stochastic gradients for both the primal and dual variables. In this work, we address this gap by proposing a novel stochastic multi-level compositional gradient descent-ascent algorithm, incorporating a smoothing technique under the nonconvex-PL condition. We establish a convergence rate to an $(\\epsilon, \\epsilon/\\sqrt{\\kappa})$-stationary point with improved dependence on the condition number at $O(\\kappa^{3/2})$, where $\\epsilon$ denotes the solution accuracy and $\\kappa$ represents the condition number. Moreover,  we design a novel stage-wise algorithm with variance reduction to address the  biased gradient issue under the two-sided PL condition. This algorithm successfully enables a translation from and $(\\epsilon, \\epsilon/\\sqrt{\\kappa})$-stationary point to an $\\epsilon$-stationary point. Finally, extensive experiments validate the effectiveness of our algorithms.",
      "keywords": "['Compositional Optimization', 'Minimax Optimization']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Ceb788Uigr",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "2Ri68h7bD1",
          "title": "Dale's Law Meets Geometric Brownian Motion: Multiplicative Updates for Sampling",
          "abstract": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Proceeding further, we propose a new formalism for multiplicative denoising score-matching, which subsumes the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.",
          "keywords": [
            "Dale's Law",
            "Geometric Brownian Motion",
            "Stochastic Differential Equations",
            "Score-matching"
          ],
          "primary_area": "other",
          "TLDR": "This papers reports a novel, biologically inspired generative model founded on Dale's law and geometric Brownian motion with links to multiplicative denoising and Hyvaerinen's score-matching.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=2Ri68h7bD1",
          "pdf_link": "https://openreview.net/pdf?id=2Ri68h7bD1"
        },
        "paper_internal_id": "2Ri68h7bD1",
        "category": "reject",
        "embedding_score": 0.7338166236877441,
        "final_score": 0.6536032557487488
      },
      "spotlight": {
        "paper": {
          "id": "L51U5RSFBo",
          "title": "Accelerating Optimization via Differentiable Stopping Time",
          "abstract": "A common approach for accelerating optimization algorithms is to minimize the loss achieved in a fixed time, which enables a differentiable framework with respect to the algorithm's hyperparameters. In contrast, the complementary objective of minimizing the time to reach a target loss is traditionally considered non-differentiable. To address this limitation, we propose a differentiable discrete stopping time and theoretically justify it based on its connection to continuous differential equations. We design an efficient algorithm to compute its sensitivities, thereby enabling a new differentiable formulation for directly accelerating algorithms. We demonstrate its effectiveness in applications such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.",
          "keywords": [
            "optimization",
            "implicit differentiation",
            "hyperparameter tuning",
            "differential equations"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=L51U5RSFBo",
          "pdf_link": "https://openreview.net/pdf?id=L51U5RSFBo"
        },
        "paper_internal_id": "L51U5RSFBo",
        "category": "spotlight",
        "embedding_score": 0.7204509973526001,
        "final_score": 0.8716254830360413
      },
      "oral": {
        "paper": {
          "id": "WhEPg4mUs6",
          "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
          "abstract": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses.  While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions.  We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.",
          "keywords": [
            "Analog AI; in-memory computing; stochastic gradient descent; stochastic optimization"
          ],
          "primary_area": "optimization",
          "TLDR": "Leveraging a residual learning framework to support the model training on non-ideal analog in-memory computing hardware",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=WhEPg4mUs6",
          "pdf_link": "https://openreview.net/pdf?id=WhEPg4mUs6"
        },
        "paper_internal_id": "WhEPg4mUs6",
        "category": "oral",
        "embedding_score": 0.6596048474311829,
        "final_score": 0.5473605394363403
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "mtJSMcF3ek",
      "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models",
      "abstract": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data.  Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the **generation-verification gap**. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries.",
      "keywords": "['LLM', 'self-improvement', 'synthetic data', 'post-training', 'test-time optimization']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "mtJSMcF3ek",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "DzKdjWe59v",
          "title": "Hint Marginalization for Improved Reasoning in Large Language Models",
          "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode the most likely answer. Empirical evaluation on several benchmark datasets for arithmetic reasoning demonstrates the superiority of the proposed approach.",
          "keywords": [
            "reasoning",
            "large language models"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=DzKdjWe59v",
          "pdf_link": "https://openreview.net/pdf?id=DzKdjWe59v"
        },
        "paper_internal_id": "DzKdjWe59v",
        "category": "reject",
        "embedding_score": 0.7459321022033691,
        "final_score": 0.9973505735397339
      },
      "poster": {
        "paper": {
          "id": "jjfve2gIXe",
          "title": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models",
          "abstract": "Large language models (LLMs) have been shown to exhibit *emergent abilities* in some downstream tasks, where model performance stagnates at first and then improves sharply and unpredictably with scale beyond a threshold. In this work, we investigate the phenomenon by grouping questions based on difficulty level and provide a possible explanation for emergent abilities. Specifically, we observe U-shaped scaling for hard questions and inverted-U scaling followed by steady improvement for easy questions. The two scaling patterns initially offset each other, causing stagnant overall performance. The performance starts to soar when the scaling pattern of easy questions reverts from inverse to standard scaling, leading to emergent abilities. Based on this finding, we propose a simple yet effective pipeline, called *Slice-and-Sandwich*, to predict the emergence threshold and model performance beyond the threshold. Our code is publicly available at https://github.com/tony10101105/ExpEmergence.",
          "keywords": [
            "large language models",
            "emergent abilities",
            "scaling laws"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "The emergent abilities of LLMs can be decomposed into complementary non-trivial scaling trends in easy and hard samples, which sheds light on deeper understanding and prediction of emergent abilities.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=jjfve2gIXe",
          "pdf_link": "https://openreview.net/pdf?id=jjfve2gIXe"
        },
        "paper_internal_id": "jjfve2gIXe",
        "category": "poster",
        "embedding_score": 0.7525168657302856,
        "final_score": 0.9975292086601257
      },
      "spotlight": {
        "paper": {
          "id": "K2jOacHUlO",
          "title": "To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts",
          "abstract": "Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model’s internal knowledge. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context to resolve knowledge conflicts. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. \nOur results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs.",
          "keywords": [
            "Large Language Model",
            "Knowledge Conflict",
            "Retrieval Augmented Generation",
            "Confidence Estimation",
            "Reasoning"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We benchmark the challenge of ensuring large language models remain situationally faithful to potentially incorrect external information and propose Self-Guided Confidence Reasoning to enhance LLM's reliability.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=K2jOacHUlO",
          "pdf_link": "https://openreview.net/pdf?id=K2jOacHUlO"
        },
        "paper_internal_id": "K2jOacHUlO",
        "category": "spotlight",
        "embedding_score": 0.7409186363220215,
        "final_score": 0.9933797121047974
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "GRMfXcAAFh",
      "title": "Oscillatory State-Space Models",
      "abstract": "We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba and LRU by nearly 2x on a sequence modeling task with sequences of length 50k.",
      "keywords": "['state-space models', 'sequence models', 'oscillators', 'long-range interactions', 'time-series']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "GRMfXcAAFh",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "WWymYrA48K",
          "title": "Test Time Learning for Time Series Forecasting",
          "abstract": "We propose the use of Test-Time Training (TTT) modules in a cascade architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements, especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that convolutional blocks as hidden layer architectures can achieve competitive results.",
          "keywords": [
            "Time Series Forecasting",
            "Test-Time Training",
            "Mamba",
            "Expressive Hidden States",
            "Modern CNN"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "Test-Time Learning Applied To Long Time Series Forecasting",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=WWymYrA48K",
          "pdf_link": "https://openreview.net/pdf?id=WWymYrA48K"
        },
        "paper_internal_id": "WWymYrA48K",
        "category": "reject",
        "embedding_score": 0.7503854632377625,
        "final_score": 0.9986680746078491
      },
      "poster": {
        "paper": {
          "id": "RoN6M3i7gJ",
          "title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics",
          "abstract": "By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models. However, the complexity of these models generally increases with the system dimensionality, requiring larger datasets, more complex deep networks, and significant computational effort.\nWe propose a novel geometric network architecture to learn physically-consistent reduced-order dynamic parameters that accurately describe the original high-dimensional system behavior.\nThis is achieved by building on recent advances in model-order reduction and by adopting a Riemannian perspective to jointly learn a non-linear structure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional dynamics of rigid and deformable systems with increased data efficiency by inferring interpretable and physically-plausible reduced Lagrangian models.",
          "keywords": [
            "physics-inspired networks",
            "dynamics learning",
            "model-order reduction",
            "Riemannian geometry",
            "deformable objects"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "a physics-inspired geometric model to learn dynamics of high-dimensional systems via structure-preserving model-order reduction",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=RoN6M3i7gJ",
          "pdf_link": "https://openreview.net/pdf?id=RoN6M3i7gJ"
        },
        "paper_internal_id": "RoN6M3i7gJ",
        "category": "poster",
        "embedding_score": 0.7541621327400208,
        "final_score": 0.9954153299331665
      },
      "spotlight": {
        "paper": {
          "id": "wkHcXDv7cv",
          "title": "Tuning Frequency Bias of State Space Models",
          "abstract": "State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an $88.26\\\\%$ accuracy on the Long-Range Arena (LRA) benchmark tasks.",
          "keywords": [
            "state-space models",
            "sequence models",
            "Long-Range Arena",
            "frequency bias"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We propose two mechanisms to diminish or increase the learning rate of high-frequency components relative to low-frequency ones in a state space model (SSM).",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-04",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=wkHcXDv7cv",
          "pdf_link": "https://openreview.net/pdf?id=wkHcXDv7cv"
        },
        "paper_internal_id": "wkHcXDv7cv",
        "category": "spotlight",
        "embedding_score": 0.8197622299194336,
        "final_score": 0.8916497826576233
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "k3tbMMW8rH",
      "title": "Feedback Schrödinger Bridge Matching",
      "abstract": "Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. \nFully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications.\nTo strike a balance between scalability and minimal supervision, we introduce Feedback Schrödinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion ($<8$% of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets.",
      "keywords": "['Diffusion models', 'Schrödinger bridge', 'Distribution matching', 'Semi-Supervised Learning']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "k3tbMMW8rH",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "Da3j02cHe0",
          "title": "Efficient Physics-Constrained Diffusion Models for Solving Inverse Problems",
          "abstract": "Solving inverse problems in scientific and engineering domains often involves complex, nonlinear forward physics and ill-posed conditions. \nRecent advancements in diffusion model have shown promise for general inverse problems, yet their application to scientific domains remains less explored and is hindered by the complexity and high non-linearity of physics constraints. We present a physics-constrained diffusion model (PCDM) designed to solve inverse problems in scientific and engineering domains by efficiently integrating pre-trained diffusion models and physics-constrained objectives.\nWe leverage accelerated diffusion sampling to enable a practical generation process while strictly adhering to physics constraints by solving optimization problems at each timestep. By decoupling the likelihood optimization from the reverse diffusion steps, we ensure that the solutions remain physically consistent, even when employing fewer sampling steps.\nWe validate our method on a wide range of challenging physics-constrained inverse problems, including data assimilation, topology optimization, and full-waveform inversion. Experimental results show that our approach significantly outperforms existing methods in efficiency and precision, making it practical for real-world applications.",
          "keywords": [
            "physics-constraints inverse problem",
            "diffusion model",
            "PDE",
            "generative modeling"
          ],
          "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
          "TLDR": "We propose a novel framework for solving physics-constrained inverse problems by integrating physics constraints and diffusion models.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=Da3j02cHe0",
          "pdf_link": "https://openreview.net/pdf?id=Da3j02cHe0"
        },
        "paper_internal_id": "Da3j02cHe0",
        "category": "reject",
        "embedding_score": 0.6938909292221069,
        "final_score": 0.39765068888664246
      },
      "poster": {
        "paper": {
          "id": "GcvLoqOoXL",
          "title": "Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior",
          "abstract": "Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512$\\times$512 ImageNet images, revealing that: 1) DPS’s conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPS’s conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPS's performance. The source code for these improvements is provided in https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior.",
          "keywords": [
            "Diffusion models",
            "Inverse problem"
          ],
          "primary_area": "generative models",
          "TLDR": "We show that diffusion posterior sampling is in fact maximizing a posterior, and propose two improvements based on that.",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=GcvLoqOoXL",
          "pdf_link": "https://openreview.net/pdf?id=GcvLoqOoXL"
        },
        "paper_internal_id": "GcvLoqOoXL",
        "category": "poster",
        "embedding_score": 0.6825116872787476,
        "final_score": 0.8104786276817322
      },
      "spotlight": {
        "paper": {
          "id": "hBGavkf61a",
          "title": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning",
          "abstract": "Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable $\\mathbf{z}$. Meanwhile, this auxiliary structure invokes an *information split problem*; the information of each data instance $\\mathbf{x}_0$ is divided into diffusion endpoint $\\mathbf{x}_T$ and encoded $\\mathbf{z}$ because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint $\\mathbf{x}_T$ has some disadvantages. The diffusion endpoint $\\mathbf{x}_T$ is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables $\\mathbf{z}$-dependent endpoint $\\mathbf{x}_T$ inference through a feed-forward architecture. This structure creates an information bottleneck at $\\mathbf{z}$, so $\\mathbf{x}_T$ becomes dependent on $\\mathbf{z}$ in its generation. This results in $\\mathbf{z}$ holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation. Our code is\navailable at https://github.com/aailab-kaist/DBAE.",
          "keywords": [
            "Diffusion Model",
            "Represenation Learning",
            "Autoencoders"
          ],
          "primary_area": "generative models",
          "TLDR": "This paper introduces Diffusion Bridge Autoencoders (DBAE) to design encoder dependent endpoint inference.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=hBGavkf61a",
          "pdf_link": "https://openreview.net/pdf?id=hBGavkf61a"
        },
        "paper_internal_id": "hBGavkf61a",
        "category": "spotlight",
        "embedding_score": 0.7478103637695312,
        "final_score": 0.41983678936958313
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "xoXn62FzD0",
      "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
      "abstract": "A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution—which can differ substantially from the LM’s base distribution—is generally intractable. In this work,\nwe develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis—we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\\times$ larger, as well as closed-source, fine-tuned ones. \nIn support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. \n[Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.",
      "keywords": "['Sequential Monte Carlo', 'Language Models', 'Semantic parsing', 'Bayesian inference', 'Probabilistic programming', 'SMC']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "xoXn62FzD0",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "8QTpYC4smR",
          "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
          "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries.",
          "keywords": [
            "Large Language Models",
            "Systematic Review"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=8QTpYC4smR",
          "pdf_link": "https://openreview.net/pdf?id=8QTpYC4smR"
        },
        "paper_internal_id": "8QTpYC4smR",
        "category": "reject",
        "embedding_score": 0.7550958395004272,
        "final_score": 0.9374927282333374
      },
      "poster": {
        "paper": {
          "id": "5X5Z7Ffrjb",
          "title": "Steering Large Language Models between Code Execution and Textual Reasoning",
          "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling behavior. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at https://yongchao98.github.io/CodeSteer/.",
          "keywords": [
            "Large Language Models",
            "Code Interpreter",
            "Code/text generation",
            "Agent",
            "Textual reasoning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Significance of guiding LLM code/text generation, limitations of current approaches such as GPT Code Interpreter, better proposed methods, and the future.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=5X5Z7Ffrjb",
          "pdf_link": "https://openreview.net/pdf?id=5X5Z7Ffrjb"
        },
        "paper_internal_id": "5X5Z7Ffrjb",
        "category": "poster",
        "embedding_score": 0.7730967998504639,
        "final_score": 0.5683354735374451
      },
      "spotlight": {
        "paper": {
          "id": "csbf1p8xUq",
          "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale",
          "abstract": "Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and \nwhile some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality responses for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages. We introduce **X-ALMA**, a model designed to ensure top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. After the final stage of training regimen, our proposed **A**daptive **R**ejection **P**reference **O**ptimization (**ARPO**) surpasses existing preference optimization methods in translation tasks.",
          "keywords": [
            "Large Language Model",
            "Machine Translation",
            "Multilingual"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We present X-ALMA, a multilingual machine translation model that prioritizes quality over quantity by delivering top-tier performance across 50 diverse languages, regardless of their resource levels",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=csbf1p8xUq",
          "pdf_link": "https://openreview.net/pdf?id=csbf1p8xUq"
        },
        "paper_internal_id": "csbf1p8xUq",
        "category": "spotlight",
        "embedding_score": 0.7472501397132874,
        "final_score": 0.1344211995601654
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "3b9SKkRAKw",
      "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
      "abstract": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.",
      "keywords": "['data synthesis', 'diffusion models', 'cardiac MRI', 'lung nodule CT', 'segmentation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "3b9SKkRAKw",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "b3VzHRXrXh",
          "title": "Causal Frameworks and Feature Discrepancy Loss: Addressing Data Scarcity and Enhancing Medical Image Segmentation",
          "abstract": "Data scarcity poses a significant challenge for deep learning models in medical imaging, particularly for training and generalization. Previous studies have demonstrated the efficacy of data pooling from various sources, facilitating the analysis of weak but significant correlations between imaging data and disease incidence. This approach is often constrained by strict data-sharing protocols among institutions, resulting in models reliant on external data sources. In this work, we address the issue of data scarcity by leveraging the available data for segmentation tasks across various medical imaging modalities. Based on our observation that samples with minimal foreground-background feature differences often demonstrate inadequate segmentation performance, we propose a causal-inspired foreground-background feature discrepancy penalty function, which improves feature separation and alleviates segmentation difficulties caused by homogeneous pixel distributions. The proposed feature discrepancy loss is mathematically grounded, with a lower bound defined by the negative logarithm of the Dice coefficient, suggesting that increased feature separation correlates with improved Dice scores. To further validate our approach, we introduce a novel ultrasound dataset for triple-negative breast cancer (TNBC), and we evaluate the method across three state-of-the-art segmentation architectures to demonstrate competitive performance. In addition, the results highlight the robustness of our method in mitigating performance decrease due to distribution shifts when new, differently distributed data batches are introduced.",
          "keywords": [
            "causal reasoning",
            "bioemdical image segmentation",
            "data dilemma"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=b3VzHRXrXh",
          "pdf_link": "https://openreview.net/pdf?id=b3VzHRXrXh"
        },
        "paper_internal_id": "b3VzHRXrXh",
        "category": "reject",
        "embedding_score": 0.763583242893219,
        "final_score": 0.6114779710769653
      },
      "poster": {
        "paper": {
          "id": "NJxCpMt0sf",
          "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
          "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term “modality-task dependence.” To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
          "keywords": [
            "Multimodal Learning",
            "Medical Imaging"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "A Sample and Task-dynamic Model for Multi-modal Multi-task Medical Image Learning",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=NJxCpMt0sf",
          "pdf_link": "https://openreview.net/pdf?id=NJxCpMt0sf"
        },
        "paper_internal_id": "NJxCpMt0sf",
        "category": "poster",
        "embedding_score": 0.708448052406311,
        "final_score": 0.17087069153785706
      },
      "oral": {
        "paper": {
          "id": "LyJi5ugyJx",
          "title": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models",
          "abstract": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512×512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512, narrowing the gap in FID scores with the best existing diffusion models to within 10\\%.",
          "keywords": [
            "continuous-time consistency models",
            "diffusion models",
            "fast sampling"
          ],
          "primary_area": "generative models",
          "TLDR": "2-step continuous-time consistency models reduce the gap to within 10\\% in sample quality (FID) compared to best diffusion models",
          "creation_date": "2024-09-20",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=LyJi5ugyJx",
          "pdf_link": "https://openreview.net/pdf?id=LyJi5ugyJx"
        },
        "paper_internal_id": "LyJi5ugyJx",
        "category": "oral",
        "embedding_score": 0.7287123799324036,
        "final_score": 0.017172757536172867
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "8oFvUBvF1u",
      "title": "DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo",
      "abstract": "Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at https://tea-lab.github.io/DenseMatcher/.",
      "keywords": "['robotics', 'correspondence', 'computer vision', '3D vision']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "8oFvUBvF1u",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "xcHIiZr3DT",
          "title": "Vision-Based Pseudo-Tactile Information Extraction and Localization for Dexterous Grasping",
          "abstract": "This study addresses the challenges of tactile perception in robotic dexterous hand grasping by focusing on two main tasks: 1) Acquiring tactile information from everyday objects using vision, termed \"pseudo-tactile\" information, and 2) Building a Dexterous Hand (RH8D) model in Isaac Sim for real-time fingertip contact localization. Utilizing Isaac Sim enables safe, cost-effective experimentation and high-precision simulations that facilitate data collection for model validation. The research establishes a scientific connection between simulated 3D coordinates, actual 3D coordinates, and pseudo-tactile information derived from point clouds, quantified through normal vectors and grayscale variance analysis. Results demonstrate the ability to extract clear object surface textures, accurately locate fingertip contact points in real-time (with precision up to $0.001 m$), and provide tactile information at contact points. This framework enhances robotic grasping capabilities and offers low-cost sensory data. The source code and dataset are publicly available now.",
          "keywords": [
            "Pseudo-Tactile Information",
            "Dexterous Grasping",
            "Vision-Based Perception",
            "Robotic Localization"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "This study extracts pseudo-tactile information from everyday objects using vision, enabling real-time localization of fingertip contact points and their corresponding pseudo-tactile 3D point cloud information in robotic dexterous hand grasping.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=xcHIiZr3DT",
          "pdf_link": "https://openreview.net/pdf?id=xcHIiZr3DT"
        },
        "paper_internal_id": "xcHIiZr3DT",
        "category": "reject",
        "embedding_score": 0.6930418014526367,
        "final_score": 0.9571897387504578
      },
      "poster": {
        "paper": {
          "id": "CNO4rbSV6v",
          "title": "Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning",
          "abstract": "Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear.\nIn this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. Code is available on https://github.com/qq456cvb/3DCorrEnhance.",
          "keywords": [
            "Vision Foundation Models; 3D Representation Learning; Fine-tuning; 3D Equivariance"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "This work evaluates and improves the 3D awareness of ViT-based models by enhancing their 3D equivariance through a simple finetuning strategy using 3D correspondences.",
          "creation_date": "2024-09-21",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=CNO4rbSV6v",
          "pdf_link": "https://openreview.net/pdf?id=CNO4rbSV6v"
        },
        "paper_internal_id": "CNO4rbSV6v",
        "category": "poster",
        "embedding_score": 0.7334260940551758,
        "final_score": 0.9866467714309692
      },
      "oral": {
        "paper": {
          "id": "meRCKuUpmc",
          "title": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation",
          "abstract": "Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available.",
          "keywords": [
            "Robotic Manipulation ; Pre-training ; Visual Foresight ; Inverse Dynamics ; Large-scale robot dataset"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=meRCKuUpmc",
          "pdf_link": "https://openreview.net/pdf?id=meRCKuUpmc"
        },
        "paper_internal_id": "meRCKuUpmc",
        "category": "oral",
        "embedding_score": 0.6982592940330505,
        "final_score": 0.9021250009536743
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "YcbE2K3i2E",
      "title": "SaTran: An efficient Transformer exploiting Spatiotemporal Redundancies for Satellite Image Time Series Representation Learning",
      "abstract": "Earth observation applications like crop yield prediction, solar energy prediction, land cover classification, etc., need large size Satellite Image Time Series (SITS) leading to huge computational requirements. A couple of BERT-based models exist which work at pixel level unable to exploit spatial correlation among pixels and also require ground truth at pixel granularity during fine-tuning, rendering them infeasible for prediction tasks. The  models based on Vision Transformer factorize spatial and time dimensions and first process images and then time series of image embeddings. However, in many cases, SITS require simultaneous analysis of both dimensions. We present a transformer, SaTran, which focuses on non-redundant patch tubes to overcome the limitations listed above. Transformers developed for RGB videos are found lacking when applied to SITS data characterized by the presence of patches with spatiotemporal redundancy persisting throughout the time series. SITS data also has patches where temporal redundancy lasts only for a few timestamps. The salient features of SaTran include: 1) an automatic patch tube selection mechanism which ignores spatiotemporally redundant patches; 2) exploitation of spatial correlation between pixels by the processing of patch tubes and handling of their temporal redundancy using tube masking; 3) two-fold handling of redundancy and distributed application of VideoMAE enables space and time efficient processing of large size SITS; and 4) learning end task agnostic representation of entire time series. Extensive experimentation shows that SaTran outperforms competing models and exhibit state-of-the-art performance for various earth observation applications. The code is available on (.. will be given after acceptance..).",
      "keywords": "['Satellite image time series analytics', 'Transformer', 'Earth observation applications', 'Spatiotemporal redundancy', 'Representation learning']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "YcbE2K3i2E",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "OD1MV7vf41",
          "title": "Deep Random Features for Scalable Interpolation of Spatiotemporal Data",
          "abstract": "The rapid growth of earth observation systems calls for a scalable approach to interpolate remote-sensing observations. These methods in principle, should acquire more information about the observed field as data grows. Gaussian processes (GPs) are candidate model choices for interpolation. However, due to their poor scalability, they usually rely on inducing points for inference, which restricts their expressivity. Moreover, commonly imposed assumptions such as stationarity prevents them from capturing complex patterns in the data. While deep GPs can overcome this issue, training and making inference with them are difficult, again requiring crude approximations via inducing points. In this work, we instead approach the problem through Bayesian deep learning, where spatiotemporal fields are represented by deep neural networks, whose layers share the inductive bias of stationary GPs on the plane/sphere via random feature expansions. This allows one to (1) capture high frequency patterns in the data, and (2) use mini-batched gradient descent for large scale training. We experiment on various remote sensing data at local/global scales, showing that our approach produce competitive or superior results to existing methods, with well-calibrated uncertainties.",
          "keywords": [
            "Random Features",
            "Deep Gaussian Processes",
            "Bayesian Deep Learning",
            "Remote Sensing"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "We propose a scalable Bayesian deep learning framework to interpolate remote sensing data for increased accuracy and flexibility.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-31",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=OD1MV7vf41",
          "pdf_link": "https://openreview.net/pdf?id=OD1MV7vf41"
        },
        "paper_internal_id": "OD1MV7vf41",
        "category": "poster",
        "embedding_score": 0.7516698837280273,
        "final_score": 0.3551701307296753
      },
      "spotlight": {
        "paper": {
          "id": "PkpNRmBZ32",
          "title": "Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions",
          "abstract": "We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism.",
          "keywords": [
            "state-space models; convolution; tensor networks; audio processing; speech recognition"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "using deep SSMs like ConvNets to do audio processing",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=PkpNRmBZ32",
          "pdf_link": "https://openreview.net/pdf?id=PkpNRmBZ32"
        },
        "paper_internal_id": "PkpNRmBZ32",
        "category": "spotlight",
        "embedding_score": 0.6783918142318726,
        "final_score": 0.03302707523107529
      },
      "oral": {
        "paper": {
          "id": "WOzffPgVjF",
          "title": "Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding",
          "abstract": "Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel $\\textbf{T}$arget-$\\textbf{A}$ware Transformer for $\\textbf{STVG}$ ($\\textbf{TA-STVG}$), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. Moreover, TTS and ASA are designed for general purpose. When applied to existing methods such as TubeDETR and STCAT, we show substantial performance gains, verifying its generality. Code is released at https://github.com/HengLan/TA-STVG.",
          "keywords": [
            "Spatio-Temporal Video Grounding"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-13",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=WOzffPgVjF",
          "pdf_link": "https://openreview.net/pdf?id=WOzffPgVjF"
        },
        "paper_internal_id": "WOzffPgVjF",
        "category": "oral",
        "embedding_score": 0.7371995449066162,
        "final_score": 0.02594229206442833
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "EqcLAU6gyU",
      "title": "Agent-Oriented Planning in Multi-Agent Systems",
      "abstract": "Through the collaboration of multiple LLM-empowered agents possessing diverse expertise and tools, multi-agent systems achieve impressive progress in solving real-world problems. Given the user queries, the meta-agents, serving as the brain within multi-agent systems, are required to decompose the queries into multiple sub-tasks that can be allocated to suitable agents capable of solving them, so-called agent-oriented planning. In this study, we identify three critical design principles of agent-oriented planning, including solvability, completeness, and non-redundancy, to ensure that each sub-task can be effectively resolved, resulting in satisfactory responses to user queries. These principles further inspire us to propose AOP, a novel framework for agent-oriented planning in multi-agent systems, leveraging a fast task decomposition and allocation process followed by an effective and efficient evaluation via a reward model. According to the evaluation results, the meta-agent is also responsible for promptly making necessary adjustments to sub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to further enhance the effectiveness and robustness of such a problem-solving process. Extensive experiments demonstrate the advancement of AOP in solving real-world problems compared to both single-agent systems and existing planning strategies for multi-agent systems. The source code is available at https://github.com/lalaliat/Agent-Oriented-Planning",
      "keywords": "['Multi-Agent System; Planning']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "EqcLAU6gyU",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "c4w1TqcSi0",
          "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
          "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods for multi-agent collaboration. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. At its core, Optima employs an iterative generate, rank, select, and train paradigm, incorporating a reward function that balances task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs for iterative LLM-based MAS training. Additionally, we integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, conceptualizing conversation turns as tree nodes to explore diverse interaction trajectories. We evaluate Optima on common multi-agent tasks, including information-asymmetric question answering and complex reasoning. Our method demonstrates consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy multi-agent information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, potentially leading to improved inference-time scaling laws. By addressing fundamental challenges in multi-agent collaboration and providing a novel optimization framework, Optima shows the potential towards scalable, efficient, and effective LLM-based MAS.",
          "keywords": [
            "llm agent",
            "multi-agent",
            "inference scaling law"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We present Optima, a framework for training LLM-based multi-agent systems that boosts communication efficiency and task effectiveness. Using iterative training, we achieve significant token reduction and performance gains across diverse tasks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=c4w1TqcSi0",
          "pdf_link": "https://openreview.net/pdf?id=c4w1TqcSi0"
        },
        "paper_internal_id": "c4w1TqcSi0",
        "category": "reject",
        "embedding_score": 0.7753111124038696,
        "final_score": 0.87552809715271
      },
      "spotlight": {
        "paper": {
          "id": "h0ZfDIrj7T",
          "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
          "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni.",
          "keywords": [
            "Multi-Agent Inference",
            "Large Language Model"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "This paper presents a method that synergistically leverage multiple LLMs to significantly improve their performance.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=h0ZfDIrj7T",
          "pdf_link": "https://openreview.net/pdf?id=h0ZfDIrj7T"
        },
        "paper_internal_id": "h0ZfDIrj7T",
        "category": "spotlight",
        "embedding_score": 0.7077158689498901,
        "final_score": 0.8730517625808716
      },
      "oral": {
        "paper": {
          "id": "st77ShxP1K",
          "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
          "abstract": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BenchForm, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs’ behavior in collaborative scenarios. Several representative LLMs are evaluated on BenchForm, using metrics such as conformity rate and independence rate to quantify conformity’s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs’ conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code are available at BenchForm.",
          "keywords": [
            "Large Language Models",
            "Conformity",
            "Multi-agent System"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=st77ShxP1K",
          "pdf_link": "https://openreview.net/pdf?id=st77ShxP1K"
        },
        "paper_internal_id": "st77ShxP1K",
        "category": "oral",
        "embedding_score": 0.7396872043609619,
        "final_score": 0.8182804584503174
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "reZKq6hjOZ",
      "title": "Broadening Target Distributions for Accelerated Diffusion Models via a Novel Analysis Approach",
      "abstract": "Accelerated diffusion models hold the potential to significantly enhance the efficiency of standard diffusion processes. Theoretically, these models have been shown to achieve faster convergence rates than the standard $\\mathcal O(1/\\epsilon^2)$ rate of vanilla diffusion models, where $\\epsilon$ denotes the target accuracy. However, current theoretical studies have established the acceleration advantage only for restrictive target distribution classes, such as those with smoothness conditions imposed along the entire sampling path or with bounded support. In this work, we significantly broaden the target distribution classes with a new accelerated stochastic DDPM sampler. In particular, we show that it achieves accelerated performance for three broad distribution classes not considered before. Our first class relies on the smoothness condition posed only to the target density $q_0$, which is far more relaxed than the existing smoothness conditions posed to all $q_t$ along the entire sampling path. Our second class requires only a finite second moment condition, allowing for a much wider class of target distributions than the existing finite-support condition. Our third class is Gaussian mixture, for which our result establishes the first acceleration guarantee. Moreover, among accelerated DDPM type samplers, our results specialized for bounded-support distributions show an improved dependency on the data dimension $d$. Our analysis introduces a novel technique for establishing performance guarantees via constructing a tilting factor representation of the convergence error and utilizing Tweedie's formula to handle Taylor expansion terms. This new analytical framework may be of independent interest.",
      "keywords": "['generative models', 'denoising diffusion probabilistic model (DDPM)', 'convergence analysis', 'accelerated methods']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "reZKq6hjOZ",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "vNZIePda08",
          "title": "Sparse-to-Sparse Training of Diffusion Models",
          "abstract": "Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown  potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and sometimes outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.",
          "keywords": [
            "Diffusion Models",
            "Sparse-to-Sparse Training",
            "Static Sparse Training",
            "Dynamic Sparse Training"
          ],
          "primary_area": "generative models",
          "TLDR": "We introduce sparse-to-sparse training to Diffusion Models, and obtain sparse DMs that are able to match and sometimes outperform the dense versions.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=vNZIePda08",
          "pdf_link": "https://openreview.net/pdf?id=vNZIePda08"
        },
        "paper_internal_id": "vNZIePda08",
        "category": "reject",
        "embedding_score": 0.7402153015136719,
        "final_score": 0.7355745434761047
      },
      "spotlight": {
        "paper": {
          "id": "TtUh0TOlGX",
          "title": "Regularization by Texts for Latent Diffusion Inverse Solvers",
          "abstract": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.",
          "keywords": [
            "Inverse problem",
            "Text regularization",
            "Diffusion model"
          ],
          "primary_area": "generative models",
          "TLDR": "We propose text regularization for inverse problem solving.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=TtUh0TOlGX",
          "pdf_link": "https://openreview.net/pdf?id=TtUh0TOlGX"
        },
        "paper_internal_id": "TtUh0TOlGX",
        "category": "spotlight",
        "embedding_score": 0.6531368494033813,
        "final_score": 0.5739317536354065
      },
      "oral": {
        "paper": {
          "id": "DJSZGGZYVi",
          "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
          "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
          "keywords": [
            "Diffusion models",
            "Representation learning"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-25",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=DJSZGGZYVi",
          "pdf_link": "https://openreview.net/pdf?id=DJSZGGZYVi"
        },
        "paper_internal_id": "DJSZGGZYVi",
        "category": "oral",
        "embedding_score": 0.6840250492095947,
        "final_score": 0.5519770383834839
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "peX9zpWgg4",
      "title": "Adaptive Shrinkage Estimation for Personalized Deep Kernel Regression in Modeling Brain Trajectories",
      "abstract": "Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners) as well as scarcity and irregularity in longitudinal measurements. Herein,\nwe introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model’s performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models—including linear mixed effects models, generalized additive models, and deep learning methods—demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at https://github.com/vatass/AdaptiveShrinkageDKGP.",
      "keywords": "['Deep Kernel Regression', 'Personalization', 'Posterior Correction', 'Longitudinal Biomarker Prediction']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "peX9zpWgg4",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "WWymYrA48K",
          "title": "Test Time Learning for Time Series Forecasting",
          "abstract": "We propose the use of Test-Time Training (TTT) modules in a cascade architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements, especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that convolutional blocks as hidden layer architectures can achieve competitive results.",
          "keywords": [
            "Time Series Forecasting",
            "Test-Time Training",
            "Mamba",
            "Expressive Hidden States",
            "Modern CNN"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "Test-Time Learning Applied To Long Time Series Forecasting",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=WWymYrA48K",
          "pdf_link": "https://openreview.net/pdf?id=WWymYrA48K"
        },
        "paper_internal_id": "WWymYrA48K",
        "category": "reject",
        "embedding_score": 0.6455531120300293,
        "final_score": 0.772193968296051
      },
      "spotlight": {
        "paper": {
          "id": "9UGfOJBuL8",
          "title": "Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies",
          "abstract": "Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation.",
          "keywords": [
            "neurodegenerative disease",
            "conditional diffusion model",
            "longitudinal data analysis"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "",
          "creation_date": "2024-09-21",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=9UGfOJBuL8",
          "pdf_link": "https://openreview.net/pdf?id=9UGfOJBuL8"
        },
        "paper_internal_id": "9UGfOJBuL8",
        "category": "spotlight",
        "embedding_score": 0.7681616544723511,
        "final_score": 0.4417051374912262
      },
      "oral": {
        "paper": {
          "id": "xByvdb3DCm",
          "title": "When Selection Meets Intervention: Additional Complexities in Causal Discovery",
          "abstract": "We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias.",
          "keywords": [
            "causal discovery",
            "selection bias",
            "experiments",
            "interventions"
          ],
          "primary_area": "causal reasoning",
          "TLDR": "",
          "creation_date": "2024-09-18",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-10",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=xByvdb3DCm",
          "pdf_link": "https://openreview.net/pdf?id=xByvdb3DCm"
        },
        "paper_internal_id": "xByvdb3DCm",
        "category": "oral",
        "embedding_score": 0.6491174697875977,
        "final_score": 0.23861804604530334
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "cJd1BgZ9CS",
      "title": "Distributed Speculative Inference (DSI): Speculation Parallelism for Provably Faster Lossless Language Model Inference",
      "abstract": "This paper introduces *distributed speculative inference (DSI)*, a novel inference algorithm that is provably faster than speculative inference (SI) [leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard autoregressive inference (non-SI). Like other SI algorithms, DSI operates on frozen language models (LMs), requiring no training or architectural modifications, and it preserves the target distribution. Prior studies on SI have demonstrated empirical speedups over non-SI—but rely on sufficiently fast and accurate drafters, which are often unavailable in practice. We identify a gap where SI can be slower than non-SI if drafters are too slow or inaccurate. We close this gap by proving that DSI is faster than both SI and non-SI—given any drafters. DSI is therefore not only faster than SI, but also unlocks the acceleration of LMs for which SI fails. DSI leverages *speculation parallelism (SP)*, a novel type of task parallelism, to orchestrate target and drafter instances that overlap in time, establishing a new foundational tradeoff between computational resources and latency. Our simulations show that DSI is 1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs and tasks. We open-source all our code.",
      "keywords": "['inference algorithms for generative models', 'LLM inference', 'speculative decoding']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "cJd1BgZ9CS",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "NI8AUSAc4i",
          "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy",
          "abstract": "The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable to pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific.\n\nThis paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of  KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance.",
          "keywords": [
            "KV Cache Compression",
            "Progressive Compression Strategy"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose to compress LLM's KV cache via a low-rank approximation of KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=NI8AUSAc4i",
          "pdf_link": "https://openreview.net/pdf?id=NI8AUSAc4i"
        },
        "paper_internal_id": "NI8AUSAc4i",
        "category": "reject",
        "embedding_score": 0.733478307723999,
        "final_score": 0.8311917185783386
      },
      "spotlight": {
        "paper": {
          "id": "kam84eEmub",
          "title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation",
          "abstract": "Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.",
          "keywords": [
            "directed acyclic graphs",
            "graph generation",
            "discrete diffusion",
            "autoregressive model"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=kam84eEmub",
          "pdf_link": "https://openreview.net/pdf?id=kam84eEmub"
        },
        "paper_internal_id": "kam84eEmub",
        "category": "spotlight",
        "embedding_score": 0.7370321750640869,
        "final_score": 0.6809217929840088
      },
      "oral": {
        "paper": {
          "id": "HD6bWcj87Y",
          "title": "Data Shapley in One Training Run",
          "abstract": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",
          "keywords": [
            "Shapley value",
            "data valuation."
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We develop a new notion of Data Shapley that requires only one model training run.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=HD6bWcj87Y",
          "pdf_link": "https://openreview.net/pdf?id=HD6bWcj87Y"
        },
        "paper_internal_id": "HD6bWcj87Y",
        "category": "oral",
        "embedding_score": 0.7074127197265625,
        "final_score": 0.6815366744995117
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "J2Jyp1SZ0n",
      "title": "MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines",
      "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search engines, e.g., SearchGPT, showcasing a new paradigm in human-internet interaction. However, most current AI search engines are limited to text-only settings, neglecting the multimodal user queries and the text-image interleaved nature of website information. Recently, Large Multimodal Models (LMMs) have made impressive strides. Yet, whether they can function as AI search engines remains under-explored, leaving the potential of LMMs in multimodal search an open question. To this end, we first design a delicate pipeline, MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On top of this, we introduce MMSearch, a comprehensive evaluation benchmark to assess the multimodal search performance of LMMs. The curated dataset contains 300 manually collected instances spanning 14 subfields, which involves no overlap with the current LMMs' training data, ensuring the correct answer can only be obtained within searching. By using MMSearch-Engine, the LMMs are evaluated by performing three individual tasks (requery, rerank, and summarization), and one challenging end-to-end task with a complete searching process. We conduct extensive experiments on closed-source and open-source LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best results, which surpasses the commercial product, Perplexity Pro, in the end-to-end task, demonstrating the effectiveness of our proposed pipeline. We further present error analysis to unveil current LMMs still struggle to fully grasp the multimodal search tasks, and conduct ablation study to indicate the potential of scaling test-time computation for AI search engine. We hope MMSearch may provide unique insights to guide the future development of multimodal AI search engine.",
      "keywords": "['Large Multimodal Model', 'AI Search Engine', 'Benchmark']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "J2Jyp1SZ0n",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "8QTpYC4smR",
          "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
          "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries.",
          "keywords": [
            "Large Language Models",
            "Systematic Review"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=8QTpYC4smR",
          "pdf_link": "https://openreview.net/pdf?id=8QTpYC4smR"
        },
        "paper_internal_id": "8QTpYC4smR",
        "category": "reject",
        "embedding_score": 0.7648705244064331,
        "final_score": 0.9543600082397461
      },
      "spotlight": {
        "paper": {
          "id": "n0OtGl6VGb",
          "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
          "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. \nHowever, their increased computational and memory demands present significant challenges, especially when handling long sequences.\nThis paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. \nUnlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights.\nIn response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20\\% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve $2.8\\times$ peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4$\\times$ (with KIVI alone) to 5$\\times$ when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK.  Our code has been made available at https://github.com/SalesforceAIResearch/ThinK.",
          "keywords": [
            "Large Language Models; KV Cache Compression; KV Cache Pruning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose a novel query-dependent KV cache channel pruning method to reduce the memory usage of LLMs during inference.",
          "creation_date": "2024-09-18",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=n0OtGl6VGb",
          "pdf_link": "https://openreview.net/pdf?id=n0OtGl6VGb"
        },
        "paper_internal_id": "n0OtGl6VGb",
        "category": "spotlight",
        "embedding_score": 0.7539922595024109,
        "final_score": 0.527684211730957
      },
      "oral": {
        "paper": {
          "id": "f4gF6AIHRy",
          "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
          "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Source code\nis available at: https://github.com/MediaBrain-SJTU/DiSF.git.",
          "keywords": [
            "file selection",
            "large language model",
            "pre-training",
            "submodular optimization"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=f4gF6AIHRy",
          "pdf_link": "https://openreview.net/pdf?id=f4gF6AIHRy"
        },
        "paper_internal_id": "f4gF6AIHRy",
        "category": "oral",
        "embedding_score": 0.7267780303955078,
        "final_score": 0.45766472816467285
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
      "abstract": "Despite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify two main issues: difficulties in handling non-textual representations (Karnaugh maps, state-transition diagrams and waveforms) and significant variability during training with models randomly making ''minor'' mistakes. To address these limitations, we enhance data curation by creating correct-by-construction data targeting non-textual representations. Additionally, we introduce an automated framework that generates error reports from various model checkpoints and injects these errors into open-source code to create targeted code repair data. Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by 3.8\\%, 10.9\\%, 6.6\\% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and RTLLM.",
      "keywords": [
        "Verilog Code Generation",
        "Synthetic Data Generation",
        "Large Language Models"
      ],
      "primary_area": "generative models",
      "TLDR": "We produce high-quality Verilog finetuning data that is correct-by-construction for non-textual representations and develop targeted code repair data by injecting errors into open-source code.",
      "creation_date": "2024-09-17",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-11",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=8KQzoD5XAr",
      "pdf_link": "https://openreview.net/pdf?id=8KQzoD5XAr",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "8KQzoD5XAr"
    },
    "query_internal_id": "8KQzoD5XAr",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "98ASXp6oPg",
          "title": "Self-Explained Keywords Empower Large Language Models for Code Generation",
          "abstract": "Large language models (LLMs) have achieved impressive performance in code generation. Despite the remarkable success, we observed that LLMs often misunderstand or overlook some problem-specific undertrained keywords during code generation, compromising the accuracy of the generated code. After explicitly explaining these undertrained keywords using well-trained terms in the prompt, LLMs are more likely to generate correct code implementation. Inspired by this observation, we propose a novel technique named SEK (Self-Explained Keywords), which empowers an LLM for better code generation by extracting and explaining the key terms in the problem description with the LLM itself. Comprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+), and APPS, with five representative LLMs, show that SEK can significantly improve LLMs in code generation, yielding substantial and consistent gains. For instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4% to 93.3% on the Humaneval benchmark. Further analysis confirms that SEK enables the LLMs to shift their attention from low-frequency keywords to their corresponding high-frequency counterparts.",
          "keywords": [
            "Large Language Model",
            "Code Generation",
            "Prompt Engineering"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We introduce SEK, a simple yet effective method that enhances LLMs' code generation by guiding them to extract, explain, and rank key terms from problem statements.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=98ASXp6oPg",
          "pdf_link": "https://openreview.net/pdf?id=98ASXp6oPg"
        },
        "paper_internal_id": "98ASXp6oPg",
        "category": "reject",
        "embedding_score": 0.783106803894043,
        "final_score": 0.9488574862480164
      },
      "spotlight": {
        "paper": {
          "id": "2hcfoCHKoB",
          "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model",
          "abstract": "Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. \nWe also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.",
          "keywords": [
            "Large Language Model",
            "Program Representation Learning",
            "Verilog Understanding and Generation"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=2hcfoCHKoB",
          "pdf_link": "https://openreview.net/pdf?id=2hcfoCHKoB"
        },
        "paper_internal_id": "2hcfoCHKoB",
        "category": "spotlight",
        "embedding_score": 0.8563259840011597,
        "final_score": 0.9654224514961243
      },
      "oral": {
        "paper": {
          "id": "YrycTjllL0",
          "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
          "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.",
          "keywords": [
            "Code Generation",
            "Tool Use",
            "Instruction Following",
            "Benchmark"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-12",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YrycTjllL0",
          "pdf_link": "https://openreview.net/pdf?id=YrycTjllL0"
        },
        "paper_internal_id": "YrycTjllL0",
        "category": "oral",
        "embedding_score": 0.756952166557312,
        "final_score": 0.5287772417068481
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "8oFvUBvF1u",
      "title": "DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo",
      "abstract": "Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at https://tea-lab.github.io/DenseMatcher/.",
      "keywords": "['robotics', 'correspondence', 'computer vision', '3D vision']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "8oFvUBvF1u",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "xcHIiZr3DT",
          "title": "Vision-Based Pseudo-Tactile Information Extraction and Localization for Dexterous Grasping",
          "abstract": "This study addresses the challenges of tactile perception in robotic dexterous hand grasping by focusing on two main tasks: 1) Acquiring tactile information from everyday objects using vision, termed \"pseudo-tactile\" information, and 2) Building a Dexterous Hand (RH8D) model in Isaac Sim for real-time fingertip contact localization. Utilizing Isaac Sim enables safe, cost-effective experimentation and high-precision simulations that facilitate data collection for model validation. The research establishes a scientific connection between simulated 3D coordinates, actual 3D coordinates, and pseudo-tactile information derived from point clouds, quantified through normal vectors and grayscale variance analysis. Results demonstrate the ability to extract clear object surface textures, accurately locate fingertip contact points in real-time (with precision up to $0.001 m$), and provide tactile information at contact points. This framework enhances robotic grasping capabilities and offers low-cost sensory data. The source code and dataset are publicly available now.",
          "keywords": [
            "Pseudo-Tactile Information",
            "Dexterous Grasping",
            "Vision-Based Perception",
            "Robotic Localization"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "This study extracts pseudo-tactile information from everyday objects using vision, enabling real-time localization of fingertip contact points and their corresponding pseudo-tactile 3D point cloud information in robotic dexterous hand grasping.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=xcHIiZr3DT",
          "pdf_link": "https://openreview.net/pdf?id=xcHIiZr3DT"
        },
        "paper_internal_id": "xcHIiZr3DT",
        "category": "reject",
        "embedding_score": 0.6930418014526367,
        "final_score": 0.9571897387504578
      },
      "poster": {
        "paper": {
          "id": "CNO4rbSV6v",
          "title": "Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning",
          "abstract": "Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear.\nIn this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. Code is available on https://github.com/qq456cvb/3DCorrEnhance.",
          "keywords": [
            "Vision Foundation Models; 3D Representation Learning; Fine-tuning; 3D Equivariance"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "This work evaluates and improves the 3D awareness of ViT-based models by enhancing their 3D equivariance through a simple finetuning strategy using 3D correspondences.",
          "creation_date": "2024-09-21",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=CNO4rbSV6v",
          "pdf_link": "https://openreview.net/pdf?id=CNO4rbSV6v"
        },
        "paper_internal_id": "CNO4rbSV6v",
        "category": "poster",
        "embedding_score": 0.7334260940551758,
        "final_score": 0.9866467714309692
      },
      "oral": {
        "paper": {
          "id": "meRCKuUpmc",
          "title": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation",
          "abstract": "Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available.",
          "keywords": [
            "Robotic Manipulation ; Pre-training ; Visual Foresight ; Inverse Dynamics ; Large-scale robot dataset"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=meRCKuUpmc",
          "pdf_link": "https://openreview.net/pdf?id=meRCKuUpmc"
        },
        "paper_internal_id": "meRCKuUpmc",
        "category": "oral",
        "embedding_score": 0.6982592940330505,
        "final_score": 0.9021250009536743
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Long-tailed Adversarial Training with Self-Distillation",
      "abstract": "Adversarial training significantly enhances adversarial robustness, yet superior performance is predominantly achieved on balanced datasets.\n Addressing adversarial robustness in the context of unbalanced or long-tailed distributions is considerably more challenging, mainly due to the scarcity of tail data instances. \n Previous research on adversarial robustness within long-tailed distributions has primarily focused on combining traditional long-tailed natural training with existing adversarial robustness methods.\n In this study, we provide an in-depth analysis for the challenge that adversarial training struggles to achieve high performance on tail classes in long-tailed distributions.\n Furthermore, we propose a simple yet effective solution to advance adversarial robustness on long-tailed distributions through a novel self-distillation technique.\n Specifically, this approach leverages a balanced self-teacher model, which is trained using a balanced dataset sampled from the original long-tailed dataset.\nOur extensive experiments demonstrate state-of-the-art performance in both clean and robust accuracy for long-tailed adversarial robustness, with significant improvements in tail class performance on various datasets.\nWe improve the accuracy against PGD attacks for tail classes by 20.3, 7.1, and 3.8 percentage points on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, while achieving the highest robust accuracy.",
      "keywords": [
        "Adversarial Robustness",
        "Adversarial Training",
        "Long-Tail Distribution Learning"
      ],
      "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
      "TLDR": "",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-24",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=vM94dZiqx4",
      "pdf_link": "https://openreview.net/pdf?id=vM94dZiqx4",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "vM94dZiqx4"
    },
    "query_internal_id": "vM94dZiqx4",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "WypSbOf9S9",
          "title": "MOREL: Enhancing Adversarial Robustness through Multi-Objective Representation Learning",
          "abstract": "Extensive research has shown that deep neural networks (DNNs) are vulnerable to slight adversarial perturbations—small changes to the input data that appear insignificant but cause the model to produce drastically different outputs. In addition to augmenting training data with adversarial examples generated from a specific attack method, most of the current defense strategies necessitate modifying the original model architecture components to improve robustness or performing test-time data purification to handle adversarial attacks. In this work, we demonstrate that strong feature representation learning during training can significantly enhance the original model's robustness. We propose MOREL, a multi-objective feature representation learning approach, encouraging classification models to produce similar features for inputs within the same class, despite perturbations. Our training method involves an embedding space where cosine similarity loss and multi-positive contrastive loss are used to align natural and adversarial features from the model encoder and ensure tight clustering. Concurrently, the classifier is motivated to achieve accurate predictions. Through extensive experiments, we demonstrate that our approach significantly enhances the robustness of DNNs against white-box and black-box adversarial attacks, outperforming other methods that similarly require no architectural changes or test-time data purification.",
          "keywords": [
            "Adversarial robustness",
            "Representation learning",
            "Multi-objective optimization",
            "Deep neural networks"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "The paper proposes MOREL, a method to improve deep neural network robustness against adversarial attacks by using a multi-objective approach to align natural and adversarial features during training, improving the accuracy-robustness trade-off.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=WypSbOf9S9",
          "pdf_link": "https://openreview.net/pdf?id=WypSbOf9S9"
        },
        "paper_internal_id": "WypSbOf9S9",
        "category": "reject",
        "embedding_score": 0.7374895215034485,
        "final_score": 0.9828647375106812
      },
      "spotlight": {
        "paper": {
          "id": "03OkC0LKDD",
          "title": "Adaptive Gradient Clipping for Robust Federated Learning",
          "abstract": "Robust federated learning aims to maintain reliable performance despite the presence of adversarial or misbehaving workers. While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods were proven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping.\nHowever, existing static clipping strategies yield inconsistent results: enhancing robustness against some attacks while being ineffective or even detrimental against others.\nTo address this limitation, we propose a principled adaptive clipping strategy, Adaptive Robust Clipping (ARC), which dynamically adjusts clipping thresholds based on the input gradients. We prove that ARC not only preserves the theoretical robustness guarantees of SOTA Robust-DGD methods but also provably improves asymptotic convergence when the model is well-initialized. Extensive experiments on benchmark image classification tasks confirm these theoretical insights, demonstrating that ARC significantly enhances robustness, particularly in highly heterogeneous and adversarial settings.",
          "keywords": [
            "Federated learning",
            "robustness",
            "Byzantine resilience"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-31",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=03OkC0LKDD",
          "pdf_link": "https://openreview.net/pdf?id=03OkC0LKDD"
        },
        "paper_internal_id": "03OkC0LKDD",
        "category": "spotlight",
        "embedding_score": 0.7459965348243713,
        "final_score": 0.974997341632843
      },
      "oral": {
        "paper": {
          "id": "syThiTmWWm",
          "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
          "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **\"null model\"** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an $86.5\\\\%$ LC win rate on AlpacaEval 2.0; an $83.0$ score on Arena-Hard-Auto; and a $9.55$ score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., $805$ samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",
          "keywords": [
            "Large Language Models",
            "Cheating",
            "Automatic LLM Benchmarks"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We show that null models that always return the same cheating responses can achieve high win rates on automatic LLM benchmarks.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-13",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=syThiTmWWm",
          "pdf_link": "https://openreview.net/pdf?id=syThiTmWWm"
        },
        "paper_internal_id": "syThiTmWWm",
        "category": "oral",
        "embedding_score": 0.6524219512939453,
        "final_score": 0.9189164042472839
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "The Geometry of Categorical and Hierarchical Concepts in Large Language Models",
      "abstract": "The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as _directions_ in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as _vectors_. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.",
      "keywords": [
        "categorical concepts",
        "hierarchical concepts",
        "linear representation hypothesis",
        "causal inner product",
        "interpretability"
      ],
      "primary_area": "interpretability and explainable AI",
      "TLDR": "We extend the linear representation hypothesis to general concepts and show that hierarchical relationships are encoded as orthogonality.",
      "creation_date": "2024-09-24",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Oral",
      "forum_link": "https://openreview.net/forum?id=bVTM2QKYuA",
      "pdf_link": "https://openreview.net/pdf?id=bVTM2QKYuA",
      "label": "oral",
      "conference": "ICLR",
      "paper_id": "bVTM2QKYuA"
    },
    "query_internal_id": "bVTM2QKYuA",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "j0sq9r3HFv",
          "title": "Automated Parameter Extraction for Biologically Realistic Neural Networks: An Initial Exploration with Large Language Models",
          "abstract": "In computational neuroscience, extracting parameters for constructing biologically realistic neural models is a resource-intensive task that requires continuous updates as new research emerges. This paper explores utilizing large language models (LLMs) in automating parameter extraction from scientific literature for biologically realistic neural models. We utilized open-source LLMs via Ollama to construct KGs, capturing parameters such as neuron morphology, synapse dynamics, and receptor properties. SNNBuilder \\cite{Gutierrez2022}, a framework for building spiking neural network (SNN) models, serves as a key validation example for our framework. However, the methodology we outline here can extend beyond SNNs and could applied to systematic modelling of the brain.By experimenting with different prompting strategies—general extraction, in-context hints, and masked prompting—we evaluated the ability of LLMs to autonomously extract relevant data and organize it within an expert-base or data-driven ontology, as well as to infer missing information for neural model construction. Additionally, we implemented retrieval-augmented generation (RAG) via LangChain to further improve the accuracy of parameter extraction through leveraging external knowledge sources. Analysis of the the generated KGs, demonstrated that LLMs, when guided by targeted prompts, can enhance the data-to-model process, paving the way for more efficient parameter extraction and model construction in computational neuroscience.",
          "keywords": [
            "Large Language Models",
            "Knowledge Graphs",
            "Computational neuroscience",
            "Neural model construction"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=j0sq9r3HFv",
          "pdf_link": "https://openreview.net/pdf?id=j0sq9r3HFv"
        },
        "paper_internal_id": "j0sq9r3HFv",
        "category": "reject",
        "embedding_score": 0.6756279468536377,
        "final_score": 0.9410926103591919
      },
      "poster": {
        "paper": {
          "id": "yVGGtsOgc7",
          "title": "Disentangling Representations through Multi-task Learning",
          "abstract": "Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure (\"disentangled\" or \"abstract\" representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along approximately orthogonal directions, thus facilitating feature-based generalization. We provide experimental and theoretical results guaranteeing the emergence of disentangled representations in agents that optimally solve multi-task evidence accumulation classification tasks, canonical in the neuroscience literature. The key conceptual finding is that, by producing accurate multi-task classification estimates, a system implicitly represents a set of coordinates specifying a disentangled representation of the underlying latent state of the data it receives. The theory provides conditions for the emergence of these representations in terms of noise, number of tasks, and evidence accumulation time, when the classification boundaries are affine in the latent space. Surprisingly, the theory also produces closed-form expressions for extracting the disentangled representation from the model's latent state $\\mathbf Z(t)$. We experimentally validate these predictions in RNNs trained on multi-task classification, which learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization in predicting latent factors. We demonstrate the robustness of our framework across autoregressive architectures, decision boundary geometries and in tasks requiring classification confidence estimation. We find that transformers are particularly suited for disentangling representations, which might explain their unique world understanding abilities. Overall, our framework establishes a formal link between competence at multiple tasks and the formation of disentangled, interpretable world models in both biological and artificial systems, and helps explain why ANNs often arrive at human-interpretable concepts, and how they both may acquire exceptional zero-shot generalization capabilities.",
          "keywords": [
            "zero-shot generalization",
            "disentanglement",
            "interpretability",
            "world models",
            "multi-task learning",
            "computational neuroscience",
            "neuroAI",
            "evidence accumulation",
            "cognitive maps",
            "continuous attractors",
            "RNNs",
            "transformers"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We theoretically prove multi-task learning is guaranteed to lead to disentangled, generalizable representations in autoregressive models, and validate our theory on RNNs and transformers performing cognitive neuroscience evidence accumulation tasks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=yVGGtsOgc7",
          "pdf_link": "https://openreview.net/pdf?id=yVGGtsOgc7"
        },
        "paper_internal_id": "yVGGtsOgc7",
        "category": "poster",
        "embedding_score": 0.693954348564148,
        "final_score": 0.9549033045768738
      },
      "spotlight": {
        "paper": {
          "id": "Njx1NjHIx4",
          "title": "Formation of Representations in Neural Networks",
          "abstract": "Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory proving that the balance between gradient noise and regularization is crucial for the emergence of the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.",
          "keywords": [
            "representation learning",
            "neural collapse",
            "neural feature ansatz"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-22",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=Njx1NjHIx4",
          "pdf_link": "https://openreview.net/pdf?id=Njx1NjHIx4"
        },
        "paper_internal_id": "Njx1NjHIx4",
        "category": "spotlight",
        "embedding_score": 0.7176490426063538,
        "final_score": 0.9295922517776489
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation",
      "abstract": "We consider a class of conditional forward-backward diffusion models for conditional generative modeling, that is, generating new data given a covariate (or control variable). To formally study the theoretical properties of these conditional generative models, we adopt a statistical framework of distribution regression to characterize the large sample properties of the conditional distribution estimators induced by these conditional forward-backward diffusion models. Here, the conditional distribution of data is assumed to smoothly change over the covariate. In particular, our derived convergence rate is minimax-optimal under the total variation metric within the regimes covered by the existing literature. Additionally, we extend our theory by allowing both the data and the covariate variable to potentially admit a low-dimensional manifold structure. In this scenario, we demonstrate that the conditional forward-backward diffusion model can adapt to both manifold structures, meaning that the derived estimation error bound (under the Wasserstein metric) depends only on the intrinsic dimensionalities of the data and the covariate.",
      "keywords": [
        "conditional distribution estimation",
        "diffusion models",
        "distribution regression",
        "generative models",
        "manifold",
        "minimax rate"
      ],
      "primary_area": "learning theory",
      "TLDR": "",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-25",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=NltQraRnbW",
      "pdf_link": "https://openreview.net/pdf?id=NltQraRnbW",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "NltQraRnbW"
    },
    "query_internal_id": "NltQraRnbW",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "FwW3jqchtY",
          "title": "Identifying neural dynamics using interventional state space models",
          "abstract": "Neural circuits produce signals that are complex and nonlinear. To facilitate the understanding of neural dynamics, a popular approach is to fit state space models (SSM) to data and analyze the dynamics of the low-dimensional latent variables. Despite the power of SSM in explaining neural circuit dynamics, it has been shown that these models merely capture statistical associations in the data and cannot be causally interpreted. Therefore, an important research problem is to build models that can predict neural dynamics under causal manipulations. Here, we propose interventional state space models (iSSM), a class of causal models that can predict neural responses to novel perturbations. We draw on recent advances in causal dynamical systems and present theoretical results for the identifiability of iSSM. In simulations of the motor cortex, we show that iSSM can recover the true latents and the underlying dynamics. In addition, we illustrate two applications of iSSM in biological datasets. First, we apply iSSM to a dataset of calcium recordings from ALM neurons in mice during photostimulation and uncover dynamical mechanisms underlying short-term memory. Second, we apply iSSM to a dataset of electrophysiological recordings from macaque dlPFC recordings during micro-stimulation and show that it successfully predicts responses to unseen perturbations.",
          "keywords": [
            "Causal dynamical systems",
            "interventions",
            "state space models",
            "photostimulation",
            "micro-stimulation"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "We develop interventional state space models (iSSM), a class of causal models that can predict neural responses to novel perturbations and identify neural dynamics.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-19",
          "venue": "ICLR 2025 Conference Withdrawn Submission",
          "forum_link": "https://openreview.net/forum?id=FwW3jqchtY",
          "pdf_link": "https://openreview.net/pdf?id=FwW3jqchtY"
        },
        "paper_internal_id": "FwW3jqchtY",
        "category": "reject",
        "embedding_score": 0.6657671332359314,
        "final_score": 0.8835338354110718
      },
      "spotlight": {
        "paper": {
          "id": "4NTrco82W0",
          "title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks",
          "abstract": "Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness.",
          "keywords": [
            "GFlowNet",
            "Generative Models",
            "f-Divergence",
            "Loss Function"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-26",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=4NTrco82W0",
          "pdf_link": "https://openreview.net/pdf?id=4NTrco82W0"
        },
        "paper_internal_id": "4NTrco82W0",
        "category": "spotlight",
        "embedding_score": 0.7350687980651855,
        "final_score": 0.9177835583686829
      },
      "oral": {
        "paper": {
          "id": "LyJi5ugyJx",
          "title": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models",
          "abstract": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512×512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512, narrowing the gap in FID scores with the best existing diffusion models to within 10\\%.",
          "keywords": [
            "continuous-time consistency models",
            "diffusion models",
            "fast sampling"
          ],
          "primary_area": "generative models",
          "TLDR": "2-step continuous-time consistency models reduce the gap to within 10\\% in sample quality (FID) compared to best diffusion models",
          "creation_date": "2024-09-20",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=LyJi5ugyJx",
          "pdf_link": "https://openreview.net/pdf?id=LyJi5ugyJx"
        },
        "paper_internal_id": "LyJi5ugyJx",
        "category": "oral",
        "embedding_score": 0.7205178737640381,
        "final_score": 0.9711571931838989
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs",
      "abstract": "In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious. However, research on sycophancy in visual language models (VLMs) has been scarce. In this work, we extend the exploration of sycophancy from LLMs to VLMs, introducing the MM-SY benchmark to evaluate this phenomenon. We present evaluation results from multiple representative models, addressing the gap in sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic dataset for training and employ methods based on prompts, supervised fine-tuning, and DPO. Our experiments demonstrate that these methods effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess the semantic impact of sycophancy and analyze the attention distribution of visual tokens. Our findings indicate that the ability to prevent sycophancy is predominantly observed in higher layers of the model. The lack of attention to image knowledge in these higher layers may contribute to sycophancy, and enhancing image attention at high layers proves beneficial in mitigating this issue.",
      "keywords": [
        "Multi-modal Model",
        "Visual-Language Model",
        "Sycophancy",
        "Hallucination"
      ],
      "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
      "TLDR": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-14",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=E2PFv7ad3p",
      "pdf_link": "https://openreview.net/pdf?id=E2PFv7ad3p",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "E2PFv7ad3p"
    },
    "query_internal_id": "E2PFv7ad3p",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "qb2QRoE4W3",
          "title": "LLM-Cite: Cheap Fact Verification with Attribution via URL Generation",
          "abstract": "Hallucinations are one of the main issues with Large Language Models (LLMs). This has led to increased interest in automated ways to verify the factuality of LLMs' responses. Existing methods either rely on: (a) search over a knowledge base (KB), which is costly especially if the KB must be updated frequently to keep up with fresh content, (b) LLM's parametric knowledge to fact-check claims, which is cheaper but does not give attribution and is limited to verifying claims related to knowledge acquired during pretraining. In this work, we present LLM-Cite, a cheap and easy to implement method that does not rely on any external search system while still providing attribution and the ability to verify fresh claims. Our key insight is to leverage an LLM to directly generate potential citation URLs for a given claim, and then use entailment checks to verify the claim against content of the URLs (which are fetched on-the-fly). We benchmark LLM-Cite on three datasets containing fresh and non-fresh claims generated by humans and models. We show that LLM-Cite performs comparable or better than existing methods on all categories of claims --- importantly, without sacrificing attribution, or requiring costly external search --- overall LLM-Cite is more than 45x cheaper than a Google Search based approach.",
          "keywords": [
            "Fact Verification",
            "Attribution",
            "Citation",
            "Factuality"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=qb2QRoE4W3",
          "pdf_link": "https://openreview.net/pdf?id=qb2QRoE4W3"
        },
        "paper_internal_id": "qb2QRoE4W3",
        "category": "reject",
        "embedding_score": 0.7449316382408142,
        "final_score": 0.9253177642822266
      },
      "spotlight": {
        "paper": {
          "id": "ztzZDzgfrh",
          "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
          "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the **Knowledge FFNs** in LLMs overemphasize parametric knowledge in the residual stream, while **Copying Heads** fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose **ReDeEP**, a novel method that detects hallucinations by decoupling LLM’s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
          "keywords": [
            "Retrieval-Augmented Generation Hallucination",
            "Hallucination Detection",
            "Mechanistic Interpretability"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We propose ReDeEP for detecting hallucinations in RAG models by decoupling external context and parametric knowledge, and AARF to reduce hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-19",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=ztzZDzgfrh",
          "pdf_link": "https://openreview.net/pdf?id=ztzZDzgfrh"
        },
        "paper_internal_id": "ztzZDzgfrh",
        "category": "spotlight",
        "embedding_score": 0.7838287949562073,
        "final_score": 0.8777793645858765
      },
      "oral": {
        "paper": {
          "id": "SPS6HzVzyt",
          "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
          "abstract": "Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\\textit{despite an initial expected increase}$. We call this phenomenon as the $\\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. \nWe tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.",
          "keywords": [
            "Instruction finetuning",
            "context-vs-parametric reliance"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We highlight a surprising phenomenon, where the context reliance of the model decreases unexpectedly, with instruction finetuning, despite an initial increase.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=SPS6HzVzyt",
          "pdf_link": "https://openreview.net/pdf?id=SPS6HzVzyt"
        },
        "paper_internal_id": "SPS6HzVzyt",
        "category": "oral",
        "embedding_score": 0.7471061944961548,
        "final_score": 0.8964572548866272
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
      "abstract": "We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.",
      "keywords": [
        "Text-to-Image Generation",
        "Interactive Image Generation"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "",
      "creation_date": "2024-09-24",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-01",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=uy31tqVuNo",
      "pdf_link": "https://openreview.net/pdf?id=uy31tqVuNo",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "uy31tqVuNo"
    },
    "query_internal_id": "uy31tqVuNo",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "VAvZ4oinpa",
          "title": "Video Generation with Learned Action Prior",
          "abstract": "Long-term stochastic video generation remains challenging, especially with moving cameras. This scenario introduces complex interactions between camera movement and observed pixels, resulting in intricate spatio-temporal dynamics and partial observability issues. Current approaches often focus on pixel-level image reconstruction, neglecting explicit modeling of camera motion dynamics. Our proposed solution incorporates camera motion or action as an extended part of the observed image state, employing a multi-modal learning framework to simultaneously model both image and action. We introduce three models: (i) Video Generation with Learning Action Prior (VG-LeAP) that treats the image-action pair as an augmented state generated from a single latent stochastic process and uses variational inference to learn the image-action latent prior; (ii) Causal-LeAP, which establishes a causal relationship between action and the observed image frame, and learns a seperate action prior, conditioned on the observed image states along with the image prior; and (iii) RAFI, which integrates the augmented image-action state concept with a conditional flow matching framework, demonstrating that this action-conditioned image generation concept can be extended to other transformer-based architectures. Through comprehensive empirical studies on robotic video dataset, RoAM, we highlight the importance of multi-modal training in addressing partially observable video generation problems.",
          "keywords": [
            "Stochastic Video Generation",
            "Variational Inference"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "We propose variational models for learning action priors for video generation tasks in situations where the camera is also moving like in autonomous cars or robots.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=VAvZ4oinpa",
          "pdf_link": "https://openreview.net/pdf?id=VAvZ4oinpa"
        },
        "paper_internal_id": "VAvZ4oinpa",
        "category": "reject",
        "embedding_score": 0.6520824432373047,
        "final_score": 0.5260350704193115
      },
      "spotlight": {
        "paper": {
          "id": "G6dMvRuhFr",
          "title": "Grounding Video Models to Actions through Goal Conditioned Exploration",
          "abstract": "Large video models, pretrained on massive quantities of amount of Internet video,  provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.\nHowever, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.\nTo tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. \nGathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.\nIn this paper, we investigate how to directly  ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.\nWe propose a framework that uses trajectory level action generation in combination with video guidance to\nenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.\nWe validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. \nWe show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.",
          "keywords": [
            "Embodied AI",
            "Decision Making",
            "Robotics",
            "Video Model"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We illustrate how we can ground video models to actions without using actions labels through goal conditioned exploration.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=G6dMvRuhFr",
          "pdf_link": "https://openreview.net/pdf?id=G6dMvRuhFr"
        },
        "paper_internal_id": "G6dMvRuhFr",
        "category": "spotlight",
        "embedding_score": 0.7159213423728943,
        "final_score": 0.2916761636734009
      },
      "oral": {
        "paper": {
          "id": "RuP17cJtZo",
          "title": "Generator Matching: Generative modeling with arbitrary Markov processes",
          "abstract": "We introduce Generator Matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that Generator Matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it expands the design space to new and unexplored Markov processes such as jump processes. Finally, Generator Matching enables the construction of superpositions of Markov generative models and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on image and multimodal generation, e.g. showing that superposition with a jump process improves performance.",
          "keywords": [
            "Flow matching",
            "Markov process",
            "Diffusion model",
            "Generative Modeling"
          ],
          "primary_area": "generative models",
          "TLDR": "The core principles of flow matching can be vastly generalized to practically all continuous-time Markov processes using Markov generators, unifying all previous methods and opening the door to new generative models agnostic to data modality.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=RuP17cJtZo",
          "pdf_link": "https://openreview.net/pdf?id=RuP17cJtZo"
        },
        "paper_internal_id": "RuP17cJtZo",
        "category": "oral",
        "embedding_score": 0.6544451117515564,
        "final_score": 0.38027486205101013
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "SimPER: A Minimalist Approach to Preference  Alignment without Hyperparameters",
      "abstract": "Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches—even without any hyperparameters or a reference model. For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: https://github.com/tengxiao1/SimPER.",
      "keywords": [
        "Large Language Model",
        "Alignment",
        "RLHF"
      ],
      "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
      "TLDR": "",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-20",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=jfwe9qNqRi",
      "pdf_link": "https://openreview.net/pdf?id=jfwe9qNqRi",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "jfwe9qNqRi"
    },
    "query_internal_id": "jfwe9qNqRi",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "OaORjvWelu",
          "title": "Cost-Efficient Multi-Fidelity Alignment for LLMs",
          "abstract": "Alignment is a critical step in large language model (LLM) post-training. It typically requires human annotations to align the model's output to human preferences, which is prohibitively expensive. This paper proposes a novel approach to reduce the alignment cost.\n Specifically, we consider multiple levels of alignment with different qualities and response-generating costs, which we refer to as multi-fidelity alignment. We develop a new approach to incorporating the varying levels of response quality to train a language model, aiming to reduce the cost of response collection for alignment while maintaining the performance of the language model. We provide theoretical insights and empirical results to support the effectiveness of the proposed multi-fidelity alignment approach. Lastly, we conduct experiments to corroborate the effectiveness of the proposed approach by comparing its performance with the vanilla alignment methods.",
          "keywords": [
            "Multi-Fidelity",
            "Alignment",
            "LLM"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "A LLM alignment approach utilizes different qualities of responses to save the total alignment cost.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=OaORjvWelu",
          "pdf_link": "https://openreview.net/pdf?id=OaORjvWelu"
        },
        "paper_internal_id": "OaORjvWelu",
        "category": "reject",
        "embedding_score": 0.8175809383392334,
        "final_score": 0.974511981010437
      },
      "spotlight": {
        "paper": {
          "id": "4FVGowGzQb",
          "title": "Learning from negative feedback, or positive feedback or both",
          "abstract": "Existing preference optimization methods often assume scenarios where paired preference feedback (preferred/positive vs. dis-preferred/negative examples) is available. This requirement limits their applicability in scenarios where only unpaired feedback—for example, either positive or negative— is available. To address this, we introduce a novel approach that decouples learning from positive and negative feedback. This decoupling enables control over the influence of each feedback type and, importantly, allows learning even when only one feedback type is present. A key contribution is demonstrating stable learning from negative feedback alone, a capability not well-addressed by current methods. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which uses expectation-maximization (EM) to directly optimize the probability of positive outcomes (as opposed to classic expected reward maximization). We address a key limitation in current EM-based methods: they solely maximize the likelihood of positive examples, while neglecting negative ones. We show how to extend EM algorithms to explicitly incorporate negative examples, leading to a theoretically grounded algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned value functions are available.",
          "keywords": [
            "Preference Optimization",
            "Policy Optimization",
            "Negative Feedback",
            "Positive feedback",
            "Reinforcement Learning",
            "Probabilistic Inference"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "A new policy optimization algorithm that learns from different type and number of feedback (positive, negative, or both) to optimize policies.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=4FVGowGzQb",
          "pdf_link": "https://openreview.net/pdf?id=4FVGowGzQb"
        },
        "paper_internal_id": "4FVGowGzQb",
        "category": "spotlight",
        "embedding_score": 0.7326031923294067,
        "final_score": 0.9785555601119995
      },
      "oral": {
        "paper": {
          "id": "vo9t20wsmd",
          "title": "Faster Cascades via Speculative Decoding",
          "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while  speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.",
          "keywords": [
            "Cascades",
            "Speculative Decoding",
            "Speculative execution",
            "LLM",
            "Inference",
            "Adaptive Inference"
          ],
          "primary_area": "generative models",
          "TLDR": "Faster language model cascades through the use of speculative execution",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-03",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=vo9t20wsmd",
          "pdf_link": "https://openreview.net/pdf?id=vo9t20wsmd"
        },
        "paper_internal_id": "vo9t20wsmd",
        "category": "oral",
        "embedding_score": 0.7580616474151611,
        "final_score": 0.9068008661270142
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers",
      "abstract": "The denoising diffusion model has recently emerged as a powerful generative technique, capable of transforming noise into meaningful data. While theoretical convergence guarantees for diffusion models are well established when the target distribution aligns with the training distribution, practical scenarios often present mismatches. One common case is in the zero-shot conditional diffusion sampling, where the target conditional distribution is different from the (unconditional) training distribution. These score-mismatched diffusion models remain largely unexplored from a theoretical perspective. In this paper, we present the first performance guarantee with explicit dimensional dependencies for general score-mismatched diffusion samplers, focusing on target distributions with finite second moments. We show that score mismatches result in an asymptotic distributional bias between the target and sampling distributions, proportional to the accumulated mismatch between the target and training distributions. This result can be directly applied to zero-shot conditional samplers for any conditional model, irrespective of measurement noise. Interestingly, the derived convergence upper bound offers useful guidance for designing a novel bias-optimal zero-shot sampler in linear conditional models that minimizes the asymptotic bias. For such bias-optimal samplers, we further establish convergence guarantees with explicit dependencies on dimension and conditioning, applied to several interesting target distributions, including those with bounded support and Gaussian mixtures. Our findings are supported by numerical studies.",
      "keywords": [
        "generative models",
        "denoising diffusion probabilistic model (DDPM)",
        "convergence analysis",
        "zero-shot conditional sampling",
        "model mismatch"
      ],
      "primary_area": "learning theory",
      "TLDR": "",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-25",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=nWT6LxbuGi",
      "pdf_link": "https://openreview.net/pdf?id=nWT6LxbuGi",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "nWT6LxbuGi"
    },
    "query_internal_id": "nWT6LxbuGi",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "vNZIePda08",
          "title": "Sparse-to-Sparse Training of Diffusion Models",
          "abstract": "Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown  potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and sometimes outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.",
          "keywords": [
            "Diffusion Models",
            "Sparse-to-Sparse Training",
            "Static Sparse Training",
            "Dynamic Sparse Training"
          ],
          "primary_area": "generative models",
          "TLDR": "We introduce sparse-to-sparse training to Diffusion Models, and obtain sparse DMs that are able to match and sometimes outperform the dense versions.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=vNZIePda08",
          "pdf_link": "https://openreview.net/pdf?id=vNZIePda08"
        },
        "paper_internal_id": "vNZIePda08",
        "category": "reject",
        "embedding_score": 0.7591153979301453,
        "final_score": 0.6691885590553284
      },
      "spotlight": {
        "paper": {
          "id": "7BQkXXM8Fy",
          "title": "What Makes a Good Diffusion Planner for Decision Making?",
          "abstract": "Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks. Code: https://github.com/Josh00-Lu/DiffusionVeteran.",
          "keywords": [
            "Diffusion Models",
            "Offline Reinforcement Learning",
            "Decision Making",
            "Planning"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "A comprehensive empirical study about key elements underlying a good diffusion planner for deicsion making.",
          "creation_date": "2024-09-18",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-06",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=7BQkXXM8Fy",
          "pdf_link": "https://openreview.net/pdf?id=7BQkXXM8Fy"
        },
        "paper_internal_id": "7BQkXXM8Fy",
        "category": "spotlight",
        "embedding_score": 0.6793150305747986,
        "final_score": 0.5522467494010925
      },
      "oral": {
        "paper": {
          "id": "DJSZGGZYVi",
          "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
          "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
          "keywords": [
            "Diffusion models",
            "Representation learning"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-25",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=DJSZGGZYVi",
          "pdf_link": "https://openreview.net/pdf?id=DJSZGGZYVi"
        },
        "paper_internal_id": "DJSZGGZYVi",
        "category": "oral",
        "embedding_score": 0.7391553521156311,
        "final_score": 0.31678539514541626
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Towards Improving Exploration through Sibling Augmented GFlowNets",
      "abstract": "Exploration is a key factor for the success of an active learning agent, especially when dealing with sparse extrinsic terminal rewards and long trajectories. We introduce Sibling Augmented Generative Flow Networks (SA-GFN), a novel framework designed to enhance exploration and training efficiency of Generative Flow Networks (GFlowNets). SA-GFN uses a decoupled dual network architecture, comprising of a main Behavior Network and an exploratory Sibling Network, to enable a diverse exploration of the underlying distribution using intrinsic rewards. Inspired by the ideas on exploration from reinforcement learning, SA-GFN provides a general-purpose exploration and learning paradigm that integrates with multiple GFlowNet training objectives and is especially helpful for exploration over a wide range of sparse or low reward distributions and task structures. An extensive set of experiments across a diverse range of tasks, reward structures and trajectory lengths, along with a thorough set of ablations, demonstrate the superior performance of SA-GFN in terms of exploration efficacy and convergence speed as compared to the existing methods. In addition, SA-GFN's versatility and compatibility with different GFlowNet training objectives and intrinsic reward methods underscores its broad applicability in various problem domains.",
      "keywords": [
        "Generative Models",
        "Generative Flow Networks",
        "Exploration"
      ],
      "primary_area": "generative models",
      "TLDR": "",
      "creation_date": "2024-09-13",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=HH4KWP8RP5",
      "pdf_link": "https://openreview.net/pdf?id=HH4KWP8RP5",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "HH4KWP8RP5"
    },
    "query_internal_id": "HH4KWP8RP5",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "gL1cNK2UEW",
          "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation",
          "abstract": "Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning---a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves---to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.",
          "keywords": [
            "curriculum learning",
            "data science agent",
            "long-term memory",
            "online data retrieval"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=gL1cNK2UEW",
          "pdf_link": "https://openreview.net/pdf?id=gL1cNK2UEW"
        },
        "paper_internal_id": "gL1cNK2UEW",
        "category": "reject",
        "embedding_score": 0.6593815088272095,
        "final_score": 0.2535141408443451
      },
      "spotlight": {
        "paper": {
          "id": "G6dMvRuhFr",
          "title": "Grounding Video Models to Actions through Goal Conditioned Exploration",
          "abstract": "Large video models, pretrained on massive quantities of amount of Internet video,  provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.\nHowever, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.\nTo tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. \nGathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.\nIn this paper, we investigate how to directly  ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.\nWe propose a framework that uses trajectory level action generation in combination with video guidance to\nenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.\nWe validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. \nWe show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.",
          "keywords": [
            "Embodied AI",
            "Decision Making",
            "Robotics",
            "Video Model"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We illustrate how we can ground video models to actions without using actions labels through goal conditioned exploration.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=G6dMvRuhFr",
          "pdf_link": "https://openreview.net/pdf?id=G6dMvRuhFr"
        },
        "paper_internal_id": "G6dMvRuhFr",
        "category": "spotlight",
        "embedding_score": 0.6819593906402588,
        "final_score": 0.09634418785572052
      },
      "oral": {
        "paper": {
          "id": "5IkDAfabuo",
          "title": "Prioritized Generative Replay",
          "abstract": "Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. \nHowever, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of \"relevance functions\" that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents. Project page available at: https://pgenreplay.github.io",
          "keywords": [
            "online learning",
            "model-based reinforcement learning",
            "generative modeling",
            "synthetic data",
            "continual learning"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "We construct a conditional generative model of an agent's online memory, allowing us to replay high-priority data at large quantities to accelerate training of online RL agents.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-09",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=5IkDAfabuo",
          "pdf_link": "https://openreview.net/pdf?id=5IkDAfabuo"
        },
        "paper_internal_id": "5IkDAfabuo",
        "category": "oral",
        "embedding_score": 0.7278531789779663,
        "final_score": 0.1595539152622223
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) are thriving through continuous fine-tuning by LLMs. Driven by the law that \"scale is everything\", MLLMs expand their training sets during version iterations. In this paper, we propose a large-scale training data engine built around an evaluating-exploring-evolving (E3) loop. Evaluating the data provides insights into its characteristics. Exploring quality rules helps identify which data enhances training. Together, these processes facilitate the systematic evolution of new, high-quality data. With the E3 loop, we introduce ZooProbe, an efficient data engine for MLLMs. First, the problem of data expansion is formalized as a tree of sampling and growth. ZooProbe introduces a small-scale model *zoo* to obtain comprehensive evaluations for child datasets. From multiple perspectives, visual, textual, and multimodal models cover over 50 dimensions of intrinsic and meta attributes, such as object and topic distribution, and higher-level properties, like annotation quality and scene complexity. ZooProbe constructs based on A$^\\star$ search, modeling the heuristic function as a quality estimate from data evaluation results. It dynamically explores the rule of data quality based on the model state of the *probe* datasets. Additionally, it evolves new targeted data with identified high-quality rules. We also develop an extra heuristic quality ranker with the data utilized and discarded during the expansion. Our experiments show that ZooProbe significantly breaks the scaling law in multimodal instruction fine-tuning at scales of 260$k$ and below.\nZooProbe generates high-quality data that accelerates MLLM training and enhances performance, automating the evolution of large-scale training data.",
      "keywords": [
        "Multimodal Large Language Model",
        "Training Data Engine",
        "Deep Learning"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "ZooProbe: a new training data engine for Multimodal LLMs that features the Evaluating-Exploring-Evolving (E3) loop.",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-01",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=T4LtGj7us1",
      "pdf_link": "https://openreview.net/pdf?id=T4LtGj7us1",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "T4LtGj7us1"
    },
    "query_internal_id": "T4LtGj7us1",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "8QTpYC4smR",
          "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
          "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries.",
          "keywords": [
            "Large Language Models",
            "Systematic Review"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=8QTpYC4smR",
          "pdf_link": "https://openreview.net/pdf?id=8QTpYC4smR"
        },
        "paper_internal_id": "8QTpYC4smR",
        "category": "reject",
        "embedding_score": 0.7560979723930359,
        "final_score": 0.9731599688529968
      },
      "spotlight": {
        "paper": {
          "id": "k3gCieTXeY",
          "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
          "abstract": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.\nOur novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.",
          "keywords": [
            "evaluation",
            "multilinguality",
            "large language models"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=k3gCieTXeY",
          "pdf_link": "https://openreview.net/pdf?id=k3gCieTXeY"
        },
        "paper_internal_id": "k3gCieTXeY",
        "category": "spotlight",
        "embedding_score": 0.73268723487854,
        "final_score": 0.9493014812469482
      },
      "oral": {
        "paper": {
          "id": "kxnoqaisCT",
          "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents",
          "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20\\% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.",
          "keywords": [
            "GUI Agents",
            "Visual Grounding",
            "Multimodal Large Language Models",
            "GUI Grounding",
            "Large Language Model"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-16",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=kxnoqaisCT",
          "pdf_link": "https://openreview.net/pdf?id=kxnoqaisCT"
        },
        "paper_internal_id": "kxnoqaisCT",
        "category": "oral",
        "embedding_score": 0.7294529676437378,
        "final_score": 0.9738794565200806
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "uvTea5Rfek",
      "title": "Extracting task-relevant preserved dynamics from contrastive aligned neural recordings",
      "abstract": "Recent work indicates that low-dimensional dynamics of neural and behavioral data are often preserved across days and subjects. However, extracting these preserved dynamics remains challenging: high-dimensional neural population activity and the recorded neuron populations vary across recording sessions. While existing modeling tools can improve alignment between neural and behavioral data, they often operate on a per-subject basis or discretize behavior into categories, disrupting its natural continuity and failing to capture the underlying dynamics. We introduce $\\underline{\\text{C}}$ontrastive $\\underline{\\text{A}}$ligned $\\underline{\\text{N}}$eural $\\underline{\\text{D}}$$\\underline{\\text{Y}}$namics (CANDY), an end‑to‑end framework that aligns neural and behavioral data using rank-based contrastive learning, adapted for continuous behavioral variables, to project neural activity from different sessions onto a shared low-dimensional embedding space. CANDY fits a shared linear dynamical system to the aligned embeddings, enabling an interpretable model of the conserved temporal structure in the latent space. We validate CANDY on synthetic and real-world datasets spanning multiple species, behaviors, and recording modalities. Our results show that CANDY is able to learn aligned latent embeddings and preserved dynamics across neural recording sessions and subjects, and it achieves improved cross-session behavior decoding performance. We further show that the latent linear dynamical system generalizes to new sessions and subjects, achieving comparable or even superior behavior decoding performance to models trained from scratch. These advances enable robust cross‑session behavioral decoding and offer a path towards identifying shared neural dynamics that underlie behavior across individuals and recording conditions. The code and two-photon imaging data of striatal neural activity that we acquired here are available at https://github.com/schnitzer-lab/CANDY-public.git.",
      "keywords": "['neural latent embedding', 'neural latent dynamics', 'contrastive learning']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "uvTea5Rfek",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "jCbbI78q4k",
          "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics",
          "abstract": "Behavioral Foundation Models (BFMs) proved successful in producing policies for arbitrary tasks in a zero-shot manner, requiring no test-time training or task-specific fine-tuning. Among the most promising BFMs are the ones that estimate the successor measure learned in an unsupervised way from task-agnostic offline data. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward–Backward (FB) representation, one of the methods from the BFM family, cannot distinguish between distinct dynamics, leading to an interference among the latent directions, which parametrize different policies. To address this, we propose a FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. We also show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. These traits allow our method to respond to the dynamics observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.",
          "keywords": [
            "zero-shot reinforcement learning",
            "unsupervised reinforcement learning",
            "successor measure"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "We provide both theoretical and empirical evidence that Forward–Backward representations cannot adapt to changing dynamics and introduce a method that overcomes this, generalizing to both seen and unseen dynamics.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=jCbbI78q4k",
          "pdf_link": "https://openreview.net/pdf?id=jCbbI78q4k"
        },
        "paper_internal_id": "jCbbI78q4k",
        "category": "reject",
        "embedding_score": 0.7564650774002075,
        "final_score": 0.4617255628108978
      },
      "poster": {
        "paper": {
          "id": "BE6QmLdJqY",
          "title": "Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling",
          "abstract": "Diffusion models, though originally designed for generative tasks, have demonstrated impressive self-supervised representation learning capabilities. A particularly intriguing phenomenon in these models is the emergence of unimodal representation dynamics, where the quality of learned features peaks at an intermediate noise level. In this work, we conduct a comprehensive theoretical and empirical investigation of this phenomenon. Leveraging the inherent low-dimensionality structure of image data, we theoretically demonstrate that the unimodal dynamic emerges when the diffusion model successfully captures the underlying data distribution. The unimodality arises from an interplay between denoising strength and class confidence across noise scales. Empirically, we further show that, in classification tasks, the presence of unimodal dynamics reliably reflects the diffusion model’s generalization: it emerges when the model generate novel images and gradually transitions to a monotonically decreasing curve as the model begins to memorize the training data.",
          "keywords": [
            "diffusion model",
            "representation learning"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=BE6QmLdJqY",
          "pdf_link": "https://openreview.net/pdf?id=BE6QmLdJqY"
        },
        "paper_internal_id": "BE6QmLdJqY",
        "category": "poster",
        "embedding_score": 0.7345325350761414,
        "final_score": 0.7945894598960876
      },
      "oral": {
        "paper": {
          "id": "cGks3s79hW",
          "title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
          "abstract": "Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.",
          "keywords": [
            "recurrent neural networks",
            "neuronal recordings",
            "visual cortex",
            "latent variable models",
            "PCA",
            "eigenvalue decay",
            "mean-field limit"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "We show that high-dimensional neural activity can arise from low-dimensional latent dynamics, both in RNNs and in the brain.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=cGks3s79hW",
          "pdf_link": "https://openreview.net/pdf?id=cGks3s79hW"
        },
        "paper_internal_id": "cGks3s79hW",
        "category": "oral",
        "embedding_score": 0.8267462253570557,
        "final_score": 0.848797082901001
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "XmV7KRABBl",
      "title": "EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks",
      "abstract": "Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection.\nHowever, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges.\nFirst, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. \nSecond, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance.\nTo address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.\nBuilding on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights.\nMoreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time.\nOur contributions include \n(a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, \n(b) a novel and efficient model that significantly improves AUROC by 23\\% and F1 score by 30\\%, compared with the dynamic GNN baseline, and \n(c) broad evaluation of our method on the challenging early seizure prediction task.",
      "keywords": "['EEG', 'graph', 'neuroscience', 'seizure', 'dynamic modeling', 'expressiveness']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "XmV7KRABBl",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "ok3iIeuQV4",
          "title": "TDFormer: Top-Down Attention-Controlled Spiking Transformer",
          "abstract": "Traditional spiking neural networks (SNNs) can be viewed as a combination of multiple subnetworks with each running for one time step, where the parameters are shared, and the membrane potential serves as the only information link between them. However, the implicit nature of the membrane potential limits its ability to effectively represent temporal information. As a result, each time step cannot fully leverage information from previous time steps, seriously limiting the model's performance. Inspired by the top-down mechanism in the brain, we introduce TDformer, a novel model with a top-down feedback structure that functions hierarchically and leverages high-order representations from earlier time steps to modulate the processing of low-order information at later stages. The feedback structure plays a role from two perspectives: 1) During forward propagation, our model increases the mutual information across time steps, indicating that richer temporal information is being transmitted and integrated in different time steps. 2) During backward propagation, we theoretically prove that the feedback structure alleviates the problem of vanishing gradients along the time dimension. We find that these mechanisms together significantly and consistently improve the model performance on multiple datasets. In particular, our model achieves state-of-the-art performance on ImageNet with an accuracy of 86.83\\%.",
          "keywords": [
            "Top-down mechanism",
            "Brain-inspired Computing",
            "Spiking Neural Networks",
            "Vision Transformers"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=ok3iIeuQV4",
          "pdf_link": "https://openreview.net/pdf?id=ok3iIeuQV4"
        },
        "paper_internal_id": "ok3iIeuQV4",
        "category": "reject",
        "embedding_score": 0.7498350143432617,
        "final_score": 0.7680318355560303
      },
      "poster": {
        "paper": {
          "id": "v13yQBxhut",
          "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics",
          "abstract": "In recent years, the expressive power of various neural architectures---including graph neural networks (GNNs), transformers, and recurrent neural networks---has been characterised using tools from logic and formal language theory. As the capabilities of basic architectures are becoming well understood, increasing attention is turning to models that combine multiple architectural paradigms. Among them particularly important, and challenging to analyse, are temporal extensions of GNNs, which integrate both spatial (graph-structure) and temporal (evolution over time) dimensions. In this paper, we initiate the study of logical characterisation of temporal GNNs by connecting them to two-dimensional product logics. We show that the expressive power of temporal GNNs depends on how graph and temporal components are combined. In particular, temporal GNNs that apply static GNNs recursively over time can capture all properties definable in the product logic of (past) propositional temporal logic PTL and the modal logic K. In contrast, architectures such as graph-and-time TGNNs and global TGNNs can only express restricted fragments of this  logic, where the interaction between temporal and spatial operators is syntactically constrained. These provide us with the first results on the logical expressiveness of temporal GNNs.",
          "keywords": [
            "temporal graph neural networks",
            "temporal logic",
            "expressiveness"
          ],
          "primary_area": "theory",
          "TLDR": "We analyse the expressive power of different classes of TGNNs via standard combinations of temporal and modal logics.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=v13yQBxhut",
          "pdf_link": "https://openreview.net/pdf?id=v13yQBxhut"
        },
        "paper_internal_id": "v13yQBxhut",
        "category": "poster",
        "embedding_score": 0.7073227167129517,
        "final_score": 0.9970809817314148
      },
      "oral": {
        "paper": {
          "id": "ImpizBSKcu",
          "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
          "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm.  We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$,\nand large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies:\n$(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network;\n$(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity;\n$(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated  `feature unlearning' regime at large times.",
          "keywords": [
            "Overfitting; feature learning; dynamical mean field theory; generalization;"
          ],
          "primary_area": "theory",
          "TLDR": "Large neural networks first learn low dimensional feature representation then overfit the data and revert to a kernel regime.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=ImpizBSKcu",
          "pdf_link": "https://openreview.net/pdf?id=ImpizBSKcu"
        },
        "paper_internal_id": "ImpizBSKcu",
        "category": "oral",
        "embedding_score": 0.6877881288528442,
        "final_score": 0.7982545495033264
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "3Wrv6Zay74",
      "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs",
      "abstract": "Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley's algorithm. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.",
      "keywords": "['deep learning', 'quantization', 'LLM', 'hadamard']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "3Wrv6Zay74",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "IZHWvvmYwx",
          "title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?",
          "abstract": "The ability to recognize patterns from examples and apply them to new ones is a primal ability for general intelligence, and is widely studied by psychology and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually <10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations often focus on classification, and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context reasoning benchmark for pattern recognition that asks LLM to predict output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for many-shot in-context reasoning, and acquired many insightful findings including scaling effect, robustness, inductive vs. transductive reasoning, retrieval Augmented Generation (RAG), coding for inductive reasoning, cross-domain generalizability, etc. Our dataset is available at https://huggingface.co/datasets/kaiyan289/MIR-Bench.",
          "keywords": [
            "pattern recognition",
            "reasoning",
            "many-shot in-context learning",
            "large language model",
            "benchmark"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "We propose a novel benchmark for pattern recognition for many-shot in-context learning for large language models and conduct extensive empirical analysis with many insights.",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track poster",
          "forum_link": "https://openreview.net/forum?id=IZHWvvmYwx",
          "pdf_link": "https://openreview.net/pdf?id=IZHWvvmYwx"
        },
        "paper_internal_id": "IZHWvvmYwx",
        "category": "poster",
        "embedding_score": 0.7484360337257385,
        "final_score": 0.8906162977218628
      },
      "spotlight": {
        "paper": {
          "id": "3k70Vt0YFS",
          "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code",
          "abstract": "Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement genuinely novel ideas from recent research papers—ideas unseen during pretraining—remains unclear. We introduce ResearchCodeBench, a benchmark that evaluates LLMs’ ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.",
          "keywords": [
            "Machine learning benchmarks",
            "Code generation",
            "Large language models",
            "Research automation"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=3k70Vt0YFS",
          "pdf_link": "https://openreview.net/pdf?id=3k70Vt0YFS"
        },
        "paper_internal_id": "3k70Vt0YFS",
        "category": "spotlight",
        "embedding_score": 0.7478840947151184,
        "final_score": 0.907738447189331
      },
      "oral": {
        "paper": {
          "id": "KnqiC0znVF",
          "title": "Large Language Diffusion Models",
          "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}.",
          "keywords": [
            "diffusion language models",
            "large language models",
            "masked diffusion models",
            "discrete diffusion models",
            "diffusion models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We present LLaDA, a diffusion  language model trained from scratch that is competitive to LLaMA 3 in performance.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=KnqiC0znVF",
          "pdf_link": "https://openreview.net/pdf?id=KnqiC0znVF"
        },
        "paper_internal_id": "KnqiC0znVF",
        "category": "oral",
        "embedding_score": 0.7501115202903748,
        "final_score": 0.8835734128952026
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "MeOTBs8BQV",
      "title": "Sync or Sink: Bounds on Algorithmic Collective Action with Noise and Multiple Groups",
      "abstract": "Collective action against algorithmic systems, which enables groups to promote their own interests, is poised to grow. Hence, there will be growth in the size and the number of distinct collectives. Currently, there is no formal analysis of how coordination challenges within a collective can impact downstream outcomes, or how multiple collectives may affect each other's success. In this work, we aim to provide guarantees on the success of collective action in the presence of both coordination noise and multiple groups. Our insight is that data generated by either multiple collectives or by coordination noise can be viewed as originating from multiple data distributions. \nUsing this framing, we derive bounds on the success of collective action. We conduct experiments to study the effects of noise on collective action. We find that sufficiently high levels of noise can reduce the success of collective action. In certain scenarios, large noise can sink a collective success rate from $100$% to just under $60$%. We identify potential trade-offs between collective size and coordination noise; for example, a collective that is twice as big but with four times more noise experiencing worse outcomes than the smaller, more coordinated one. This work highlights the importance of understanding nuanced dynamics of strategic behavior in algorithmic systems.",
      "keywords": "['Algorithmic Collective Action', 'Social Computing', 'Data Campaigns']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "MeOTBs8BQV",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "kNXTUCksnh",
          "title": "The Burden of Interactive Alignment with Inconsistent Preferences",
          "abstract": "From media platforms to chatbots, algorithms shape how people interact, learn, and discover information. Such interactions between users and an algorithm often unfold over multiple steps, during which strategic users can guide the algorithm to better align with their true interests by selectively engaging with content. However, users frequently exhibit inconsistent preferences: they may spend considerable time on content that offers little long-term value, inadvertently signaling that such content is desirable. Focusing on the user side, this raises a key question: what does it take for such users to align the algorithm with their true interests?\n\nTo investigate these dynamics, we model the user’s decision process as split between a rational \"system 2\" that decides whether to engage and an impulsive \"system 1\" that determines how long engagement lasts. We then study a multi-leader, single-follower extensive Stackelberg game, where users, specifically system 2, lead by committing to engagement strategies and the algorithm best-responds based on observed interactions. We define the burden of alignment as the minimum horizon over which users must optimize to effectively steer the algorithm. We show that a critical horizon exists: users who are sufficiently foresighted can achieve alignment, while those who are not are instead aligned to the algorithm’s objective. This critical horizon can be long, imposing a substantial burden. However, even a small, costly signal (e.g., an extra click) can significantly reduce it. Overall, our framework explains how users with inconsistent preferences can align an engagement-driven algorithm with their interests in a Stackelberg equilibrium, highlighting both the challenges and potential remedies for achieving alignment.",
          "keywords": [
            "Human AI Interaction",
            "Alignment",
            "Strategic Behavior",
            "Stackelberg Equilibrium"
          ],
          "primary_area": "theory",
          "TLDR": "Our framework explains how users with inconsistent preferences can align an engagement-driven algorithm with their interests in equilibrium, highlighting both the challenges and potential remedies for achieving alignment.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=kNXTUCksnh",
          "pdf_link": "https://openreview.net/pdf?id=kNXTUCksnh"
        },
        "paper_internal_id": "kNXTUCksnh",
        "category": "poster",
        "embedding_score": 0.7455974221229553,
        "final_score": 0.6967713832855225
      },
      "spotlight": {
        "paper": {
          "id": "XBMjXb6f4w",
          "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D",
          "abstract": "AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI’s Inspect framework and we make our code publicly available.",
          "keywords": [
            "Sabotage Evaluations",
            "Sandbagging",
            "AI Control",
            "AI Safety",
            "AI Alignment",
            "Dangerous Capability Evals"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We evaluate frontier LM agents' capabilities to sabotage and sandbag ML engineering tasks without being detected by automated monitors.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=XBMjXb6f4w",
          "pdf_link": "https://openreview.net/pdf?id=XBMjXb6f4w"
        },
        "paper_internal_id": "XBMjXb6f4w",
        "category": "spotlight",
        "embedding_score": 0.6860913038253784,
        "final_score": 0.28548118472099304
      },
      "oral": {
        "paper": {
          "id": "WCRPgBpbcA",
          "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
          "abstract": "In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number $N$ of tokens is large and the inverse temperature parameter $\\beta$ of the model scales together with $N$. In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations.",
          "keywords": [
            "mean-field limits",
            "moderate interaction",
            "mean-field transformers",
            "self-attention models",
            "clustering",
            "multiscale"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-01",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=WCRPgBpbcA",
          "pdf_link": "https://openreview.net/pdf?id=WCRPgBpbcA"
        },
        "paper_internal_id": "WCRPgBpbcA",
        "category": "oral",
        "embedding_score": 0.6826198101043701,
        "final_score": 0.10150174796581268
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "kUzBGEuu7w",
      "title": "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "abstract": "In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a novel family of discrete diffusion models for language modeling. HDLM builds on a hierarchical vocabulary where low-level tokens with detailed semantics are surjectively mapped to high-level tokens with coarse-grained meanings. In the forward process, each token is independently perturbed to its higher-level ancestor with more abstract semantics according to the scheduler, while in the reverse process the model progressively predicts the next, more detailed semantics. Taken together, HDLM provides a general time-varying next semantic scale prediction process for language modeling. We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner while including the existing MDLM as a special case. We also propose practical training techniques based on the insights. Extensive text generation experiments validate the effectiveness of HDLM, which demonstrates consistently lower validation and generative perplexity than baselines.",
      "keywords": "['Discrete Diffusion Model', 'Theory for Discrete Diffusion', 'Diffusion Language Modeling']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "kUzBGEuu7w",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "hncKzAnyKQ",
          "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
          "abstract": "While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the “aha moment”, their ability to generate informative critiques and refine prior solutions remains limited. In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-criticism and iterative refinement of their previous solutions. By fine-tuning on a curated DC-1.7K dataset of 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques. We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs. Notably, Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4\\% to 18.2\\% compared to the original long-CoT LLMs. These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique.",
          "keywords": [
            "Large Language Models",
            "Reasoning",
            "Long Chain-of-Thought",
            "Critique"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=hncKzAnyKQ",
          "pdf_link": "https://openreview.net/pdf?id=hncKzAnyKQ"
        },
        "paper_internal_id": "hncKzAnyKQ",
        "category": "reject",
        "embedding_score": 0.6799614429473877,
        "final_score": 0.6835947036743164
      },
      "spotlight": {
        "paper": {
          "id": "FLiMxTkIeu",
          "title": "AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios",
          "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from $50$ real-world agentic applications. (2) Long, averaging $1,723$ words with a maximum of $15,630$ words. (3) Complex, averaging $11.9$ constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints.\nTo construct AgentIF, we collect $707$ human-annotated instructions across $50$ agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation.\nWe use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.",
          "keywords": [
            "evaluation",
            "instruction following",
            "agent",
            "large language models"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "We propose a benchmark to evaluate the large language models' instruction following ability in agentic scenarios.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=FLiMxTkIeu",
          "pdf_link": "https://openreview.net/pdf?id=FLiMxTkIeu"
        },
        "paper_internal_id": "FLiMxTkIeu",
        "category": "spotlight",
        "embedding_score": 0.7024378776550293,
        "final_score": 0.9333674311637878
      },
      "oral": {
        "paper": {
          "id": "NM8Apk61NA",
          "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
          "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Code is available at \\url{https://github.com/godlin-sjtu/HyperET}.",
          "keywords": [
            "Efficient Training",
            "Multi-modal Large Language Models",
            "Granularity Levels",
            "Hyperbolic Space"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-18",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=NM8Apk61NA",
          "pdf_link": "https://openreview.net/pdf?id=NM8Apk61NA"
        },
        "paper_internal_id": "NM8Apk61NA",
        "category": "oral",
        "embedding_score": 0.7676099538803101,
        "final_score": 0.2283463478088379
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "9GsgCUJtic",
      "title": "When do GFlowNets learn the right distribution?",
      "abstract": "Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols.",
      "keywords": "['GFlowNets']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "9GsgCUJtic",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "tL8dpJmECp",
          "title": "Improving Fairness and Mitigating MADness in Generative Models",
          "abstract": "Generative models unfairly penalize data belonging to minority classes, suffer from model autophagy disorder (MADness), and learn biased estimates of the underlying distribution parameters.  Our theoretical and empirical results show that training generative models with intentionally designed hypernetworks leads to models that 1) are more fair when generating datapoints belonging to minority classes 2) are more stable in a self-consumed (i.e., MAD) setting, and 3) learn parameters that are less statistically biased.  To further mitigate unfairness, MADness, and bias, we introduce a regularization term that penalizes discrepancies between a generative model’s estimated weights when trained on real data versus its own synthetic data.  To facilitate training existing deep generative models within our framework, we offer a scalable implementation of hypernetworks that automatically generates a hypernetwork architecture for any given generative model.",
          "keywords": [
            "Hypernetworks",
            "Generative Models",
            "Fairness",
            "MADness",
            "Maximum Likelihood Estimation",
            "Bias"
          ],
          "primary_area": "generative models",
          "TLDR": "We show how hypernetwork training leads to more fair, less biased, and less MAD generative models.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=tL8dpJmECp",
          "pdf_link": "https://openreview.net/pdf?id=tL8dpJmECp"
        },
        "paper_internal_id": "tL8dpJmECp",
        "category": "reject",
        "embedding_score": 0.7638751268386841,
        "final_score": 0.8823531270027161
      },
      "poster": {
        "paper": {
          "id": "Xj66fkrlTk",
          "title": "Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization",
          "abstract": "Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects with probabilities proportional to a given reward function. The key concept behind GFlowNets is the use of two stochastic policies: a forward policy, which incrementally constructs compositional objects, and a backward policy, which sequentially deconstructs them. Recent results show a close relationship between GFlowNet training and entropy-regularized reinforcement learning (RL) problems with a particular reward design. However, this connection applies only in the setting of a fixed backward policy, which might be a significant limitation. As a remedy to this problem, we introduce a simple backward policy optimization algorithm that involves direct maximization of the value function in an entropy-regularized Markov Decision Process (MDP) over intermediate rewards. We provide an extensive experimental evaluation of the proposed approach across various benchmarks in combination with both RL and GFlowNet algorithms and demonstrate its faster convergence and mode discovery in complex environments.",
          "keywords": [
            "generative flow networks",
            "gflownets",
            "reinforcement learning",
            "sampling",
            "generative models"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "backward policy optimization algorithm for GFlowNets that directly maximizes value in an entropy-regularized MDP",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=Xj66fkrlTk",
          "pdf_link": "https://openreview.net/pdf?id=Xj66fkrlTk"
        },
        "paper_internal_id": "Xj66fkrlTk",
        "category": "poster",
        "embedding_score": 0.7765791416168213,
        "final_score": 0.9943243265151978
      },
      "oral": {
        "paper": {
          "id": "RuP17cJtZo",
          "title": "Generator Matching: Generative modeling with arbitrary Markov processes",
          "abstract": "We introduce Generator Matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that Generator Matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it expands the design space to new and unexplored Markov processes such as jump processes. Finally, Generator Matching enables the construction of superpositions of Markov generative models and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on image and multimodal generation, e.g. showing that superposition with a jump process improves performance.",
          "keywords": [
            "Flow matching",
            "Markov process",
            "Diffusion model",
            "Generative Modeling"
          ],
          "primary_area": "generative models",
          "TLDR": "The core principles of flow matching can be vastly generalized to practically all continuous-time Markov processes using Markov generators, unifying all previous methods and opening the door to new generative models agnostic to data modality.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=RuP17cJtZo",
          "pdf_link": "https://openreview.net/pdf?id=RuP17cJtZo"
        },
        "paper_internal_id": "RuP17cJtZo",
        "category": "oral",
        "embedding_score": 0.7254590392112732,
        "final_score": 0.8329915404319763
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "xak8c9l1nu",
      "title": "Computational Explorations of Total Variation Distance",
      "abstract": "We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance.\nFirst, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.\nThis corresponds to a special case, whereby the TV distance between the two distributions is zero.\nSecond, we prove that unless $\\mathsf{NP} \\subseteq \\mathsf{RP}$ it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting.",
      "keywords": "['total variation distance', 'TV distance', 'mixtures of products', 'equivalence checking', 'Ising models', 'computational complexity', 'FPRAS']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "xak8c9l1nu",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "CrOHzVtWmH",
          "title": "Relative-Translation Invariant Wasserstein Distance",
          "abstract": "In many real-world applications, data distributions are often subject to translation shifts caused by various factors such as changes in environmental conditions, sensor settings, or shifts in data collection practices. These distribution shifts pose a significant challenge for measuring the similarity between probability distributions, particularly in tasks like domain adaptation or transfer learning. To address this issue, we introduce a new family of distances, relative-translation invariant Wasserstein distances ($RW_p$), to measure the similarity of two probability distributions under distribution shift. Generalizing it from the classical optimal transport model, we show that $RW_p$ distances are also real distance metrics defined on the quotient set $\\mathcal{P}_p(\\mathbb{R}^n)/\\sim$ and invariant to distribution translations, which forms a family of new metric spaces. When $p=2$, the $RW_2$ distance enjoys more exciting properties, including decomposability of the optimal transport model and translation-invariance of the $RW_2$ distance. Based on these properties, we show that a distribution shift, measured by $W_2$ distance, can be explained in the bias-variance perspective. In addition, we propose two algorithms: one algorithm is a two-stage optimization algorithm for computing the general case of $RW_p$ distance, and the other is a variant of the Sinkhorn algorithm, named $RW_2$ Sinkhorn algorithm, for efficiently calculating $RW_2$ distance, coupling solutions, as well as $W_2$ distance. We also provide the analysis of numerical stability and time complexity for the proposed algorithms. Finally, we validate the $RW_p$ distance metric and the algorithm performance with two experiments. We conduct one numerical validation for the $RW_2$ Sinkhorn algorithm and demonstrate the effectiveness of using $RW_p$ under distribution shift for similar thunderstorm detection. The experimental results report that our proposed algorithm significantly improves the computational efficiency of Sinkhorn in practical applications, and the $RW_p$ distance is robust to distribution translations.",
          "keywords": [
            "Optimal transport theory",
            "Wasserstein distance",
            "Distribution shift"
          ],
          "primary_area": "optimization",
          "TLDR": "Propose a new family of distances to measure the similarity of two probability distributions under distribution shift.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=CrOHzVtWmH",
          "pdf_link": "https://openreview.net/pdf?id=CrOHzVtWmH"
        },
        "paper_internal_id": "CrOHzVtWmH",
        "category": "reject",
        "embedding_score": 0.7540037631988525,
        "final_score": 0.8185678720474243
      },
      "poster": {
        "paper": {
          "id": "MT3aOfXIbY",
          "title": "Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel",
          "abstract": "Sampling algorithms play an important role in controlling the quality and runtime of diffusion model inference. In recent years, a number of works (Chen et al., 2023c;b; Benton et al., 2023; Lee et al., 2022) have analyzed algorithms for diffusion sampling with provable guarantees; these works show that for essentially any data distribution, one can approximately sample in polynomial time given a sufficiently accurate estimate of its score functions at different noise levels. \n\nIn this work, we propose a new scheme inspired by Shen and Lee's randomized midpoint method for log-concave sampling  (Shen & Lee, 2019). We prove that this approach achieves the best known dimension dependence for sampling from arbitrary smooth distributions in total variation distance ($\\widetilde O(d^{5/12})$ compared to $\\widetilde O(\\sqrt{d})$ from prior work). We also show that our algorithm can be parallelized to run in only $\\widetilde O(\\log^2 d)$ parallel rounds, constituting the first provable guarantees for parallel sampling with diffusion models.\n    \nAs a byproduct of our methods, for the well-studied problem of log-concave sampling in total variation distance, we give an algorithm and simple analysis achieving dimension dependence $\\widetilde O(d^{5/12})$ compared to $\\widetilde O(\\sqrt{d})$ from prior work.",
          "keywords": [
            "Diffusion Sampling",
            "Generative Model",
            "Statistical Theory"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-29",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=MT3aOfXIbY",
          "pdf_link": "https://openreview.net/pdf?id=MT3aOfXIbY"
        },
        "paper_internal_id": "MT3aOfXIbY",
        "category": "poster",
        "embedding_score": 0.7476881742477417,
        "final_score": 0.6140688061714172
      },
      "oral": {
        "paper": {
          "id": "ijbA5swmoK",
          "title": "Second-Order Min-Max Optimization with Lazy Hessians",
          "abstract": "This paper studies second-order methods for convex-concave minimax optimization.  \nMonteiro & Svaiter (2012)  proposed a method to solve the problem with an optimal iteration complexity of \n$\\mathcal{O}(\\epsilon^{-3/2})$ to find an $\\epsilon$-saddle point.  However, it is unclear whether the\ncomputational complexity, $\\mathcal{O}((N+ d^2) d \\epsilon^{-2/3})$, can be improved. In the above, we follow  Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as $N$ and the complexity of obtaining a second-order oracle as $dN$. \nIn this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of $\\tilde{\\mathcal{O}}( (N+d^2)(d+ d^{2/3}\\epsilon^{-2/3}))$, which improves those of previous methods by a factor of $d^{1/3}$. \nFurthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \\kappa^{2/3}) )$ when the condition number of the problem is $\\kappa$, enjoying a similar speedup upon the state-of-the-art method. \nNumerical experiments on both real and synthetic datasets also verify the efficiency of our method.",
          "keywords": [
            "min-max optimization; second-order methods; computational complexity"
          ],
          "primary_area": "optimization",
          "TLDR": "We propose novel second-order methods for min-max optimization that are provably better than existing optimal methods",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=ijbA5swmoK",
          "pdf_link": "https://openreview.net/pdf?id=ijbA5swmoK"
        },
        "paper_internal_id": "ijbA5swmoK",
        "category": "oral",
        "embedding_score": 0.6879113912582397,
        "final_score": 0.12585891783237457
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "MFZjrTFE7h",
      "title": "D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement",
      "abstract": "We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GO-LSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and models: https://github.com/Peterande/D-FINE.",
      "keywords": "['Object Detection', 'Real-Time', 'Detection Transformer', 'Knowledge Distillation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "MFZjrTFE7h",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "9GJ6JKoCVp",
          "title": "NaN Pooling and Convolution Accelerate U-Nets",
          "abstract": "Recent advancements in deep learning for neuroimaging have resulted in the development of increasingly complex models designed for a wide range of tasks. Despite significant improvements in hardware, enhancing inference and training times for these models remains crucial. Through a numerical analysis of convolutional neural networks (CNNs) inference, we found that a substantial amount of operations in these models are applied to pure numerical noise, with little to no impact on the final output. As a result, some CNNs consume up to two-thirds of their floating-point operations unnecessarily.\n\nTo address this inefficiency, we introduce NaN Pooling & Convolution---novel variations of PyTorch's max pooling and 2D convolution operations. These techniques identify numerically unstable voxels and replace them with NaNs, allowing  models to bypass operations on irrelevant data. We evaluate NaN Pooling and Convolution on two models: the FastSurfer CNN, a widely used neuroimaging tool, and a CNN designed to classify the MNIST dataset. For FastSurfer, our approach significantly improves computational efficiency, skipping between 33.24% and 69.30\\% of convolutions in certain layers while preserving the model's original accuracy. On MNIST, our approach skips up to 28.38% of convolutions, again without major impact on the accuracy.",
          "keywords": [
            "Pooling",
            "Convolutions",
            "Deep learning",
            "Optimization",
            "Neuroimaging",
            "Convolutional Neural Networks",
            "Numerical Analysis"
          ],
          "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
          "TLDR": "We introduce NaN Pooling and Convolution, techniques that enhance CNN efficiency in neuroimaging by skipping operations on numerically unstable voxels, reducing computational load by up to two-thirds while maintaining model accuracy.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=9GJ6JKoCVp",
          "pdf_link": "https://openreview.net/pdf?id=9GJ6JKoCVp"
        },
        "paper_internal_id": "9GJ6JKoCVp",
        "category": "reject",
        "embedding_score": 0.6622093319892883,
        "final_score": 0.9436437487602234
      },
      "poster": {
        "paper": {
          "id": "ziw5bzg2NO",
          "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding",
          "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly expanded their utility in tasks like image captioning and visual question answering. However, they still struggle with object hallucination, where models generate descriptions that inaccurately reflect the visual content by including nonexistent objects or misrepresenting existing ones. While previous methods, such as data augmentation and training-free approaches, strive to tackle this issue, they still encounter scalability challenges and often depend on additional external modules. In this work, we propose Ensemble Decoding (ED), a novel strategy that splits the input image into sub-images and combines logit distributions by assigning weights through the attention map. Furthermore, we introduce ED adaptive plausibility constraint to calibrate logit distribution and FastED, a variant designed for speed-critical applications. Extensive experiments across hallucination benchmarks demonstrate that our proposed method achieves state-of-the-art performance, validating the effectiveness of our approach.",
          "keywords": [
            "Hallucination",
            "Multimodal Hallucination",
            "Large Vision-Language Model"
          ],
          "primary_area": "generative models",
          "TLDR": "We introduce Ensemble Decoding (ED), a method designed to mitigate object hallucination in Large Vision-Language Models by dividing an image into sub-images and combining logit distributions with attention-guided weights to improve accuracy.",
          "creation_date": "2024-09-19",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=ziw5bzg2NO",
          "pdf_link": "https://openreview.net/pdf?id=ziw5bzg2NO"
        },
        "paper_internal_id": "ziw5bzg2NO",
        "category": "poster",
        "embedding_score": 0.668097198009491,
        "final_score": 0.9468991756439209
      },
      "oral": {
        "paper": {
          "id": "P4o9akekdf",
          "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images",
          "abstract": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. Code and trained models are available at https://noposplat.github.io/.",
          "keywords": [
            "3D Gaussian Splatting",
            "Pose Free",
            "Pose Estimation",
            "Novel View Synthesis",
            "3D Reconstruction"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "NoPoSplat is a novel feed-forward model that reconstructs scenes from unposed images by predicting Gaussians in a canonical space, demonstrating superior performance in both novel view synthesis and pose estimation.",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-30",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=P4o9akekdf",
          "pdf_link": "https://openreview.net/pdf?id=P4o9akekdf"
        },
        "paper_internal_id": "P4o9akekdf",
        "category": "oral",
        "embedding_score": 0.6876024007797241,
        "final_score": 0.862244725227356
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "LCk3umTAXx",
      "title": "Gamified crowd-sourcing of high-quality data for visual fine-tuning",
      "abstract": "This paper introduces gamified adversarial prompting (GAP), a framework that\ncrowd-sources high-quality data for visual instruction tuning of large multimodal\nmodels. GAP transforms the data collection process into an engaging game, in-\ncentivizing players to provide fine-grained, challenging questions and answers\nthat target gaps in the model’s knowledge. Our contributions include (1) an ap-\nproach to capture question-answer pairs from humans that directly address weak-\nnesses in a model’s knowledge, (2) a method for evaluating and rewarding players\nthat successfully incentivizes them to provide high-quality submissions, and (3) a\nscalable, gamified platform that succeeds in collecting this data from over 50,000\nparticipants in just a few weeks. Our implementation of GAP has significantly im-\nproved the accuracy of a small multimodal model, namely MiniCPM-Llama3-V-\n2.5-8B, increasing its GPT score from 0.147 to 0.477 on our dataset, approaching\nthe benchmark set by the much larger GPT-4V. Moreover, we demonstrate that\nthe data generated using MiniCPM-Llama3-V-2.5-8B also enhances its perfor-\nmance across other benchmarks, and exhibits cross-model benefits. Specifically,\nthe same data improves the performance of QWEN2-VL-2B and QWEN2-VL-7B\non the same multiple benchmarks.",
      "keywords": "['Large Multimodal Models', 'Visual Question Answering', 'Visual Instruction Tuning', 'Gamification', 'Supervised Learning', 'Data Generation']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "LCk3umTAXx",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "tpHqsyZ3YX",
          "title": "ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs",
          "abstract": "As large language models (LLMs) are increasingly being integrated into various real-world applications, the identification of their vulnerabilities to jailbreaking attacks becomes an essential component of ensuring the safety and reliability of LLMs. \nPrevious studies have developed LLM assistants, known as the adversarial prompter, to automatically generate suffixes that manipulate target LLMs into generating harmful and undesirable outputs.\nHowever, these approaches often suffer from low performance or generate semantically meaningless prompts, which can be easily identified by perplexity-based defenses.\nIn this paper, we introduce a novel two-stage method, $\\texttt{ProAdvPrompter}$, that significantly improves the performance of adversarial prompters.\nIn $\\texttt{ProAdvPrompter}$, the first stage (Exploration) utilizes the loss information to guide the adversarial prompter in generating suffixes that are more likely to elicit harmful responses.\nThen the second stage (Exploitation) iteratively fine-tunes the prompter using high-quality generated adversarial suffixes to further boost performance.\nAdditionally, we incorporate the prompt template to aid in the Exploration stage and propose a filtering mechanism to accelerate the training process in the Exploitation stage.\nWe evaluate $\\texttt{ProAdvPrompter}$ against the well-aligned LLMs (i.e., Llama2-Chat-7B and Llama3-chat-8B), achieving attack success rates of 99.68% and 97.12% respectively after 10 trials on the AdvBench dataset, thereby enhancing performance by $\\sim 2$ times compared to previous works.\nMoreover, $\\texttt{ProAdvPrompter}$ reduces training time by 20% on Llama3-Instruct-8B, generates more generalized adversarial suffixes, and demonstrates resilience against the perplexity defense.\nAn ablation study further evaluates the effects of key components in $\\texttt{ProAdvPrompter}$ (the prompt template and the filtering mechanism).",
          "keywords": [
            "jailbreaking attacks; large language model"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=tpHqsyZ3YX",
          "pdf_link": "https://openreview.net/pdf?id=tpHqsyZ3YX"
        },
        "paper_internal_id": "tpHqsyZ3YX",
        "category": "poster",
        "embedding_score": 0.7198420763015747,
        "final_score": 0.9140946269035339
      },
      "spotlight": {
        "paper": {
          "id": "8UFG9D8xeU",
          "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations",
          "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies.",
          "keywords": [
            "Efficient Post-training Preference Alignment",
            "Alignment from demonstrations",
            "Multi-agent Motion Generation"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We propose an efficient post-training alignment approach that significantly improves the pre-trained motion generation model’s quality without requiring additional post-training human preference annotation or expansive compute.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-26",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=8UFG9D8xeU",
          "pdf_link": "https://openreview.net/pdf?id=8UFG9D8xeU"
        },
        "paper_internal_id": "8UFG9D8xeU",
        "category": "spotlight",
        "embedding_score": 0.7099483013153076,
        "final_score": 0.12317647784948349
      },
      "oral": {
        "paper": {
          "id": "Pujt3ADZgI",
          "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
          "abstract": "Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent\nRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In\nthis paper, we explore RLHF under a general preference framework and approach\nit from a game-theoretic perspective. Specifically, we formulate the problem as\na two-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via no-\nregret learning, thereby approximating the Nash policy. Unlike previous methods,\nINPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,\nwe introduce a new loss objective that is directly minimized over a preference\ndataset. We provide theoretical analysis for our approach and demonstrate its\neffectiveness through experiments on various representative benchmarks. With an\nLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled win\nrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantial\nimprovement over the state-of-the-art online RLHF algorithms.",
          "keywords": [
            "RLHF Theory",
            "LLM Alignment"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=Pujt3ADZgI",
          "pdf_link": "https://openreview.net/pdf?id=Pujt3ADZgI"
        },
        "paper_internal_id": "Pujt3ADZgI",
        "category": "oral",
        "embedding_score": 0.7249994874000549,
        "final_score": 0.10580530017614365
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "k3y0oyK7sn",
      "title": "Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function",
      "abstract": "The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of these models remain underexplored. This paper introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple uncertainty quantification methods across three popular datasets with three representative network architectures. Our study focuses on the effectiveness of quantified uncertainty in detecting misclassified and out-of-distribution (OOD) pixels while also improving model calibration. Through empirical analysis, we uncover challenges in existing uncertainty quantification methods and demonstrate the potential of evidential deep learning techniques, which capture both aleatoric and epistemic uncertainty. To address these challenges, we propose a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improves both uncertainty quantification and model calibration for BEV segmentation.",
      "keywords": "['Uncertainty Quantification', 'Evidential Deep Learning', \"Bird's Eye View (BEV) Segmentation\"]",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "k3y0oyK7sn",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "XeRvg7GQH4",
          "title": "One Training Fits All: Generalized Data Condensation via Mixture-of-Information Bottleneck Guidance",
          "abstract": "Data condensation (DC) technologies are widely used in buffer-constrained scenarios to reduce the memory demand of training samples and maintain  DNN training performance. However, due to the storage constraint of deployment devices and the high energy costs of condensation procedure, synthetic datasets generated by DC often have inferior performance in terms of training efficiency and scalability, which greatly limits its practical application on various edge devices. \nThis dilemma arises due to two reasons: i) existing state-of-the-art (SoTA) data condensation approaches that update synthetic datasets by intuitively matching intermediate training outputs (e.g.,  gradients, features and distributions) between real datasets and synthetic datasets without improving their representational information capabilities from the perspective of the useful information contained. ii) DC lacks sufficient consideration for the heterogeneity of storage constraints among various edge devices, which will result in large training overheads (i.e., consumption or storage). \nTo tackle the above issue, We propose a novel method named Mixture-of-Information Bottleneck Dataset Condensation (MIBDC), which employs information bottlenecks from synthetic datasets with various Image Per Class (IPC) numbers to improve the overall DC generalization and scalability. \nSpecifically, in this paper, the following two phenomena are found: i) The quality of synthetic datasets improves with increased synthetic dataset quantity. ii) The smaller the number of synthetic datasets, the earlier they can reach the convergence peak.\nBased on the above two findings, this paper proposes that i) large synthetic datasets can guide the better convergence of smaller ones. ii)  information contained in  synthetic datasets with different IPC numbers can play a collaborative role in the guidance of dataset condensation generalization.\nComprehensive experimental results on three well-known datasets show that, compared with state-of-the-art dataset condensation methods, MIBDC can not only enhance the generalization performance of trained models but also achieve superior scalability.",
          "keywords": [
            "Dataset Condensation"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=XeRvg7GQH4",
          "pdf_link": "https://openreview.net/pdf?id=XeRvg7GQH4"
        },
        "paper_internal_id": "XeRvg7GQH4",
        "category": "reject",
        "embedding_score": 0.6575163006782532,
        "final_score": 0.03292905539274216
      },
      "spotlight": {
        "paper": {
          "id": "GjfIZan5jN",
          "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability",
          "abstract": "The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks, while the widespread applications for pre-trained visual models have posed new requirements for representation interpretability. However, it remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within the representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are available at https://github.com/ssfgunner/IIS.",
          "keywords": [
            "Representation interpretability",
            "vision representations",
            "image understanding"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We find a positive correlation between representation interpretability and classifiability.",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-04",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=GjfIZan5jN",
          "pdf_link": "https://openreview.net/pdf?id=GjfIZan5jN"
        },
        "paper_internal_id": "GjfIZan5jN",
        "category": "spotlight",
        "embedding_score": 0.7021862268447876,
        "final_score": 0.009640847332775593
      },
      "oral": {
        "paper": {
          "id": "Y6aHdDNQYD",
          "title": "MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection",
          "abstract": "LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging $\\textit{cross-corruption}$ scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS.",
          "keywords": [
            "Test-Time Adaptation",
            "3D Object Detection"
          ],
          "primary_area": "transfer learning, meta learning, and lifelong learning",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=Y6aHdDNQYD",
          "pdf_link": "https://openreview.net/pdf?id=Y6aHdDNQYD"
        },
        "paper_internal_id": "Y6aHdDNQYD",
        "category": "oral",
        "embedding_score": 0.702263593673706,
        "final_score": 0.010916703380644321
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "B6bE2GC71a",
      "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "abstract": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage.\nWe present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. \nBy training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. \nKey insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. \nTo facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",
      "keywords": "['Language Models', 'Training Dynamics', 'Pretraining', 'Post-training']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "B6bE2GC71a",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "FZURCro04D",
          "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
          "abstract": "Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their reliance on step-by-step reasoning can make them brittle when tasks do not align with such structured approaches. In contrast, human cognition flexibly alternates between fast, intuitive reasoning (System 1) and slow, analytical reasoning (System 2), depending on context. To bridge this gap, we curate a dataset of 2K examples, each with valid responses from both reasoning styles, and explicitly align LLMs with System 1 and System 2 reasoning. Evaluations across diverse reasoning benchmarks reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.",
          "keywords": [
            "Alignment",
            "System 1 and System 2 thinking",
            "Cognitive heuristics",
            "LLM",
            "NLP"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=FZURCro04D",
          "pdf_link": "https://openreview.net/pdf?id=FZURCro04D"
        },
        "paper_internal_id": "FZURCro04D",
        "category": "reject",
        "embedding_score": 0.7694692611694336,
        "final_score": 0.4094669222831726
      },
      "poster": {
        "paper": {
          "id": "ySFDPoiANu",
          "title": "Execution Guided Line-by-Line Code Generation",
          "abstract": "We present a novel approach to neural code generation that incorporates real-time execution signals into the language model generation process. While large language models (LLMs) have demonstrated impressive code generation capabilities, they typically do not utilize execution feedback during inference, a critical signal that human programmers regularly leverage. Our method, Execution-Guided Classifier-Free Guidance EG-CFG, dynamically incorporates execution signals as the model generates code, providing line-by-line feedback that guides the generation process toward executable solutions.\nEG-CFG employs a multi-stage process: first, we conduct beam search to sample candidate program completions for each line; second, we extract execution signals by executing these candidates against test cases; and finally, we incorporate these signals into the prompt during generation. By maintaining consistent signals across tokens within the same line and refreshing signals at line boundaries, our approach provides coherent guidance while preserving syntactic structure. Moreover, the method naturally supports native parallelism at the task level in which multiple agents operate in parallel, exploring diverse reasoning paths and collectively generating a broad set of candidate solutions.\nOur experiments across diverse coding tasks demonstrate that EG-CFG significantly improves code generation performance compared to standard approaches, achieving state-of-the-art results across various levels of complexity, from foundational problems to challenging competitive programming and data science tasks.",
          "keywords": [
            "Program Induction",
            "Problem Solving",
            "Reasoning",
            "Natural Language Processing"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce EG-CFG, a code generation method that incorporates real-time execution feedback during LLM inference to guide line-by-line generation, significantly improving code accuracy.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=ySFDPoiANu",
          "pdf_link": "https://openreview.net/pdf?id=ySFDPoiANu"
        },
        "paper_internal_id": "ySFDPoiANu",
        "category": "poster",
        "embedding_score": 0.7668377161026001,
        "final_score": 0.7655952572822571
      },
      "spotlight": {
        "paper": {
          "id": "VrCdsZBbIg",
          "title": "Language Modeling by Language Models",
          "abstract": "*Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks).  We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
          "keywords": [
            "Autonomous Scientific Discovery",
            "Large Language Model Agents",
            "Genetic Programming"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=VrCdsZBbIg",
          "pdf_link": "https://openreview.net/pdf?id=VrCdsZBbIg"
        },
        "paper_internal_id": "VrCdsZBbIg",
        "category": "spotlight",
        "embedding_score": 0.809499204158783,
        "final_score": 0.5927201509475708
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "XO9fhSZkBh",
      "title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "abstract": "We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on $\\mathbb{R}^d$. \nWhile the question has been resolved in special cases, the best known lower bound in general is still 2. \nWe focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan.  \nFor such neural networks, we prove a non-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to exactly represent the maximum of $d$ numbers. Additionally, we provide a combinatorial proof that neural networks satisfying this assumption require three hidden layers to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far.\nFinally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers.",
      "keywords": "['Neural Networks', 'Piecewise Linear Functions', 'Exact Representations', 'Polyhedral Geometry', 'Braid Fan', 'Boolean Lattice']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "XO9fhSZkBh",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "wh3p37VYm2",
          "title": "Mechanistic Insights into Grokking from the Embedding Layer",
          "abstract": "Grokking, a delayed generalization in neural networks after perfect training performance, has been observed in Transformers and MLPs, but the components driving it remain underexplored. We show that embeddings are central to grokking: introducing them into MLPs induces delayed generalization in modular arithmetic tasks, whereas MLPs without embeddings can generalize immediately. Our analysis identifies two key mechanisms: (1) Embedding update dynamics, where rare tokens stagnate due to sparse gradient updates and weight decay, and (2) Bilinear coupling, where the interaction between embeddings and downstream weights introduces saddle points and increases sensitivity to initialization.  \nTo confirm these mechanisms, we investigate frequency-aware sampling, which balances token updates by minimizing gradient variance, and embedding-specific learning rates, derived from the asymmetric curvature of the bilinear loss landscape. We prove that an adaptive learning rate ratio, \\(\\frac{\\eta_E}{\\eta_W} \\propto \\frac{\\sigma_{\\max}(E)}{\\sigma_{\\max}(W)} \\cdot \\frac{f_W}{f_E}\\), mitigates bilinear coupling effects, accelerating convergence. Our methods not only improve grokking dynamics but also extend to broader challenges in Transformer optimization, where bilinear interactions hinder efficient training.",
          "keywords": [
            "Embedding learning",
            "Token frequencey",
            "Coupled system"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "Explain the embedding role in optimization of MLP",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=wh3p37VYm2",
          "pdf_link": "https://openreview.net/pdf?id=wh3p37VYm2"
        },
        "paper_internal_id": "wh3p37VYm2",
        "category": "reject",
        "embedding_score": 0.695809006690979,
        "final_score": 0.8813294172286987
      },
      "poster": {
        "paper": {
          "id": "Jsln9ZyMl4",
          "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets",
          "abstract": "We study the parameter complexity of robust memorization for ReLU networks: the number of parameters required to interpolate any dataset with $\\epsilon$-separation between differently labeled points, while ensuring predictions remain consistent within a $\\mu$-ball around each training example. We establish upper and lower bounds on the parameter count as a function of the robustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a fine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain tighter upper and lower bounds that improve upon existing results. Our findings reveal that the parameter complexity of robust memorization matches that of non-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$. As a special case, when the input dimension is comparable to or exceeds the dataset size, our bounds become tight (up to logarithmic factors) across the entire range of $\\rho$.",
          "keywords": [
            "Robust memorization",
            "Memorization",
            "Adversarial training",
            "Parameter Complexity"
          ],
          "primary_area": "theory",
          "TLDR": "We analyze the parameter bounds for robust memorization as a function of the robustness ratio.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-12",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=Jsln9ZyMl4",
          "pdf_link": "https://openreview.net/pdf?id=Jsln9ZyMl4"
        },
        "paper_internal_id": "Jsln9ZyMl4",
        "category": "poster",
        "embedding_score": 0.7725521326065063,
        "final_score": 0.9926784038543701
      },
      "spotlight": {
        "paper": {
          "id": "bP5cU0OYSn",
          "title": "Fast Projection-Free Approach (without Optimization Oracle) for Optimization over Compact Convex Set",
          "abstract": "Projection-free first-order methods, e.g., the celebrated Frank-Wolfe (FW) algorithms, have emerged as powerful tools for optimization over simple convex sets such as polyhedra, because of their scalability, fast convergence, and iteration-wise feasibility without costly projections. \nHowever, extending these methods effectively to general compact convex sets remains challenging and largely open, as FW methods rely on expensive linear optimization oracles (LOO), while penalty-based methods often struggle with poor feasibility. \nWe tackle this open challenge by presenting **Hom-PGD**, a novel projection-free method without expensive (optimization) oracles. \nOur method constructs a homeomorphism between the convex constraint set and a unit ball, transforming the original problem into an equivalent ball-constrained formulation, thus enabling efficient gradient-based optimization while preserving the original problem structure. \nWe prove that Hom-PGD attains *optimal* convergence rates matching gradient descent with constant step-size to find an $\\epsilon$-approximate (stationary) solution: $\\mathcal{O}(\\log (1/\\epsilon))$ for strongly convex objectives, $\\mathcal{O}(\\epsilon^{-1})$ for convex objectives, \nand $\\mathcal{O}(\\epsilon^{-2})$ for non-convex objectives. \nMeanwhile, Hom-PGD enjoys a low per-iteration complexity of $\\mathcal{O}(n^2)$, without expensive oracles like LOO or projection, where $n$ is the input size. \nOur framework further extends to certain non-convex sets, broadening its applicability in practical optimization scenarios with complex constraints. Extensive numerical experiments demonstrate that Hom-PGD achieves comparable convergence rates to state-of-the-art projection-free methods, while significantly reducing per-iteration runtime  (up to 5 orders of magnitude faster) and thus the total problem-solving time.",
          "keywords": [
            "projection-free",
            "homeomorphism",
            "gauge mapping",
            "constrained optimization",
            "convex set"
          ],
          "primary_area": "optimization",
          "TLDR": "A novel projection-free framework for solving (non-)convex optimization over general compact convex set without expensive per-iteration optimization oracles.",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=bP5cU0OYSn",
          "pdf_link": "https://openreview.net/pdf?id=bP5cU0OYSn"
        },
        "paper_internal_id": "bP5cU0OYSn",
        "category": "spotlight",
        "embedding_score": 0.6955117583274841,
        "final_score": 0.9637396931648254
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "R73ybUciQF",
      "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "abstract": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
      "keywords": "['sparse autoencoders', 'SAEs', 'interpretability', 'NLP']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "R73ybUciQF",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "DfHcKzmHpp",
          "title": "Can We Partially Rewrite Transformers in Natural Language?",
          "abstract": "The greatest ambition of mechanistic interpretability is to completely rewrite deep neural networks in a format that is more amenable to human understanding, while preserving their behavior and performance. In this paper we evaluate whether sparse autoencoders (SAEs) and transcoders can be used for this purpose. We use an automated pipeline to generate explanations for each of the sparse coder latents. We then simulate the activation of each latent on a number of different inputs using an LLM prompted with the explanation we generated in the previous step, and \"partially rewrite'' the original model by patching the simulated activations into its forward pass. We find that current sparse coding techniques and automated interpretability pipelines are not up to the task of rewriting even a single layer of a transformer: the model is severely degraded by patching in the simulated activations. We believe this approach is the most thorough way to assess the quality of SAEs and transcoders, despite its high computational cost.",
          "keywords": [
            "Sparse autoencoders",
            "interpretability",
            "language models"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "With current techniques it's impossible to rewrite LLMs with natural language and sparse autocoders",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=DfHcKzmHpp",
          "pdf_link": "https://openreview.net/pdf?id=DfHcKzmHpp"
        },
        "paper_internal_id": "DfHcKzmHpp",
        "category": "reject",
        "embedding_score": 0.7753972411155701,
        "final_score": 0.9699714779853821
      },
      "poster": {
        "paper": {
          "id": "p8lKcNkJRi",
          "title": "Dense SAE Latents Are Features, Not Bugs",
          "abstract": "Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are *dense*), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs---suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and final to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
          "keywords": [
            "sae",
            "sparse autoencoder",
            "interpretability",
            "mechanistic interpretability",
            "language model"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=p8lKcNkJRi",
          "pdf_link": "https://openreview.net/pdf?id=p8lKcNkJRi"
        },
        "paper_internal_id": "p8lKcNkJRi",
        "category": "poster",
        "embedding_score": 0.8357850313186646,
        "final_score": 0.9919294118881226
      },
      "spotlight": {
        "paper": {
          "id": "C7LxkebvVW",
          "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models",
          "abstract": "Diffusion models have become the go-to method for text-to-image generation, producing high-quality images from pure noise. However, the inner workings of diffusion models is still largely a mystery due to their black-box nature and complex, multi-step generation process. Mechanistic interpretability techniques, such as Sparse Autoencoders (SAEs), have been successful in understanding and steering the behavior of large language models at scale. However, the great potential of SAEs has not yet been applied toward gaining insight into the intricate generative process of diffusion models. In this work, we leverage the SAE framework to probe the inner workings of a popular text-to-image diffusion model, and uncover a variety of human-interpretable concepts in its activations. Interestingly, we find that *even before the first reverse diffusion step* is completed, the final composition of the scene can be predicted surprisingly well by looking at the spatial distribution of activated concepts. Moreover, going beyond correlational analysis, we design intervention techniques aimed at manipulating image composition and style, and demonstrate that (1) in early stages of diffusion image composition can be effectively controlled, (2) in the middle stages image composition is finalized, however stylistic interventions are effective, and (3) in the final stages only minor textural details are subject to change.",
          "keywords": [
            "mechanistic interpretability",
            "diffusion models",
            "sparse autoencoders",
            "controlled generation"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We investigate how human-interpretable concepts evolve in diffusion models through the generative process.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=C7LxkebvVW",
          "pdf_link": "https://openreview.net/pdf?id=C7LxkebvVW"
        },
        "paper_internal_id": "C7LxkebvVW",
        "category": "spotlight",
        "embedding_score": 0.7762687802314758,
        "final_score": 0.9437074661254883
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "8P3QNSckMp",
      "title": "A Clean Slate for Offline Reinforcement Learning",
      "abstract": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral.",
      "keywords": "['Offline Reinforcement Learning', 'Evaluation', 'Open-Source']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "8P3QNSckMp",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "ziLHIExi1j",
          "title": "Quantifying First‐Order Markov Breakdowns in Noisy Reinforcement Learning: A Causal Discovery Approach",
          "abstract": "Reinforcement learning (RL) methods often assume that each new observation fully captures the environment’s state, ensuring Markovian (one‐step) transitions. Real‐world deployments, however, frequently violate this assumption due to partial observability or noise in sensors and actuators. This paper introduces a systematic methodology for diagnosing such violations, combining a partial correlation based causal discovery procedure (PCMCI) with a newly proposed Markov Violation score (MVS). The MVS quantifies multi‐step dependencies that emerge when noise or incomplete state information disrupts the Markov property.\n\nClassic control tasks (CartPole, Pendulum, Acrobot) are used to assess how targeted noise and dimension omissions affect both RL performance and the measured Markov consistency. Contrary to expectations, heavy observation noise often fails to induce strong multi‐lag dependencies in certain tasks (e.g., Acrobot). Dimension‐dropping experiments further reveal that omitting certain state variables (e.g., angular velocities in CartPole and Pendulum) substantially degrades returns and elevates MVS, while other dimensions can be removed with negligible effect.\n\nThese findings highlight the importance of identifying and safeguarding the most causally critical dimensions to maintain effective one‐step learning. By bridging partial correlation tests and RL performance metrics, the proposed approach uniquely pinpoints when and where the Markov property breaks. This framework offers a principled tool for designing robust policies, guiding representation learning, and handling partial observability in real‐world RL tasks. All code and experimental logs are publicly available for reproducibility (URL omitted for double‐blind review).",
          "keywords": [
            "Markov Property",
            "PCMCI (Causal Discovery)",
            "PPO",
            "Noisy Reinforcement Learning."
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "Uses PCMCI-based causal discovery to introduce a Markov Violation Score that measures how noise or partial observability breaks the Markov property in reinforcement learning tasks.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=ziLHIExi1j",
          "pdf_link": "https://openreview.net/pdf?id=ziLHIExi1j"
        },
        "paper_internal_id": "ziLHIExi1j",
        "category": "reject",
        "embedding_score": 0.732100248336792,
        "final_score": 0.7661099433898926
      },
      "poster": {
        "paper": {
          "id": "QVheAhJefR",
          "title": "Learning Preferences without Interaction for Cooperative AI: A Hybrid Offline-Online Approach",
          "abstract": "Reinforcement learning (RL) for collaborative agents capable of cooperating with humans to accomplish tasks has long been a central goal in the RL community. While prior approaches have made progress in adapting collaborative agents to diverse human partners, they often focus solely on optimizing task performance and overlook human preferences—despite the fact that such preferences often diverge from the reward-maximization objective of the environment.\nAddressing this discrepancy poses significant challenges: humans typically provide only a small amount of offline, preference-related feedback and are unable to engage in online interactions, resulting in a distributional mismatch between the agent’s online learning process and the offline human data. To tackle this, we formulate the problem as an online&offline reinforcement learning problem that jointly integrates online generalization and offline preference learning, entirely under an offline training regime.\nWe propose a simple yet effective training framework built upon existing RL algorithms that alternates between offline preference learning and online generalization recovery, ensuring the stability and alignment of both learning objectives.\nWe evaluate our approach on a benchmark built upon the Overcooked environment—a standard environment  for human-agent collaboration—and demonstrate remarkable performance across diverse preference styles and cooperative scenarios.",
          "keywords": [
            "Cooperative AI",
            "offline preference learning",
            "online adaptation"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=QVheAhJefR",
          "pdf_link": "https://openreview.net/pdf?id=QVheAhJefR"
        },
        "paper_internal_id": "QVheAhJefR",
        "category": "poster",
        "embedding_score": 0.7251928448677063,
        "final_score": 0.9199519753456116
      },
      "spotlight": {
        "paper": {
          "id": "hguaupzLCU",
          "title": "Horizon Reduction Makes RL Scalable",
          "abstract": "In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000× larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL.",
          "keywords": [
            "reinforcement learning",
            "offline reinforcement learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=hguaupzLCU",
          "pdf_link": "https://openreview.net/pdf?id=hguaupzLCU"
        },
        "paper_internal_id": "hguaupzLCU",
        "category": "spotlight",
        "embedding_score": 0.7936729192733765,
        "final_score": 0.8458368182182312
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "XPe55Uffd7",
      "title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "abstract": "We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that $\\textit{every}$ concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning.",
      "keywords": "['Active learning', 'Agnostic learning', 'PAC learning', 'Query complexity', 'Minimax analysis', 'VC dimension', 'Star number', 'Disagreement coefficient']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "XPe55Uffd7",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "zmlhP8myaT",
          "title": "Stepwise Feature Learning in Self-Supervised Learning",
          "abstract": "Recent advances in self-supervised learning (SSL) have shown remarkable progress in representation learning. However, SSL models often exhibit shortcut learning phenomenon, where they exploit dataset-specific biases rather than learning generalizable features, sometimes leading to severe over-optimization on particular datasets. We present a theoretical framework that analyzes this shortcut learning phenomenon through the lens of $\\textit{extent bias}$ and $\\textit{amplitude bias}$. By investigating the relations among extent bias, amplitude bias, and learning priorities in SSL, we demonstrate that learning dynamics is fundamentally governed by the dimensional properties and amplitude of features rather than their semantic importance. Our analysis reveals how the eigenvalues of the feature cross-correlation matrix influence which features are learned earlier, providing insights into why models preferentially learn shortcut features over more generalizable features.",
          "keywords": [
            "shortcut learning",
            "self-supervised learning",
            "stepwise learning",
            "feature learning",
            "learning dynamics"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=zmlhP8myaT",
          "pdf_link": "https://openreview.net/pdf?id=zmlhP8myaT"
        },
        "paper_internal_id": "zmlhP8myaT",
        "category": "reject",
        "embedding_score": 0.7222679853439331,
        "final_score": 0.2517089545726776
      },
      "poster": {
        "paper": {
          "id": "qRPIWtf3SE",
          "title": "Learning single index models via harmonic decomposition",
          "abstract": "We study the problem of learning single-index models, where the label $y \\in \\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through an unknown one-dimensional projection $\\langle \\boldsymbol{w_*}, \\boldsymbol{x} \\rangle$. Prior work has shown that under Gaussian inputs, the statistical and computational complexity of recovering $\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function. In this paper, we propose a new perspective: we argue that *spherical harmonics*---rather than *Hermite polynomials*---provide the natural basis for this problem, as they capture its intrinsic \\textit{rotational symmetry}. Building on this insight, we characterize the complexity of learning single-index models under arbitrary spherically symmetric input distributions. We introduce two families of estimators---based on tensor-unfolding and online SGD---that respectively achieve either  optimal sample complexity or optimal runtime, and argue that estimators achieving both may not exist in general. When specialized to Gaussian inputs, our theory not only recovers and clarifies existing results but also reveals new phenomena that had previously been overlooked.",
          "keywords": [
            "single-index models",
            "statistical and computational complexity"
          ],
          "primary_area": "theory",
          "TLDR": "We revisit the single-index models and argue that spherical harmonics, not Hermite polynomials, are a natural basis. We characterize the complexity for any spherically symmetric input measure, & provide several new insights for the Gaussian case.",
          "creation_date": "2025-05-05",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=qRPIWtf3SE",
          "pdf_link": "https://openreview.net/pdf?id=qRPIWtf3SE"
        },
        "paper_internal_id": "qRPIWtf3SE",
        "category": "poster",
        "embedding_score": 0.7347780466079712,
        "final_score": 0.6918842792510986
      },
      "spotlight": {
        "paper": {
          "id": "sXpyn3lAb5",
          "title": "Accelerating data-driven algorithm selection for combinatorial partitioning problems",
          "abstract": "Data-driven algorithm selection is a powerful approach for choosing effective heuristics for computational problems. It operates by evaluating a set of candidate algorithms on a collection of representative training instances and selecting the one with the best empirical performance. However, running each algorithm on every training instance is computationally expensive, making scalability a central challenge. In practice, a common workaround is to evaluate algorithms on smaller proxy instances derived from the original inputs. However, this practice has remained largely ad hoc and lacked theoretical grounding. We provide the first theoretical foundations for this practice by formalizing the notion of size generalization: predicting an algorithm's performance on a large instance by evaluating it on a smaller, representative instance, subsampled from the original instance. We provide size generalization guarantees for three widely used clustering algorithms (single-linkage, k-means++, and Gonzalez's k-centers heuristic) and two canonical max-cut algorithms (Goemans-Williamson and Greedy). We characterize the subsample size sufficient to ensure that performance on the subsample reflects performance on the full instance, and our experiments support these findings.",
          "keywords": [
            "data-driven algorithm selection",
            "sub-sampling",
            "clustering",
            "max-cut",
            "Goemans-williamson"
          ],
          "primary_area": "theory",
          "TLDR": "We formalize the notion of size generalization in the context of data-driven algorithm selection and prove size generalization guarantees for three clustering algorithms and two max-cut algorithms.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=sXpyn3lAb5",
          "pdf_link": "https://openreview.net/pdf?id=sXpyn3lAb5"
        },
        "paper_internal_id": "sXpyn3lAb5",
        "category": "spotlight",
        "embedding_score": 0.7291343808174133,
        "final_score": 0.45709753036499023
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "EoebmBe9fG",
      "title": "Optimal Mistake Bounds for Transductive Online Learning",
      "abstract": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class $\\mathcal{H}$ with Littlestone dimension $d$, the transductive mistake bound is at least $\\Omega(\\sqrt{d})$. This establishes an exponential improvement over previous lower bounds of $\\Omega(\\log \\log d)$, $\\Omega(\\sqrt{\\log d})$, and $\\Omega(\\log d)$, respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\\sqrt{d})$. Our upper bound also improves the previous best known upper bound of $(2/3) \\cdot d$ from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.",
      "keywords": "['Online Learning']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "EoebmBe9fG",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "vWaMUMrBpF",
          "title": "Inconsistency-Aware Minimization: Improving Generalization with Unlabeled Data",
          "abstract": "Accurately estimating the generalization gap and devising optimization methods that generalize better are crucial for deep learning models, particularly in both theoretical understanding and practical applications. The ability to leverage unlabeled data for these purposes offers significant advantages in real-world scenarios. This paper introduces a novel generalization measure, termed $\\textit{local inconsistency}$, developed from an information-geometric perspective of the neural network's parameter space; a key feature is its computability from unlabeled data. We establish its theoretical underpinnings by connecting local inconsistency to the Fisher Information Matrix (FIM) and the loss Hessian. Empirically, we demonstrate that local inconsistency not only correlates with the generalization gap but also exhibits characteristics comparable to $\\textit{sharpness}$. Based on these findings, we propose Inconsistency-Aware Minimization (IAM), a regularization strategy that incorporates local inconsistency. We demonstrate that in standard supervised learning settings, IAM enhances generalization, achieving performance comparable to existing methods such as Sharpness-Aware Minimization (SAM). Furthermore, IAM exhibits notable efficacy in semi-supervised learning scenarios, where the local inconsistency regularizer is computed from the unlabeled data portion to further improve model performance.",
          "keywords": [
            "Generalization",
            "Regularization",
            "Training Method",
            "Deep Learning",
            "Inconsistency"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=vWaMUMrBpF",
          "pdf_link": "https://openreview.net/pdf?id=vWaMUMrBpF"
        },
        "paper_internal_id": "vWaMUMrBpF",
        "category": "reject",
        "embedding_score": 0.7136519551277161,
        "final_score": 0.5841484069824219
      },
      "poster": {
        "paper": {
          "id": "AQ21krZgax",
          "title": "Formal Models of Active Learning from Contrastive Examples",
          "abstract": "Machine learning can greatly benefit from providing learning algorithms with pairs of contrastive training examples---typically pairs of instances that differ only slightly, yet have different class labels. Intuitively, the difference in the instances helps explain the difference in the class labels. This paper proposes a theoretical framework in which the effect of various types of contrastive examples on active learners is studied formally. The focus is on the sample complexity of learning concept classes and how it is influenced by the choice of contrastive examples. We illustrate our results with geometric concept classes and classes of Boolean functions. Interestingly, we reveal a connection between learning from contrastive examples and the classical model of self-directed learning.",
          "keywords": [
            "membership queries",
            "self-directed learning",
            "learning boolean functions",
            "learning from contrastive examples"
          ],
          "primary_area": "theory",
          "TLDR": "This purely theoretical paper introduces and studies new models of query learning with contrastive examples.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=AQ21krZgax",
          "pdf_link": "https://openreview.net/pdf?id=AQ21krZgax"
        },
        "paper_internal_id": "AQ21krZgax",
        "category": "poster",
        "embedding_score": 0.757544755935669,
        "final_score": 0.9503875374794006
      },
      "spotlight": {
        "paper": {
          "id": "e8R0ytPhLv",
          "title": "Eluder dimension: localise it!",
          "abstract": "We establish a lower bound on the eluder dimension in generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.",
          "keywords": [
            "eluder dimension",
            "bandits",
            "reinforcement learning"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=e8R0ytPhLv",
          "pdf_link": "https://openreview.net/pdf?id=e8R0ytPhLv"
        },
        "paper_internal_id": "e8R0ytPhLv",
        "category": "spotlight",
        "embedding_score": 0.7201083898544312,
        "final_score": 0.8978778719902039
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "kVz9uvqUna",
      "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "abstract": "Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching.\nFirst, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance.",
      "keywords": "['flow matching', 'generalization', 'memorization']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "kVz9uvqUna",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "2mDquK2qMI",
          "title": "One Step Diffusion via Flow Fitting",
          "abstract": "Diffusion and flow-matching models have demonstrated impressive performance in generating diverse, high-fidelity images by learning transformations from noise to data. However, their reliance on multi-step sampling requires repeated neural network evaluations, leading to high computational cost. We propose FlowFit, a family of generative models that enables high-quality sample generation through both single-phase training and single-step inference. FlowFit learns to approximate the continuous flow trajectory between latent noise \\(x_0\\) and data \\(x_1\\) by fitting a basis of functions parameterized over time \\(t \\in [0, 1]\\) during training. At inference time, sampling is performed by simply evaluating the flow only at the terminal time \\(t = 1\\), avoiding iterative denoising or numerical integration. Empirically, FlowFit outperforms prior diffusion-based single-phase training methods achieving superior sample quality.",
          "keywords": [
            "Efficient generative models",
            "Single step diffusion"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=2mDquK2qMI",
          "pdf_link": "https://openreview.net/pdf?id=2mDquK2qMI"
        },
        "paper_internal_id": "2mDquK2qMI",
        "category": "reject",
        "embedding_score": 0.8260005712509155,
        "final_score": 0.1903633028268814
      },
      "poster": {
        "paper": {
          "id": "XF4JM2MTSF",
          "title": "CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices",
          "abstract": "Normalizing flows are deep generative models that achieve efficient likelihood estimation and sampling through invertible transformations. A key challenge is designing linear layers that enhance expressiveness while enabling efficient computation of the Jacobian determinant and inverse. In this work, we introduce a novel invertible linear layer based on the product of circulant and diagonal matrices. This decomposition provides a parameter- and computation-efficient formulation, reducing the parameter complexity from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$ by using $m$ diagonal matrices together with $m-1$ circulant matrices, while approximating arbitrary linear transformations.Furthermore, leveraging the Fast Fourier Transform (FFT), our method reduces the time complexity of matrix inversion from $\\mathcal{O}(n^{3})$ to $\\mathcal{O}(mn \\log n)$ and matrix log-determinant from $\\mathcal{O}(n^{3})$ to $\\mathcal{O}(mn)$, where $n$ is the input dimension. Building upon this, we introduce a novel normalizing flow model called Circulant-Diagonal Flow (CDFlow). Empirical results demonstrate that CDFlow excels in density estimation for natural image datasets and effectively models data with inherent periodicity. In terms of computational efficiency, our method speeds up the matrix inverse and log-determinant computations by $1.17\\times$ and $4.31\\times$, respectively, compared to the general dense matrix, when the number of channels is set to 96.",
          "keywords": [
            "Normalizing Flows; Fast Fourier Transform (FFT); Matrix Inversion; Jacobian Determinant"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=XF4JM2MTSF",
          "pdf_link": "https://openreview.net/pdf?id=XF4JM2MTSF"
        },
        "paper_internal_id": "XF4JM2MTSF",
        "category": "poster",
        "embedding_score": 0.7488565444946289,
        "final_score": 0.3923620581626892
      },
      "spotlight": {
        "paper": {
          "id": "MVYz4GmcUH",
          "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
          "abstract": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from arbitrarily images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We use our framework to achieve state-of-the-art ImageNet FID and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",
          "keywords": [
            "ambient diffusion",
            "diffusion models",
            "corrupted data",
            "generative AI"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We improve the quality of generative models by using low-quality, corrupted, and out-of-distribution data",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=MVYz4GmcUH",
          "pdf_link": "https://openreview.net/pdf?id=MVYz4GmcUH"
        },
        "paper_internal_id": "MVYz4GmcUH",
        "category": "spotlight",
        "embedding_score": 0.7394198179244995,
        "final_score": 0.09205401688814163
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Q3qAsZAEZw",
      "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
      "keywords": "['Large Language Models (LLMs)', 'Reproducibility', 'Numerical precision', 'Deterministic inference']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Q3qAsZAEZw",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "FZURCro04D",
          "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
          "abstract": "Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their reliance on step-by-step reasoning can make them brittle when tasks do not align with such structured approaches. In contrast, human cognition flexibly alternates between fast, intuitive reasoning (System 1) and slow, analytical reasoning (System 2), depending on context. To bridge this gap, we curate a dataset of 2K examples, each with valid responses from both reasoning styles, and explicitly align LLMs with System 1 and System 2 reasoning. Evaluations across diverse reasoning benchmarks reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.",
          "keywords": [
            "Alignment",
            "System 1 and System 2 thinking",
            "Cognitive heuristics",
            "LLM",
            "NLP"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=FZURCro04D",
          "pdf_link": "https://openreview.net/pdf?id=FZURCro04D"
        },
        "paper_internal_id": "FZURCro04D",
        "category": "reject",
        "embedding_score": 0.7688605189323425,
        "final_score": 0.6985234022140503
      },
      "poster": {
        "paper": {
          "id": "iBFfb6bGOz",
          "title": "Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities",
          "abstract": "Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of \"atomic thinking\".",
          "keywords": [
            "Large Language Models",
            "Mathematical Reasoning",
            "Atomic Thinking"
          ],
          "primary_area": "evaluation",
          "TLDR": "We have decoupled the math atomic capabilities of large language models and explored their interaction relationships in mathematical reasoning tasks.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=iBFfb6bGOz",
          "pdf_link": "https://openreview.net/pdf?id=iBFfb6bGOz"
        },
        "paper_internal_id": "iBFfb6bGOz",
        "category": "poster",
        "embedding_score": 0.7655728459358215,
        "final_score": 0.9992737174034119
      },
      "spotlight": {
        "paper": {
          "id": "WJIDorHiuZ",
          "title": "CoRe: Benchmarking LLMs’ Code Reasoning Capabilities through Static Analysis Tasks",
          "abstract": "Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, \nleaving the models' ability of program semantic reasoning underexplored.\nThis work presents CoRe, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CoRe includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. \nTo ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. \nWe evaluate 10 state-of-the-art LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning.\nWe further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs’ code reasoning capabilities.",
          "keywords": [
            "Large Language Models",
            "Code Reasoning",
            "Program Analysis",
            "Code Understanding",
            "Static Analysis",
            "Benchmarking"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "CoRe: a high-quality, multi-lingual benchmark for evaluating LLMs’ Code Reasoning capabilities with fundamental static analysis tasks.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=WJIDorHiuZ",
          "pdf_link": "https://openreview.net/pdf?id=WJIDorHiuZ"
        },
        "paper_internal_id": "WJIDorHiuZ",
        "category": "spotlight",
        "embedding_score": 0.7466867566108704,
        "final_score": 0.9819115400314331
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "ImpizBSKcu",
      "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm.  We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$,\nand large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies:\n$(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network;\n$(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity;\n$(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated  `feature unlearning' regime at large times.",
      "keywords": "['Overfitting; feature learning; dynamical mean field theory; generalization;']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "ImpizBSKcu",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "wh3p37VYm2",
          "title": "Mechanistic Insights into Grokking from the Embedding Layer",
          "abstract": "Grokking, a delayed generalization in neural networks after perfect training performance, has been observed in Transformers and MLPs, but the components driving it remain underexplored. We show that embeddings are central to grokking: introducing them into MLPs induces delayed generalization in modular arithmetic tasks, whereas MLPs without embeddings can generalize immediately. Our analysis identifies two key mechanisms: (1) Embedding update dynamics, where rare tokens stagnate due to sparse gradient updates and weight decay, and (2) Bilinear coupling, where the interaction between embeddings and downstream weights introduces saddle points and increases sensitivity to initialization.  \nTo confirm these mechanisms, we investigate frequency-aware sampling, which balances token updates by minimizing gradient variance, and embedding-specific learning rates, derived from the asymmetric curvature of the bilinear loss landscape. We prove that an adaptive learning rate ratio, \\(\\frac{\\eta_E}{\\eta_W} \\propto \\frac{\\sigma_{\\max}(E)}{\\sigma_{\\max}(W)} \\cdot \\frac{f_W}{f_E}\\), mitigates bilinear coupling effects, accelerating convergence. Our methods not only improve grokking dynamics but also extend to broader challenges in Transformer optimization, where bilinear interactions hinder efficient training.",
          "keywords": [
            "Embedding learning",
            "Token frequencey",
            "Coupled system"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "Explain the embedding role in optimization of MLP",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=wh3p37VYm2",
          "pdf_link": "https://openreview.net/pdf?id=wh3p37VYm2"
        },
        "paper_internal_id": "wh3p37VYm2",
        "category": "reject",
        "embedding_score": 0.7289801239967346,
        "final_score": 0.4729838967323303
      },
      "poster": {
        "paper": {
          "id": "XbAtacZeEp",
          "title": "Beyond Benign Overfitting in Nadaraya-Watson Interpolators",
          "abstract": "In recent years, there has been much interest in understanding the generalization behavior of interpolating predictors, which overfit on noisy training data. Whereas standard analyses are concerned with whether a method is consistent or not, recent observations have shown that even inconsistent predictors can generalize well. In this work, we revisit the classic interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method), and study its generalization capabilities  through this modern viewpoint. In particular, by varying a single bandwidth-like hyperparameter, we prove the existence of multiple overfitting behaviors, ranging non-monotonically from catastrophic, through benign, to tempered.\nOur results highlight how even classical interpolating methods can exhibit intricate generalization behaviors. In addition, for the purpose of tuning the hyperparameter, the results suggest that over-estimating the intrinsic dimension of the data is less harmful than under-estimating it. Numerical experiments complement our theory, demonstrating the same phenomena.",
          "keywords": [
            "Benign overfitting",
            "tempered overfitting",
            "interpolation",
            "generalization",
            "Nadaraya-Watson",
            "kernel"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=XbAtacZeEp",
          "pdf_link": "https://openreview.net/pdf?id=XbAtacZeEp"
        },
        "paper_internal_id": "XbAtacZeEp",
        "category": "poster",
        "embedding_score": 0.7254865169525146,
        "final_score": 0.7604977488517761
      },
      "spotlight": {
        "paper": {
          "id": "Tk5nQnTGmP",
          "title": "Is Grokking a Computational Glass Relaxation?",
          "abstract": "Understanding neural network' (NN) generalizability remains a central question in deep learning research.\nThe special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability.\nHere we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. \nThis mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. \nOur experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition.\nWe identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. \nInspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. \nThis provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.",
          "keywords": [
            "grokking",
            "entropy",
            "generalization",
            "glass"
          ],
          "primary_area": "theory",
          "TLDR": "By framing grokking as computational glass relaxation, this work explains grokking from the perspective of Boltzmann entropy and proposes a physics-based grokking-resistant optimizer.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=Tk5nQnTGmP",
          "pdf_link": "https://openreview.net/pdf?id=Tk5nQnTGmP"
        },
        "paper_internal_id": "Tk5nQnTGmP",
        "category": "spotlight",
        "embedding_score": 0.7691147327423096,
        "final_score": 0.6905313730239868
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Tk5nQnTGmP",
      "title": "Is Grokking a Computational Glass Relaxation?",
      "abstract": "Understanding neural network' (NN) generalizability remains a central question in deep learning research.\nThe special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability.\nHere we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. \nThis mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. \nOur experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition.\nWe identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. \nInspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. \nThis provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.",
      "keywords": "['grokking', 'entropy', 'generalization', 'glass']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Tk5nQnTGmP",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "vWaMUMrBpF",
          "title": "Inconsistency-Aware Minimization: Improving Generalization with Unlabeled Data",
          "abstract": "Accurately estimating the generalization gap and devising optimization methods that generalize better are crucial for deep learning models, particularly in both theoretical understanding and practical applications. The ability to leverage unlabeled data for these purposes offers significant advantages in real-world scenarios. This paper introduces a novel generalization measure, termed $\\textit{local inconsistency}$, developed from an information-geometric perspective of the neural network's parameter space; a key feature is its computability from unlabeled data. We establish its theoretical underpinnings by connecting local inconsistency to the Fisher Information Matrix (FIM) and the loss Hessian. Empirically, we demonstrate that local inconsistency not only correlates with the generalization gap but also exhibits characteristics comparable to $\\textit{sharpness}$. Based on these findings, we propose Inconsistency-Aware Minimization (IAM), a regularization strategy that incorporates local inconsistency. We demonstrate that in standard supervised learning settings, IAM enhances generalization, achieving performance comparable to existing methods such as Sharpness-Aware Minimization (SAM). Furthermore, IAM exhibits notable efficacy in semi-supervised learning scenarios, where the local inconsistency regularizer is computed from the unlabeled data portion to further improve model performance.",
          "keywords": [
            "Generalization",
            "Regularization",
            "Training Method",
            "Deep Learning",
            "Inconsistency"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=vWaMUMrBpF",
          "pdf_link": "https://openreview.net/pdf?id=vWaMUMrBpF"
        },
        "paper_internal_id": "vWaMUMrBpF",
        "category": "reject",
        "embedding_score": 0.6947877407073975,
        "final_score": 0.6807358264923096
      },
      "poster": {
        "paper": {
          "id": "YTbLri0siT",
          "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent",
          "abstract": "Hebbian learning is a key principle underlying learning in biological neural networks. We relate a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent with respect to a non-convex loss function on the probability simplex. Despite the constant injection of noise and the non-convexity of the underlying optimization problem, one can rigorously prove that the considered Hebbian learning dynamic identifies the presynaptic neuron with the highest activity and that the convergence is exponentially fast in the number of iterations. This is non-standard and surprising as typically noisy gradient descent with fixed noise level only converges to a stationary regime where the noise causes the dynamic to fluctuate around a minimiser.",
          "keywords": [
            "Biological neural networks",
            "Hebbian learning",
            "Spike-timing-dependent plasticity",
            "Noisy gradient descent",
            "Mirror descent"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "Spike-timing-dependent plasticity can be rephrased as noisy gradient, which allows to obtain convergence guarantees for the stochastic learning dynamics.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=YTbLri0siT",
          "pdf_link": "https://openreview.net/pdf?id=YTbLri0siT"
        },
        "paper_internal_id": "YTbLri0siT",
        "category": "poster",
        "embedding_score": 0.6977271437644958,
        "final_score": 0.8294580578804016
      },
      "oral": {
        "paper": {
          "id": "KurYdcCbjv",
          "title": "Generalized Linear Mode Connectivity for Transformers",
          "abstract": "Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is $\\textit{linear mode connectivity}$ (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.",
          "keywords": [
            "Neural Network Merging",
            "Linear Mode Connectivity",
            "Model Re-basin",
            "Parameter Space Geometry",
            "Transformer",
            "Permutation Invariance",
            "Model Fusion"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose a unified framework for model merging that leverages multiple symmetry classes to enable low- and zero-loss interpolation between independently trained Transformer models, including Vision Transformers and GPT-2.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=KurYdcCbjv",
          "pdf_link": "https://openreview.net/pdf?id=KurYdcCbjv"
        },
        "paper_internal_id": "KurYdcCbjv",
        "category": "oral",
        "embedding_score": 0.7193616628646851,
        "final_score": 0.9323143362998962
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "xVI8g50Qfk",
      "title": "Error Forcing in Recurrent Neural Networks",
      "abstract": "How should feedback influence recurrent neural network (RNN) learning? One way to address the known limitations of backpropagation through time is to directly adjust neural activities during the learning process. However, it remains unclear how to effectively use feedback to shape RNN dynamics. Here, we introduce error forcing (EF), where the network activity is guided orthogonally toward the zero-error manifold during learning. This method contrasts with alternatives like teaching forcing, which impose stronger constraints on neural activity and thus induce larger feedback influence on circuit dynamics. Furthermore, EF can be understood from a Bayesian perspective as a form of approximate dynamic inference.  Empirically, EF consistently outperforms other learning algorithms across several tasks and its benefits persist when additional biological constraints are taken into account. Overall, EF is a powerful temporal credit assignment mechanism and a promising candidate model for learning in biological systems.",
      "keywords": "['recurrent neural networks', 'bio-plausible learning']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "xVI8g50Qfk",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "KkOMqJQiWU",
          "title": "Meta-learning local learning rules for structured credit assignment with sparse feedback",
          "abstract": "Biological neural networks can learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via **backpropagation through learning**. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.",
          "keywords": [
            "Biologically Plausible Deep Networks",
            "Plasticity and Adaptation",
            "Recurrent Networks",
            "Reinforcement Learning (Cognitive/Neuroscience)"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "We meta-learn local learning rules for training recurrent networks with reinforcement signals",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=KkOMqJQiWU",
          "pdf_link": "https://openreview.net/pdf?id=KkOMqJQiWU"
        },
        "paper_internal_id": "KkOMqJQiWU",
        "category": "reject",
        "embedding_score": 0.7561047077178955,
        "final_score": 0.9730561375617981
      },
      "poster": {
        "paper": {
          "id": "bd8kppxyB3",
          "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences",
          "abstract": "Proper initialization is critical for Recurrent Neural Networks (RNNs), particularly in long-range reasoning tasks, where repeated application of the same weight matrix can cause vanishing or exploding signals.\nA common baseline for linear recurrences is Glorot initialization, designed to ensure stable signal propagation---but derived under the infinite-width, fixed-length regime—an unrealistic setting for RNNs processing long sequences. In this work, we show that Glorot initialization is in fact unstable: small positive deviations in the spectral radius are amplified through time and cause the hidden state to explode. Our theoretical analysis demonstrates that sequences of length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to induce instability. To address this, we propose a simple, dimension-aware rescaling of Glorot that shifts the spectral radius slightly below one, preventing rapid signal explosion or decay. These results suggest that standard initialization schemes may break down in the long-sequence regime, motivating a separate line of theory for stable recurrent initialization.",
          "keywords": [
            "Recurrent Networks",
            "Initialization",
            "Signal Propagation"
          ],
          "primary_area": "theory",
          "TLDR": "Standard Glorot initialization becomes unstable when used in RNNs with long sequences, leading to exploding hidden states. To address this, we propose a simple rescaling that effectively mitigates the instability.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=bd8kppxyB3",
          "pdf_link": "https://openreview.net/pdf?id=bd8kppxyB3"
        },
        "paper_internal_id": "bd8kppxyB3",
        "category": "poster",
        "embedding_score": 0.7045314311981201,
        "final_score": 0.9891530275344849
      },
      "oral": {
        "paper": {
          "id": "cGks3s79hW",
          "title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
          "abstract": "Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.",
          "keywords": [
            "recurrent neural networks",
            "neuronal recordings",
            "visual cortex",
            "latent variable models",
            "PCA",
            "eigenvalue decay",
            "mean-field limit"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "We show that high-dimensional neural activity can arise from low-dimensional latent dynamics, both in RNNs and in the brain.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=cGks3s79hW",
          "pdf_link": "https://openreview.net/pdf?id=cGks3s79hW"
        },
        "paper_internal_id": "cGks3s79hW",
        "category": "oral",
        "embedding_score": 0.6951422691345215,
        "final_score": 0.9540390968322754
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "hSX7Dd8dxy",
      "title": "Inference-Time Reward Hacking in Large Language Models",
      "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM’s output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for  complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce $\\texttt{HedgeTune}$, an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups.",
      "keywords": "['reward hacking', 'large language models', 'inference time alignment', 'information theory']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "hSX7Dd8dxy",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "cEd00CXWE5",
          "title": "Beyond Two-Stage Training: Integrating SFT and RL for Improved Reasoning in LLMs",
          "abstract": "Reinforcement learning (RL) has proven effective in incentiving the reasoning abilities of large language models (LLMs), but faces significant efficiency challenges due to its extensive trial-and-error nature. A common practice is to employ supervised fine-tuning (SFT) as a warm-up stage; however, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. Specifically, the SFT objective is explicitly conditioned on the optimal solution of the RL objective. During training, lower-level updates enable the model to receive SFT supervision concurrently with RL-based exploration, while upper-level updates are optimized to ensure that the joint training yields higher rewards than RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.",
          "keywords": [
            "LLM",
            "Reasoning",
            "RL",
            "SFT"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-11-26",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=cEd00CXWE5",
          "pdf_link": "https://openreview.net/pdf?id=cEd00CXWE5"
        },
        "paper_internal_id": "cEd00CXWE5",
        "category": "reject",
        "embedding_score": 0.7560189962387085,
        "final_score": 0.8946466445922852
      },
      "poster": {
        "paper": {
          "id": "10s01YrlKp",
          "title": "metaTextGrad: Automatically optimizing language model optimizers",
          "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that using LLM-based optimizers to automatically optimize model prompts, demonstrations, predictions themselves, or other components can significantly enhance the performance of AI systems, as demonstrated by frameworks such as DSPy and TextGrad. However, optimizers built on language models themselves are usually designed by humans with manual design choices; optimizers themselves are not optimized. Moreover, these optimizers are general purpose by design, to be useful to a broad audience, and are not tailored for specific tasks. To address these challenges, we propose metaTextGrad, which focuses on designing a meta-optimizer to further enhance existing optimizers and align them to be good optimizers for a given task. Our approach consists of two key components: a meta prompt optimizer and a meta structure optimizer. The combination of these two significantly improves performance across multiple benchmarks, achieving an average absolute performance improvement of up to 6% compared to the best baseline.",
          "keywords": [
            "programming models",
            "prompting techniques",
            "meta-learning",
            "LLM optimizer"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose an approach to flexibly optimize LLM optimizers.",
          "creation_date": "2025-05-03",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=10s01YrlKp",
          "pdf_link": "https://openreview.net/pdf?id=10s01YrlKp"
        },
        "paper_internal_id": "10s01YrlKp",
        "category": "poster",
        "embedding_score": 0.7380506992340088,
        "final_score": 0.9806971549987793
      },
      "oral": {
        "paper": {
          "id": "4OsgYD7em5",
          "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
          "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. \nIt is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. \nIn this study, we take a critical look at \\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\textit{k} at large \\textit{k} values as the evaluation metric.\nWhile RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\emph{not} elicit fundamentally new reasoning patterns.\nWe observe that while RLVR-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models achieve higher pass@$k$ score when $k$ is large.\nMoreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses.\nFurther coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \\textit{bounded} by the base model. \nFrom this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model.\nIn contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities.\nTaken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.",
          "keywords": [
            "reinforcement learning with verifiable reward",
            "LLM reasoning"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We systematically examine the current state of RLVR and surprisingly find that it does not elicit fundamentally new reasoning patterns—revealing a gap between the potential of RL and the actual impact of current RLVR methods.",
          "creation_date": "2025-05-04",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=4OsgYD7em5",
          "pdf_link": "https://openreview.net/pdf?id=4OsgYD7em5"
        },
        "paper_internal_id": "4OsgYD7em5",
        "category": "oral",
        "embedding_score": 0.7492713928222656,
        "final_score": 0.7489485144615173
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "RPRqKhjrr6",
      "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
      "abstract": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this —typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item—using both AI judges and specialized verifier programs—then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods on top of a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks — RLCF is the only method to help on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. We show that RLCF can also be used off-policy to improve Llama 3.1 8B Instruct and OLMo 2 7B Instruct. These results establish rubrics as a key tool for improving language models' support of queries that express a multitude of needs. We release our our dataset of rubrics (WildChecklists), models, and code to the public.",
      "keywords": "['alignment', 'rubrics', 'instruction following', 'RLHF']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "RPRqKhjrr6",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "obXGSmmG70",
          "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06% on APP, while maintaining high performance on complex tasks. This substantial token decrease directly translates to a significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering a practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications.",
          "keywords": [
            "Adaptive Reasoning",
            "Chain-of-Thought",
            "Large Language Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "LLMs using Chain-of-Thought (CoT) for everything is wasteful. We built AdaCoT, a smart system that teaches LLMs when to use CoT based on clear principles, saving compute and improving user experience without sacrificing performance on hard tasks.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=obXGSmmG70",
          "pdf_link": "https://openreview.net/pdf?id=obXGSmmG70"
        },
        "paper_internal_id": "obXGSmmG70",
        "category": "reject",
        "embedding_score": 0.7306450605392456,
        "final_score": 0.7902212142944336
      },
      "poster": {
        "paper": {
          "id": "oN5YVZ9JeF",
          "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning",
          "abstract": "Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high–quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promote robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples in 40 minutes on a single GPU. Our code is available at https://github.com/Dynamite321/T-SHIRT.",
          "keywords": [
            "Large Language Models",
            "Instruction tuning",
            "Data Selection",
            "Token-selective Quality Score",
            "Robust Hierarchical Selection"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce T-SHIRT, a new data selection method for instruction tuning LLMs that scores data at the token level and emphasizes robustness.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=oN5YVZ9JeF",
          "pdf_link": "https://openreview.net/pdf?id=oN5YVZ9JeF"
        },
        "paper_internal_id": "oN5YVZ9JeF",
        "category": "poster",
        "embedding_score": 0.7720232009887695,
        "final_score": 0.9638134241104126
      },
      "oral": {
        "paper": {
          "id": "B6bE2GC71a",
          "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
          "abstract": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage.\nWe present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. \nBy training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. \nKey insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. \nTo facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",
          "keywords": [
            "Language Models",
            "Training Dynamics",
            "Pretraining",
            "Post-training"
          ],
          "primary_area": "evaluation",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=B6bE2GC71a",
          "pdf_link": "https://openreview.net/pdf?id=B6bE2GC71a"
        },
        "paper_internal_id": "B6bE2GC71a",
        "category": "oral",
        "embedding_score": 0.7607305645942688,
        "final_score": 0.905735433101654
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "S8XcHutp7Z",
      "title": "X-Field: A Physically Informed Representation for 3D X-ray Reconstruction",
      "abstract": "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly regulated due to radiation exposure. Recent research borrows representations from the 3D reconstruction area to complete two tasks with reduced radiation dose: X-ray Novel View Synthesis (NVS) and Computed Tomography (CT) reconstruction. \nHowever, these representations fail to fully capture the penetration and attenuation properties of X-ray imaging as they originate from visible light imaging.\nIn this paper, we introduce X-Field, a 3D representation informed in the physics of X-ray imaging. \nFirst, we employ homogeneous 3D ellipsoids with distinct attenuation coefficients to accurately model diverse materials within internal structures. Second, we introduce an efficient path-partitioning algorithm that resolves the intricate intersection of ellipsoids to compute cumulative attenuation along an X-ray path.\nWe further propose a hybrid progressive initialization to refine the geometric accuracy of X-Field and incorporate material-based optimization to enhance model fitting along material boundaries.\nExperiments show that X-Field achieves superior visual fidelity on both real-world human organ and synthetic object datasets, outperforming state-of-the-art methods in X-ray NVS and CT Reconstruction.",
      "keywords": "['3D Reconstruction; X-ray']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "S8XcHutp7Z",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "nMUpDatZBh",
          "title": "VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics",
          "abstract": "In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose \\textit{Vision In-Context Operator Networks} (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON's adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9\\% compared to DPOT and 44.7\\% compared to MPP, while requiring only 72.5\\% and 34.8\\% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in \\textit{imperfect measurement systems} where sampling frequencies may differ or frames might be dropped—common challenges in real-world settings—without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41\\% relative performance degradation compared to 71.37\\%-74.49\\% degradation in baseline methods, demonstrating its versatility for depolying in realistic applications.",
          "keywords": [
            "AI4Science",
            "Learning PDE",
            "Fluid Dynamics",
            "In-Context Learning"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "VICON is a SOTA vision transformer-based model that efficiently learns multi-PDE operators from 2D data while handling irregular sampling and imperfect measurements, commonly seen in real industry.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=nMUpDatZBh",
          "pdf_link": "https://openreview.net/pdf?id=nMUpDatZBh"
        },
        "paper_internal_id": "nMUpDatZBh",
        "category": "reject",
        "embedding_score": 0.673329770565033,
        "final_score": 0.06197237968444824
      },
      "poster": {
        "paper": {
          "id": "nxGSj1xkm3",
          "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis",
          "abstract": "Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 43.31% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we provide the supervised fine-tuning (SFT) process utilizing our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., Qwen2.5-VL-7B demonstrates a 24.73% improvement. MMOral holds significant potential as a critical foundation for intelligent dentistry and enables more clinically impactful multimodal AI systems in the dental field.",
          "keywords": [
            "Medical benchmark",
            "Multimodal instruction data",
            "Large vision language models"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "We introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral-Bench is a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track poster",
          "forum_link": "https://openreview.net/forum?id=nxGSj1xkm3",
          "pdf_link": "https://openreview.net/pdf?id=nxGSj1xkm3"
        },
        "paper_internal_id": "nxGSj1xkm3",
        "category": "poster",
        "embedding_score": 0.7028053998947144,
        "final_score": 0.48807746171951294
      },
      "oral": {
        "paper": {
          "id": "DwFDfrPsm8",
          "title": "NOVA: A Benchmark for Rare Anomaly Localization and Clinical Reasoning in Brain MRI",
          "abstract": "In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Open-world recognition ensures that such systems remain robust as ever-emerging, previously _unknown_ categories appear and must be addressed without retraining.\nFoundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging.\nHowever, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.\n\nWe therefore present NOVA, a challenging, real-life _evaluation-only_ benchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. \nBecause NOVA is never used for training, it serves as an _extreme_ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space.  \nBaseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops, with approximately a 65\\% gap in localisation compared to natural-image benchmarks and 40\\% and 20\\% gaps in captioning and reasoning, respectively, compared to resident radiologists. Therefore, NOVA establishes a testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.",
          "keywords": [
            "Vision-Language Models",
            "Zero-shot Learning",
            "Anomaly Detection",
            "Dataset Benchmarking",
            "Medical Imaging",
            "Brain MRI",
            "Multi-modal Data",
            "Rare Diseases"
          ],
          "primary_area": "AL/ML_data_and_benchmarks_for_health_sciences",
          "TLDR": "NOVA is an extreme OOD stress-test dataset of ∼900 multi-modal brain MRI scans (with 281 rare pathologies) for benchmarking VLMs on three clinical tasks: anomaly localization, captioning, and diagnostic reasoning.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track oral",
          "forum_link": "https://openreview.net/forum?id=DwFDfrPsm8",
          "pdf_link": "https://openreview.net/pdf?id=DwFDfrPsm8"
        },
        "paper_internal_id": "DwFDfrPsm8",
        "category": "oral",
        "embedding_score": 0.7056851387023926,
        "final_score": 0.2735171616077423
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "ziLHIExi1j",
      "title": "Quantifying First‐Order Markov Breakdowns in Noisy Reinforcement Learning: A Causal Discovery Approach",
      "abstract": "Reinforcement learning (RL) methods often assume that each new observation fully captures the environment’s state, ensuring Markovian (one‐step) transitions. Real‐world deployments, however, frequently violate this assumption due to partial observability or noise in sensors and actuators. This paper introduces a systematic methodology for diagnosing such violations, combining a partial correlation based causal discovery procedure (PCMCI) with a newly proposed Markov Violation score (MVS). The MVS quantifies multi‐step dependencies that emerge when noise or incomplete state information disrupts the Markov property.\n\nClassic control tasks (CartPole, Pendulum, Acrobot) are used to assess how targeted noise and dimension omissions affect both RL performance and the measured Markov consistency. Contrary to expectations, heavy observation noise often fails to induce strong multi‐lag dependencies in certain tasks (e.g., Acrobot). Dimension‐dropping experiments further reveal that omitting certain state variables (e.g., angular velocities in CartPole and Pendulum) substantially degrades returns and elevates MVS, while other dimensions can be removed with negligible effect.\n\nThese findings highlight the importance of identifying and safeguarding the most causally critical dimensions to maintain effective one‐step learning. By bridging partial correlation tests and RL performance metrics, the proposed approach uniquely pinpoints when and where the Markov property breaks. This framework offers a principled tool for designing robust policies, guiding representation learning, and handling partial observability in real‐world RL tasks. All code and experimental logs are publicly available for reproducibility (URL omitted for double‐blind review).",
      "keywords": "['Markov Property', 'PCMCI (Causal Discovery)', 'PPO', 'Noisy Reinforcement Learning.']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "ziLHIExi1j",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "dAAz7afWJR",
          "title": "Robust and Scalable Autonomous Reinforcement Learning in Irreversible Environments",
          "abstract": "Reinforcement learning (RL) typically assumes repetitive resets to provide an agent with diverse and unbiased experiences. These resets require significant human intervention and result in poor training efficiency in real-world settings. Autonomous RL (ARL) addresses this challenge by jointly training forward and reset policies. While recent ARL algorithms have shown promise in reducing human intervention, they assume narrow support over the distributions of initial or goal states and rely on task-specific knowledge to identify irreversible states. In this paper, we propose a robust and scalable ARL algorithm, called RSA, that enables an agent to handle diverse initial and goal states and to avoid irreversible states without task-specific knowledge. RSA generates a curriculum by identifying informative states based on the learning progress of an agent. We hypothesize that informative states are neither overly difficult nor trivially easy for the agent being trained. To detect and avoid irreversible states without task-specific knowledge, RSA encodes the behaviors exhibited in those states rather than the states themselves. Experimental results demonstrate that RSA outperforms existing ARL algorithms with fewer manual resets in both reversible and irreversible environments.",
          "keywords": [
            "Reinforcement Learning",
            "Autonomous Reinforcement Learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=dAAz7afWJR",
          "pdf_link": "https://openreview.net/pdf?id=dAAz7afWJR"
        },
        "paper_internal_id": "dAAz7afWJR",
        "category": "poster",
        "embedding_score": 0.7569173574447632,
        "final_score": 0.9839789867401123
      },
      "spotlight": {
        "paper": {
          "id": "QtnCPZMxYg",
          "title": "Trajectory Graph Learning: Aligning with Long Trajectories in Reinforcement Learning Without Reward Design",
          "abstract": "Reinforcement learning (RL) often relies on manually designed reward functions, which are difficult to specify and can lead to issues such as reward hacking and suboptimal behavior. Alternatives like inverse RL and preference-based RL attempt to infer surrogate rewards from demonstrations or preferences but suffer from ambiguity and distribution mismatch. A more direct approach, inspired by imitation learning, avoids reward modeling by leveraging expert demonstrations. However, most existing methods align actions only at individual states, failing to capture the coherence of long-horizon trajectories.\n\nIn this work, we study the problem of directly aligning policies with expert-labeled trajectories to preserve long-horizon behavior without relying on reward signals. Specifically, we aim to learn a policy that maximizes the probability of generating the expert trajectories. Nevertheless, we prove that, in its general form, this trajectory alignment problem is NP-complete. \nTo address this, we propose Trajectory Graph Learning (TGL), a framework that leverages structural assumptions commonly satisfied in practice—such as bounded realizability of expert trajectories or a tree-structured MDP. These enable a graph-based policy planning algorithm that computes optimal policies in polynomial time under known dynamics. For settings with unknown dynamics, we develop a sample-efficient algorithm based on UCB-style exploration and establish sub-linear regret. Experiments on grid-world tasks demonstrate that TGL substantially outperforms standard imitation learning methods for long-trajectory planning.",
          "keywords": [
            "Reinforcement Learning",
            "Trajectory Alignment",
            "Trajectory Graph Learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=QtnCPZMxYg",
          "pdf_link": "https://openreview.net/pdf?id=QtnCPZMxYg"
        },
        "paper_internal_id": "QtnCPZMxYg",
        "category": "spotlight",
        "embedding_score": 0.7099801898002625,
        "final_score": 0.9743015766143799
      },
      "oral": {
        "paper": {
          "id": "sYK4yPDuT1",
          "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
          "abstract": "Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection.\nIn this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective.",
          "keywords": [
            "data attribution",
            "online reinforcement learning"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We propose the first framework of data attribution for online RL.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-16",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=sYK4yPDuT1",
          "pdf_link": "https://openreview.net/pdf?id=sYK4yPDuT1"
        },
        "paper_internal_id": "sYK4yPDuT1",
        "category": "oral",
        "embedding_score": 0.7290108799934387,
        "final_score": 0.9340669512748718
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "FZURCro04D",
      "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
      "abstract": "Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their reliance on step-by-step reasoning can make them brittle when tasks do not align with such structured approaches. In contrast, human cognition flexibly alternates between fast, intuitive reasoning (System 1) and slow, analytical reasoning (System 2), depending on context. To bridge this gap, we curate a dataset of 2K examples, each with valid responses from both reasoning styles, and explicitly align LLMs with System 1 and System 2 reasoning. Evaluations across diverse reasoning benchmarks reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.",
      "keywords": "['Alignment', 'System 1 and System 2 thinking', 'Cognitive heuristics', 'LLM', 'NLP']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "FZURCro04D",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "b7uniOw0sZ",
          "title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions",
          "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. \nHowever, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. \nTo address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics.\nOur Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning.\nBased on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts LiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct.\nMoreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax.",
          "keywords": [
            "influence functions",
            "llm reasoning"
          ],
          "primary_area": "deep_learning",
          "TLDR": "In this paper, we propose a fine-grained influence function framework to trace how training data on SFT phase shapes LLM reasoning in math and code tasks.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=b7uniOw0sZ",
          "pdf_link": "https://openreview.net/pdf?id=b7uniOw0sZ"
        },
        "paper_internal_id": "b7uniOw0sZ",
        "category": "poster",
        "embedding_score": 0.8205581903457642,
        "final_score": 0.9999071359634399
      },
      "spotlight": {
        "paper": {
          "id": "853SwC2dMZ",
          "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap’s and Zipf’s laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.",
          "keywords": [
            "Large Language Model",
            "Scaling Law",
            "Information Theory"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=853SwC2dMZ",
          "pdf_link": "https://openreview.net/pdf?id=853SwC2dMZ"
        },
        "paper_internal_id": "853SwC2dMZ",
        "category": "spotlight",
        "embedding_score": 0.7868062257766724,
        "final_score": 0.9994654059410095
      },
      "oral": {
        "paper": {
          "id": "Q3qAsZAEZw",
          "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
          "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
          "keywords": [
            "Large Language Models (LLMs)",
            "Reproducibility",
            "Numerical precision",
            "Deterministic inference"
          ],
          "primary_area": "deep_learning",
          "TLDR": "This paper demonstrates that low precision causes non-reproducible LLM inference across different setups, proposing a hybrid-precision method, LayerCast, that computes in FP32 to achieve determinism while saving memory.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=Q3qAsZAEZw",
          "pdf_link": "https://openreview.net/pdf?id=Q3qAsZAEZw"
        },
        "paper_internal_id": "Q3qAsZAEZw",
        "category": "oral",
        "embedding_score": 0.7957755923271179,
        "final_score": 0.9665911793708801
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "n33JVwCz38",
      "title": "Approximate Message Passing for Bayesian Neural Networks",
      "abstract": "Bayesian methods have the ability to consider model uncertainty within a single framework and provide a powerful tool for decision-making. Bayesian neural networks (BNNs) hold great potential for better uncertainty quantification and data efficiency, making them promising candidates for more trustworthy AI in critical applications, and as backbones in data-constrained settings such as real-world reinforcement learning.  However, current approaches often face limitations such as overconfidence, sensitivity to hyperparameters, and posterior collapse, highlighting the need for alternative approaches. In this paper, we introduce a novel method that leverages message passing (MP) to model the predictive posterior of BNNs as a factor graph. Unlike previous MP-based methods, our framework is the first to support convolutional neural networks (CNNs) while addressing the issue of double-counting training data, which has been a key source of overconfidence in prior work. Multiple open datasets are used to demonstrate the general applicability of the method and to illustrate its differences to existing inference methods.",
      "keywords": "['Bayesian Neural Networks', 'Message Passing', 'Uncertainty Quantification', 'Bayesian Inference']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "n33JVwCz38",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "zdRW39Tc3C",
          "title": "Architectural and Inferential Inductive Biases for Exchangeable Sequence Modeling",
          "abstract": "Autoregressive models have emerged as a powerful framework for modeling exchangeable sequences---i.i.d. observations when conditioned on some latent factor---enabling direct modeling of uncertainty from missing data (rather than a latent). Motivated by the critical role posterior inference plays as a subroutine in decision-making (e.g., active learning, bandits), we study the inferential and architectural inductive biases that are most effective  for exchangeable sequence modeling. For the inference stage, we highlight a fundamental limitation of the prevalent single-step generation approach: its inability to distinguish between epistemic and aleatoric uncertainty. Instead, a long line of works in Bayesian statistics advocates for multi-step autoregressive generation; we demonstrate this \"correct approach\" enables superior uncertainty quantification that translates into better performance on downstream decision-making tasks. This naturally leads to the next question: which architectures are best suited for multi-step inference? We identify a subtle yet important gap between recently proposed Transformer architectures for exchangeable sequences (Müller et al., 2022; Nguyen & Grover, 2022; Ye & Namkoong, 2024), and prove that they in fact cannot guarantee exchangeability despite introducing significant computational overhead.  Through empirical evaluation, we find that these custom architectures can significantly underperform compared to standard causal masking, highlighting the need for new architectural innovations in Transformer-based modeling of exchangeable sequences.",
          "keywords": [
            "Exchangeability",
            "Inductive Bias",
            "Epistemic uncertainty",
            "Multi-step inference",
            "Transformer architecture"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We investiage the use of autoregressive models for exchangeable sequences in decision-making, showing multi-step inference improves decision making and standard causal architectures outperform existing custom ones.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=zdRW39Tc3C",
          "pdf_link": "https://openreview.net/pdf?id=zdRW39Tc3C"
        },
        "paper_internal_id": "zdRW39Tc3C",
        "category": "poster",
        "embedding_score": 0.7430629134178162,
        "final_score": 0.7237180471420288
      },
      "spotlight": {
        "paper": {
          "id": "FEugj28qhC",
          "title": "BayeSQP: Bayesian Optimization through Sequential Quadratic Programming",
          "abstract": "We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.",
          "keywords": [
            "Bayesian optimization",
            "sequential quadratic programming",
            "constrained optimization"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "We introduce BayeSQP, a novel black-box optimization algorithm that combines sequential quadratic programming with Bayesian optimization for high-dimensional constrained problems..",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=FEugj28qhC",
          "pdf_link": "https://openreview.net/pdf?id=FEugj28qhC"
        },
        "paper_internal_id": "FEugj28qhC",
        "category": "spotlight",
        "embedding_score": 0.6992731094360352,
        "final_score": 0.41036027669906616
      },
      "oral": {
        "paper": {
          "id": "gxfusMqPIs",
          "title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
          "abstract": "This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). \nUnder a Mat\\'ern kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\\tilde{O}(\\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\\sqrt{T \\ln^2 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner.",
          "keywords": [
            "Gaussian process bandits",
            "regret analysis",
            "Bayesian optimization"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=gxfusMqPIs",
          "pdf_link": "https://openreview.net/pdf?id=gxfusMqPIs"
        },
        "paper_internal_id": "gxfusMqPIs",
        "category": "oral",
        "embedding_score": 0.6864864826202393,
        "final_score": 0.12224707752466202
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "KkOMqJQiWU",
      "title": "Meta-learning local learning rules for structured credit assignment with sparse feedback",
      "abstract": "Biological neural networks can learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via **backpropagation through learning**. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.",
      "keywords": "['Biologically Plausible Deep Networks', 'Plasticity and Adaptation', 'Recurrent Networks', 'Reinforcement Learning (Cognitive/Neuroscience)']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "KkOMqJQiWU",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "YTbLri0siT",
          "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent",
          "abstract": "Hebbian learning is a key principle underlying learning in biological neural networks. We relate a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent with respect to a non-convex loss function on the probability simplex. Despite the constant injection of noise and the non-convexity of the underlying optimization problem, one can rigorously prove that the considered Hebbian learning dynamic identifies the presynaptic neuron with the highest activity and that the convergence is exponentially fast in the number of iterations. This is non-standard and surprising as typically noisy gradient descent with fixed noise level only converges to a stationary regime where the noise causes the dynamic to fluctuate around a minimiser.",
          "keywords": [
            "Biological neural networks",
            "Hebbian learning",
            "Spike-timing-dependent plasticity",
            "Noisy gradient descent",
            "Mirror descent"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "Spike-timing-dependent plasticity can be rephrased as noisy gradient, which allows to obtain convergence guarantees for the stochastic learning dynamics.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=YTbLri0siT",
          "pdf_link": "https://openreview.net/pdf?id=YTbLri0siT"
        },
        "paper_internal_id": "YTbLri0siT",
        "category": "poster",
        "embedding_score": 0.7618743181228638,
        "final_score": 0.7281457185745239
      },
      "spotlight": {
        "paper": {
          "id": "y0wDflmpLk",
          "title": "Continuous Thought Machines",
          "abstract": "Biological brains demonstrate complex neural activity, where neural dynamics are critical to how brains process information. Most artificial neural networks ignore the complexity of individual neurons. We challenge that paradigm. By incorporating neuron-level processing and synchronization, we reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two innovations: (1) neuron-level temporal processing}, where each neuron uses unique weight parameters to process incoming histories; and (2) neural synchronization as a latent representation. The CTM aims to strike a balance between neuron abstractions and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable. We demonstrate the CTM's performance and versatility across a range of tasks, including solving 2D mazes, ImageNet-1K classification, parity computation, and more. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. We provide an accompanying [interactive online demonstration](https://pub.sakana.ai/ctm/) and an [extended technical report](https://pub.sakana.ai/ctm/paper).",
          "keywords": [
            "Biologically Inspired Neural Network",
            "Neural Dynamics",
            "Neural Synchronization",
            "Reasoning Model"
          ],
          "primary_area": "deep_learning",
          "TLDR": "Neurons in brains use timing and synchronization in the way that they compute, so we built a model that does the same.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=y0wDflmpLk",
          "pdf_link": "https://openreview.net/pdf?id=y0wDflmpLk"
        },
        "paper_internal_id": "y0wDflmpLk",
        "category": "spotlight",
        "embedding_score": 0.7544168829917908,
        "final_score": 0.7728704810142517
      },
      "oral": {
        "paper": {
          "id": "RF3miSqdXa",
          "title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
          "abstract": "Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes\nof neural networks, wherein independently trained models have been observed to\nbe connected—up to permutation symmetries—by linear paths in parameter space\nalong which the loss remains consistently low. This observation challenges classical\nviews of non-convex optimization and has implications for model ensembling,\ngeneralization, and our understanding of neural loss geometry. Inspired by recent\nstudies on LMC in standard neural networks, we systematically investigate this\nphenomenon within Mixture-of-Experts (MoE) architectures—a class of models\nknown for their scalability and computational efficiency, which combine traditional\nneural networks—referred to as experts—through a learnable gating mechanism.\nWe begin by conducting a comprehensive analysis of both dense and sparse gating\nregimes, demonstrating that the symmetries inherent to MoE architectures are\nfully characterized by permutations acting on both the expert components and the\ngating function. Building on these foundational findings, we propose a matching\nalgorithm that enables alignment between independently trained MoEs, thereby\nfacilitating the discovery of LMC. Finally, we empirically validate the presence of\nLMC using our proposed algorithm across diverse MoE configurations—including\ndense, sparse, and shared-expert variants—under a wide range of model settings\nand datasets of varying scales and modalities. Our results confirm the existence\nof LMC in MoE architectures and offer fundamental insights into the functional\nlandscape and optimization dynamics of deep learning models.",
          "keywords": [
            "linear mode connectivity",
            "mixture-of-experts"
          ],
          "primary_area": "theory",
          "TLDR": "We investigate Linear Mode Connectivity (LMC) in Mixture-of-Experts (MoE) architectures by analyzing their underlying permutation symmetries and proposing expert-matching algorithms that align independently trained MoEs to reveal LMC.",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=RF3miSqdXa",
          "pdf_link": "https://openreview.net/pdf?id=RF3miSqdXa"
        },
        "paper_internal_id": "RF3miSqdXa",
        "category": "oral",
        "embedding_score": 0.7076443433761597,
        "final_score": 0.24081169068813324
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "krF62hkrfR",
      "title": "Neural Bayesian Filtering",
      "abstract": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining posteriors, called beliefs, over hidden states in partially observable systems.\nNBF is trained to find a good latent representation of the beliefs induced by a task.\nIt maps beliefs to fixed-length embedding vectors, which can condition generative models for sampling.\nDuring filtering, particle-style updates compute posteriors in this embedding space using incoming observations and environment dynamics.\nNBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of *particle impoverishment*.\nWe validate NBF in state estimation tasks in partially observable variants of Gridworld and the card game Goofspiel.",
      "keywords": "['Partially observable systems', 'belief state modeling', 'particle filtering', 'bayesian filtering', 'normalizing flows']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "krF62hkrfR",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "fqfYzp4GKi",
          "title": "Martingale Posterior Neural Networks for Fast Sequential Decision Making",
          "abstract": "We introduce scalable algorithms for online learning of neural network parameters and Bayesian sequential decision making.\nUnlike classical Bayesian neural networks,\nwhich induce predictive uncertainty through a posterior over model parameters,\nour methods adopt a predictive-first perspective based on martingale posteriors.\nIn particular, we work directly with the one-step-ahead posterior predictive, which we\nparameterize with a neural network and update sequentially with incoming observations.\nThis decouples Bayesian decision-making from parameter-space inference:\nwe sample from the posterior predictive for decision making,\nand update the parameters of the posterior predictive via fast, frequentist Kalman-filter-like\nrecursions. \nOur algorithms operate in a fully online, replay-free setting, providing principled uncertainty quantification without costly posterior sampling.\nEmpirically, they achieve competitive performance–speed trade-offs in non-stationary contextual bandits and Bayesian optimization,\noffering 10–100 times faster inference than classical Thompson sampling while maintaining comparable or superior decision performance.",
          "keywords": [
            "online learning",
            "neural bandits",
            "sequential decision making",
            "Kalman filtering",
            "frequentist",
            "bayes",
            "martingale posterior"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "Scalable online training of neural networks using frequentist filtering with martingale-posterior-inspired posteriors for sequential decision making.",
          "creation_date": "2025-05-05",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=fqfYzp4GKi",
          "pdf_link": "https://openreview.net/pdf?id=fqfYzp4GKi"
        },
        "paper_internal_id": "fqfYzp4GKi",
        "category": "poster",
        "embedding_score": 0.7442712187767029,
        "final_score": 0.8143778443336487
      },
      "spotlight": {
        "paper": {
          "id": "rVyBrD8h2b",
          "title": "Preconditioned Langevin Dynamics with Score-based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems",
          "abstract": "Designing algorithms for solving high-dimensional Bayesian inverse problems directly in infinite‑dimensional function spaces – where such problems are naturally formulated – is crucial to ensure stability and  convergence as the discretization of the underlying  problem is refined. In this paper, we contribute to this line of work by analyzing a widely used sampler for linear inverse problems: Langevin dynamics driven by score‑based generative models (SGMs) acting as priors, formulated directly in function space. Building on the  theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition of this sampler in the infinite-dimensional setting and derive, for the first time, error estimates that explicitly depend on the approximation error of the score. As a consequence, we obtain sufficient conditions for global convergence in Kullback–Leibler divergence on the underlying function space. Preventing numerical instabilities requires preconditioning of the Langevin algorithm and we prove the existence  and form of an optimal preconditioner. The preconditioner depends on both the score error and the forward operator and guarantees a uniform convergence rate across all posterior modes. Our analysis applies to both Gaussian and a general class of non‑Gaussian priors. Finally, we present examples that illustrate and validate our theoretical findings.",
          "keywords": [
            "theory",
            "score-based generative models",
            "error analysis",
            "hilbert space",
            "langevin dynamics",
            "preconditioning"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=rVyBrD8h2b",
          "pdf_link": "https://openreview.net/pdf?id=rVyBrD8h2b"
        },
        "paper_internal_id": "rVyBrD8h2b",
        "category": "spotlight",
        "embedding_score": 0.6882609128952026,
        "final_score": 0.364663302898407
      },
      "oral": {
        "paper": {
          "id": "uWj4s7rMnR",
          "title": "Mean Flows for One-step Generative Modeling",
          "abstract": "We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \\textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256$\\times$256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.",
          "keywords": [
            "Generative Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-04-06",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=uWj4s7rMnR",
          "pdf_link": "https://openreview.net/pdf?id=uWj4s7rMnR"
        },
        "paper_internal_id": "uWj4s7rMnR",
        "category": "oral",
        "embedding_score": 0.684759259223938,
        "final_score": 0.013739910908043385
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "G10Y4vrhGF",
      "title": "FedFree: Breaking Knowledge-sharing Barriers through Layer-wise Alignment in Heterogeneous Federated Learning",
      "abstract": "Heterogeneous Federated Learning (HtFL) enables collaborative learning across clients with diverse model architectures and non-IID data distributions, which are prevalent in real-world edge computing applications. Existing HtFL approaches typically employ proxy datasets to facilitate knowledge sharing or implement coarse-grained model-level knowledge transfer. However, such approaches not only elevate risks of user privacy leakage but also lead to the loss of fine-grained model-specific knowledge, ultimately creating barriers to effective knowledge sharing. To address these challenges, we propose FedFree, a novel data-free and model-free HtFL framework featuring two key innovations. First, FedFree introduces a reverse layer-wise knowledge transfer mechanism that aggregates heterogeneous client models into a global model solely using Gaussian-based pseudo data, eliminating reliance on proxy datasets. Second, it leverages Knowledge Gain Entropy (KGE) to guide targeted layer-wise knowledge alignment, ensuring that each client receives the most relevant global updates tailored to its specific architecture. We provide rigorous theoretical convergence guarantees for FedFree and conduct extensive experiments on CIFAR-10 and CIFAR-100. Results demonstrate that FedFree achieves substantial performance gains, with relative accuracy improving up to 46.3% over state-of-the-art baselines. The framework consistently excels under highly heterogeneous model/data distributions and in large scale settings.",
      "keywords": "['Heterogeneous Federated Learning', 'Public-Data-Free', 'Knowledge Gain Entropy']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "G10Y4vrhGF",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "InyYuWLWHD",
          "title": "LayerGuard: Poisoning-Resilient Federated Learning via Layer-Wise Similarity Analysis",
          "abstract": "In recent years, model poisoning attacks have gradually evolved from conventional global parameter manipulations to more stealthy and strategic Targeted Layer Poisoning (TLP) attacks.These attacks achieve high attack success rates by selectively poisoning only a subset of layers. However, most existing defenses rely on evaluation of the entire network and are thus ineffective against TLP attacks, posing new challenges to the security of Federated Learning (FL).In this paper, we propose \\textbf{LayerGuard}, a comprehensive defense framework featuring dynamic detection and adaptive aggregation to protect FL against advanced model poisoning attacks. Diverging from traditional methods that analyze the entire network collectively, \\textbf{LayerGuard} performs layer-wise similarity analysis to detect anomalous clients and adaptively identifies layers under attack based on the clustering behavior of malicious updates, facilitating more precise threat detection. Building on this, we introduce a joint weighting mechanism in the aggregation process, which evaluates each client's credibility at the layer level from two complementary informational dimensions: inter-layer and intra-layer, balancing attack mitigation and benign contribution retention. Extensive experiments across various datasets and model architectures demonstrate that \\textbf{LayerGuard} successfully reduces the average attack success rate of TLP attacks to around 5\\%. Moreover, when confronted with other advanced model poisoning attacks, \\textbf{LayerGuard} consistently maintains global model accuracy—even under high poisoning rates and severe non-IID conditions—comparable to that of FedAvg under no-attack settings, marking a significant improvement over existing defenses.",
          "keywords": [
            "Federated Learning; Security; Model Poisoning Attacks; Robust Aggregation"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "A new FL defense achieves state-of-the-art robustness against advanced model poisoning attacks and effectively counters the emerging threat of Targeted Layer Poisoning (TLP) attacks.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=InyYuWLWHD",
          "pdf_link": "https://openreview.net/pdf?id=InyYuWLWHD"
        },
        "paper_internal_id": "InyYuWLWHD",
        "category": "reject",
        "embedding_score": 0.7089737057685852,
        "final_score": 0.8930267095565796
      },
      "spotlight": {
        "paper": {
          "id": "b7waOsMnq8",
          "title": "Sharp Gaussian approximations for Decentralized Federated Learning",
          "abstract": "Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.",
          "keywords": [
            "Federated Learning",
            "Distributed Systems",
            "SGD",
            "Gaussian approximation"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=b7waOsMnq8",
          "pdf_link": "https://openreview.net/pdf?id=b7waOsMnq8"
        },
        "paper_internal_id": "b7waOsMnq8",
        "category": "spotlight",
        "embedding_score": 0.6407217979431152,
        "final_score": 0.9048391580581665
      },
      "oral": {
        "paper": {
          "id": "CaSQgef484",
          "title": "Exploring Diffusion Transformer Designs via Grafting",
          "abstract": "Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation.\nInspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present *grafting*, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs. 2.27 for DiT-XL/2)\nusing $<2$% pretraining compute. We then graft a text-to-image model (PixArt-$\\Sigma$), achieving a 1.43$\\times$ speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2$\\times$ and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu.",
          "keywords": [
            "Diffusion Transformers",
            "Model Grafting",
            "Architectural Editing",
            "Hybrid Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose grafting, a simple approach to materialize new architectures by editing pretrained diffusion transformers. It enables architectural exploration under small compute budgets.",
          "creation_date": "2025-05-01",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=CaSQgef484",
          "pdf_link": "https://openreview.net/pdf?id=CaSQgef484"
        },
        "paper_internal_id": "CaSQgef484",
        "category": "oral",
        "embedding_score": 0.6237231492996216,
        "final_score": 0.7696700692176819
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Jzr9VOiJYd",
      "title": "Contimask: Explaining Irregular Time Series via Perturbations in Continuous Time",
      "abstract": "Explaining black-box models for time series data is critical for the wide-scale adoption of deep learning techniques across domains such as healthcare. Recently, explainability methods for deep time series models have seen significant progress by adopting saliency methods that perturb masked segments of time series to uncover their importance towards the prediction of black-box models. Thus far, such methods have been largely restricted to regular time series. Irregular time series, however, sampled at irregular time intervals and potentially with missing values, are the dominant form of time series in various critical domains (e.g., hospital records). In this paper, we conduct the first evaluation of saliency methods for the interpretation of irregular time series models. We first translate techniques for regular time series into the continuous time realm of irregular time series and show under which circumstances such techniques are still applicable. However, existing perturbation techniques neglect the timing and structure of observed data, e.g., informative missingness when data is not missing at random. Thus, we propose Contimask, a simple framework to also apply non-differentiable perturbations, such as simulating that parts of the data had not been observed using NeuroEvolution. Doing so, we successfully detect how structural differences in the data can bias irregular time series models on a real-world sepsis prediction task where 90% of the data is missing. Source code is available on GitHub.",
      "keywords": "['Irregular Time Series', 'Explainability', 'Interpretability', 'Explanations', 'Perturbations']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Jzr9VOiJYd",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "K61Y6cTMRl",
          "title": "Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals",
          "abstract": "Time-series foundation models excel at tasks like forecasting across diverse data types by leveraging informative waveform representations. Wearable sensing data, however, pose unique challenges due to their variability in patterns and frequency bands, especially for healthcare-related outcomes. The main obstacle lies in crafting generalizable representations that adapt efficiently across heterogeneous sensing configurations and applications. To address this, we propose NormWear, the first multi-modal and ubiquitous foundation model designed to extract generalized and informative representations from wearable sensing data. Specifically, we design a channel-aware attention mechanism with a shared special liaison [CLS] token to detect signal patterns in both intra-sensor and inter-sensors. This helps the model to extract more meaningful information considering both time series themselves and the relationships between input sensors. This helps the model to be widely compatible with various sensors settings. NormWear is pretrained on a diverse set of physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various public datasets. Our model shows exceptional generalizability across 11 public wearable sensing datasets, spanning 18 applications in mental health, body state inference, vital sign estimation, and disease risk evaluation. It consistently outperforms competitive baselines under zero-shot, partial-shot, and full-shot settings, indicating broad applicability in real-world health applications.",
          "keywords": [
            "Foundation Model",
            "Signal Processing",
            "Time Series",
            "Wearable Sensing",
            "Digital Health"
          ],
          "primary_area": "deep_learning",
          "TLDR": "NormWear: A Normative Foundation Model that Learns Representations of Multimodal Wearable Sensing Signals for Diverse Digital Healthcare Applications",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=K61Y6cTMRl",
          "pdf_link": "https://openreview.net/pdf?id=K61Y6cTMRl"
        },
        "paper_internal_id": "K61Y6cTMRl",
        "category": "reject",
        "embedding_score": 0.7069482803344727,
        "final_score": 0.9428592920303345
      },
      "spotlight": {
        "paper": {
          "id": "UwtFuWbW6B",
          "title": "Non-Asymptotic Analysis Of Data Augmentation For Precision Matrix Estimation",
          "abstract": "This paper addresses the problem of inverse covariance (also known as precision matrix) estimation in high-dimensional settings. Specifically, we focus on two classes of estimators: linear shrinkage estimators with a target proportional to the identity matrix, and estimators derived from data augmentation (DA). Here, DA refers to the common practice of enriching a dataset with artificial samples—typically generated via a generative model or through random transformations of the original data—prior to model fitting.\nFor both classes of estimators, we derive estimators and provide concentration bounds for their quadratic error. This allows for both method comparison and hyperparameter tuning, such as selecting the optimal proportion of artificial samples.\nOn the technical side, our analysis relies on tools from random matrix theory. We introduce a novel deterministic equivalent for generalized resolvent matrices, accommodating dependent samples with specific structure. We support our theoretical results with numerical experiments.",
          "keywords": [
            "Data Augmentation",
            "Precision Matrix",
            "High Dimensional Statistics",
            "Random Matrix Theory"
          ],
          "primary_area": "theory",
          "TLDR": "We introduce a data-centric estimator of the squared error of usual precision matrix estimates, both in the non-augmented case and the augmented case. Furthermore, we give non-asymptotic guarantees on our estimate.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=UwtFuWbW6B",
          "pdf_link": "https://openreview.net/pdf?id=UwtFuWbW6B"
        },
        "paper_internal_id": "UwtFuWbW6B",
        "category": "spotlight",
        "embedding_score": 0.6494122743606567,
        "final_score": 0.9491335153579712
      },
      "oral": {
        "paper": {
          "id": "jMhRbV47pS",
          "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
          "abstract": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
          "keywords": [
            "emergence",
            "sparse attention",
            "in-context learning",
            "induction head"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We show that learning sparse attention is prone to emerging behaviors during training, and study (theoretically and empirically) how data and model design influence emergence speed.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=jMhRbV47pS",
          "pdf_link": "https://openreview.net/pdf?id=jMhRbV47pS"
        },
        "paper_internal_id": "jMhRbV47pS",
        "category": "oral",
        "embedding_score": 0.6581610441207886,
        "final_score": 0.9380475282669067
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "mPuOMcN9E7",
      "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
      "abstract": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged—motivated by PbRL’s recent empirical success, particularly in aligning large language models (LLMs)—most existing studies focus only on pairwise comparisons. A few recent works  (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024)  have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve—and can even deteriorate—as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett–Luce (PL) model for ranking feedback over action subsets and propose **M-AUPO**, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that **M-AUPO** achieves a suboptimality gap of $\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter’s norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
      "keywords": "['Preference-based Reinforcement Learning', 'Ranking Feedback', 'Plackett–Luce Model', 'Reinforcement Learning from Human Feedback', 'Dueling Bandit']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "mPuOMcN9E7",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "vMfJM9oBYL",
          "title": "Learning from Preferences and Mixed Demonstrations in General Settings",
          "abstract": "Reinforcement learning is a general method for learning in sequential settings, but it can often be difficult to specify a good reward function when the task is complex.\nIn these cases, preference feedback or expert demonstrations can be used instead.\nHowever, existing approaches utilising both together are either ad-hoc or rely on domain-specific properties.\nBuilding upon previous work, we develop a mathematical framework for learning from human data and based on this we introduce LEOPARD: Learning Estimated Objectives from Preferences And Ranked Demonstrations.\nLEOPARD can simultaneously learn from a broad range of data, including negative/failed demonstrations, to effectively learn reward functions in general domains.\nIt does this by modelling the human feedback as reward-rational partial orderings over available trajectories.\nWe find that when a limited amount of preference and demonstration feedback is available, LEOPARD outperforms baselines by a significant margin.\nFurthermore, we use LEOPARD to investigate learning from many types of feedback compared to just a single one, and find that a combination of feedback types is often beneficial.",
          "keywords": [
            "reinforcement learning",
            "rl",
            "human feedback",
            "rlhf",
            "modelling",
            "preferences",
            "demonstrations",
            "rankings",
            "machine learning",
            "reward learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "We introduce a principled method for learning reward functions in RL from preferences, and ranked positive and negative demonstrations.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=vMfJM9oBYL",
          "pdf_link": "https://openreview.net/pdf?id=vMfJM9oBYL"
        },
        "paper_internal_id": "vMfJM9oBYL",
        "category": "reject",
        "embedding_score": 0.7689007520675659,
        "final_score": 0.8158180713653564
      },
      "spotlight": {
        "paper": {
          "id": "7ZVRlBFuEv",
          "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
          "abstract": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL).\nThese capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. \nIn contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning.\nTo this end, we propose, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.",
          "keywords": [
            "diffusion language models",
            "post-training",
            "reinforcement learning",
            "reasoning",
            "large language models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=7ZVRlBFuEv",
          "pdf_link": "https://openreview.net/pdf?id=7ZVRlBFuEv"
        },
        "paper_internal_id": "7ZVRlBFuEv",
        "category": "spotlight",
        "embedding_score": 0.718053936958313,
        "final_score": 0.8954718112945557
      },
      "oral": {
        "paper": {
          "id": "sYK4yPDuT1",
          "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
          "abstract": "Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection.\nIn this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective.",
          "keywords": [
            "data attribution",
            "online reinforcement learning"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We propose the first framework of data attribution for online RL.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-16",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=sYK4yPDuT1",
          "pdf_link": "https://openreview.net/pdf?id=sYK4yPDuT1"
        },
        "paper_internal_id": "sYK4yPDuT1",
        "category": "oral",
        "embedding_score": 0.7208089828491211,
        "final_score": 0.9583553671836853
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "yRxX01oRIi",
      "title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps",
      "abstract": "Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning—inferring latent rules from sparse examples—remains limited. \nIt is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. \nWe investigate this assumption with creating four controlled, diagnostic game-based tasks—chess, Texas Hold’em, dice games, and blackjack—with hidden human-defined rules. \nWe find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.\n\nTo explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. \nBased on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",
      "keywords": "['Large Lauange Model', 'Inductive Abilities', 'Reasoning']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "yRxX01oRIi",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "FZURCro04D",
          "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
          "abstract": "Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their reliance on step-by-step reasoning can make them brittle when tasks do not align with such structured approaches. In contrast, human cognition flexibly alternates between fast, intuitive reasoning (System 1) and slow, analytical reasoning (System 2), depending on context. To bridge this gap, we curate a dataset of 2K examples, each with valid responses from both reasoning styles, and explicitly align LLMs with System 1 and System 2 reasoning. Evaluations across diverse reasoning benchmarks reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.",
          "keywords": [
            "Alignment",
            "System 1 and System 2 thinking",
            "Cognitive heuristics",
            "LLM",
            "NLP"
          ],
          "primary_area": "neuroscience_and_cognitive_science",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=FZURCro04D",
          "pdf_link": "https://openreview.net/pdf?id=FZURCro04D"
        },
        "paper_internal_id": "FZURCro04D",
        "category": "reject",
        "embedding_score": 0.8155100345611572,
        "final_score": 0.8260189890861511
      },
      "spotlight": {
        "paper": {
          "id": "roKj4IwaVT",
          "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
          "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
          "keywords": [
            "LLM",
            "reasoning",
            "parallel generation"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose a parallel generation method for LLMs, where multiple instances synchronize through a shared, dynamically-updated attention cache",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=roKj4IwaVT",
          "pdf_link": "https://openreview.net/pdf?id=roKj4IwaVT"
        },
        "paper_internal_id": "roKj4IwaVT",
        "category": "spotlight",
        "embedding_score": 0.770415186882019,
        "final_score": 0.9527609944343567
      },
      "oral": {
        "paper": {
          "id": "Q3qAsZAEZw",
          "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
          "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
          "keywords": [
            "Large Language Models (LLMs)",
            "Reproducibility",
            "Numerical precision",
            "Deterministic inference"
          ],
          "primary_area": "deep_learning",
          "TLDR": "This paper demonstrates that low precision causes non-reproducible LLM inference across different setups, proposing a hybrid-precision method, LayerCast, that computes in FP32 to achieve determinism while saving memory.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=Q3qAsZAEZw",
          "pdf_link": "https://openreview.net/pdf?id=Q3qAsZAEZw"
        },
        "paper_internal_id": "Q3qAsZAEZw",
        "category": "oral",
        "embedding_score": 0.7241644859313965,
        "final_score": 0.9738940596580505
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "g2vViuEVDS",
      "title": "Intrinsic Goals for Autonomous Agents: Model-Based Exploration in Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics",
      "abstract": "Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in reward-free environments, including a class of methods known as *model-based intrinsic motivation*, exhibit inconsistent exploration patterns and do not converge to an exploratory policy, thus failing to capture robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in ethological, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed after the principles of autonomous exploration in animals. Our method (3M-Progress) achieves animal-like exploration by tracking divergence between an online world model and a fixed prior learned from an ecological niche. To the best of our knowledge, we introduce the first autonomous embodied agent that predicts brain data entirely from self-supervised optimization of an intrinsic goal—without any behavioral or neural training data—demonstrating that 3M-Progress agents capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously behaving larval zebrafish, thereby providing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.",
      "keywords": "['NeuroAI', 'intrinsic motivation', 'zebrafish', 'neural-glial', 'embodied agents', 'reinforcement learning', 'autonomy.']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "g2vViuEVDS",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "eR8raBLZW7",
          "title": "BriLLM: Brain-inspired Large Language Model",
          "abstract": "This paper reports the brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of \"least resistance\" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM versions in Chinese and English, with 4000 tokens, 32-dimensional node size, 32-token sequence prediction ability, model sizes around 2B and 1B respectively, bringing language model prediction performance comparable to GPT-1.",
          "keywords": [
            "LLM"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=eR8raBLZW7",
          "pdf_link": "https://openreview.net/pdf?id=eR8raBLZW7"
        },
        "paper_internal_id": "eR8raBLZW7",
        "category": "reject",
        "embedding_score": 0.6499362587928772,
        "final_score": 0.4287213981151581
      },
      "spotlight": {
        "paper": {
          "id": "3a18D8IeQ1",
          "title": "Quantization-Free Autoregressive Action Transformer",
          "abstract": "Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.",
          "keywords": [
            "Imitation Learning",
            "Reinforcement Learning",
            "Transformers"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "We introduce a quantization-free way to train autoregressive transformers for continuous action decision making, improving on discretized action methods.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=3a18D8IeQ1",
          "pdf_link": "https://openreview.net/pdf?id=3a18D8IeQ1"
        },
        "paper_internal_id": "3a18D8IeQ1",
        "category": "spotlight",
        "embedding_score": 0.6307863593101501,
        "final_score": 0.784982442855835
      },
      "oral": {
        "paper": {
          "id": "1IpHkK5Q8F",
          "title": "Real-Time Hyper-Personalized Generative AI Should Be Regulated to Prevent the Rise of \"Digital Heroin\"",
          "abstract": "This position paper argues that real-time generative AI has the potential to become the next wave of addictive digital media, creating a new class of digital content akin to ``digital heroin'' with severe implications for mental health and youth development. By shortening the content-generation feedback loop to mere seconds, these advanced models will soon be able to hyper-personalize outputs on the fly. When paired with misaligned incentives (e.g., maximizing user engagement), this will fuel unprecedented compulsive consumption patterns with far-reaching consequences for mental health, cognitive development, and social stability. Drawing on interdisciplinary research, from clinical observations of social media addiction to neuroscientific studies of dopamine-driven feedback, we illustrate how real-time tailored content generation may erode user autonomy, foment emotional distress, and disproportionately endanger vulnerable groups, such as adolescents. Due to the rapid advancement of generative AI and its potential to induce severe addiction-like effects, we call for strong government oversight akin to existing controls on addictive substances, particularly for minors. We further urge the machine learning community to act proactively by establishing robust design guidelines, collaborating with public health experts, and supporting targeted policy measures to ensure responsible and ethical deployment, rather than paving the way for another wave of unregulated digital dependence.",
          "keywords": [
            "generative ai",
            "real-time personalization",
            "behavioral addiction",
            "digital media",
            "public health",
            "policy interventions",
            "machine learning ethics"
          ],
          "primary_area": "generative ai",
          "TLDR": "",
          "creation_date": "2025-05-21",
          "original_date": "2025-10-29",
          "modification_date": "2025-11-08",
          "venue": "NeurIPS 2025 Position Paper Track Oral",
          "forum_link": "https://openreview.net/forum?id=1IpHkK5Q8F",
          "pdf_link": "https://openreview.net/pdf?id=1IpHkK5Q8F"
        },
        "paper_internal_id": "1IpHkK5Q8F",
        "category": "oral",
        "embedding_score": 0.6441711783409119,
        "final_score": 0.3873012065887451
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "iQoZv77o3g",
      "title": "Predicting Functional Brain Connectivity with Context-Aware Deep Neural Networks",
      "abstract": "Spatial location and molecular interactions have long been linked to the connectivity patterns of neural circuits. Yet, at the macroscale of human brain networks, the interplay between spatial position, gene expression, and connectivity remains incompletely understood. Recent efforts to map the human transcriptome and connectome have yielded spatially resolved brain atlases, however modeling the relationship between high-dimensional transcriptomic data and connectivity while accounting for inherent spatial confounds presents a significant challenge. In this paper, we present the first deep learning approaches for predicting whole-brain functional connectivity from gene expression and regional spatial coordinates, including our proposed Spatiomolecular Transformer (SMT). SMT explicitly models biological context by tokenizing genes based on their transcription start site (TSS) order to capture multi-scale genomic organization, and incorporating regional 3D spatial location via a dedicated context [CLS] token within its multi-head self-attention mechanism. We rigorously benchmark context-aware neural networks, including SMT and a single-gene resolution Multilayer-Perceptron (MLP), to established rules-based and bilinear methods. Crucially, to ensure that learned relationships in any model are not mere artifacts of spatial proximity, we introduce novel  spatiomolecular null maps, preserving both spatial and transcriptomic autocorrelation. Context-aware neural networks outperform linear methods, significantly exceed our stringent null shuffle models, and generalize across diverse connectomic datasets and parcellation resolutions. Together, these findings demonstrate a strong, predictable link between the spatial distributions of gene expression and functional brain network architecture, and establish a rigorously validated deep learning framework for decoding this relationship. Code to reproduce our results is available at: github.com/neuroinfolab/GeneEx2Conn.",
      "keywords": "['neuroscience', 'fMRI', 'connectomics', 'transcriptomics', 'attention']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "iQoZv77o3g",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "WOXyOiVd4B",
          "title": "FragFM: Hierarchical Framework for Efficient Molecule Generation via Fragment-Level Discrete Flow Matching",
          "abstract": "We introduce FragFM, a novel hierarchical framework via fragment-level discrete flow matching for efficient molecular graph generation. FragFM generates molecules at the fragment level, leveraging a coarse-to-fine autoencoder to reconstruct details at the atom level. Together with a stochastic fragment bag strategy to effectively handle an extensive fragment space, our framework enables more efficient and scalable molecular generation. We demonstrate that our fragment-based approach achieves better property control than the atom-based method and additional flexibility through conditioning the fragment bag. We also propose a Natural Product Generation benchmark (NPGen) to evaluate modern molecular graph generative models' ability to generate natural product-like molecules. Since natural products are biologically prevalidated and differ from typical drug-like molecules, our benchmark provides a more challenging yet meaningful evaluation relevant to drug discovery. We conduct a FragFM comparative study against various models on diverse molecular generation benchmarks, including NPGen, demonstrating superior performance. The results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.",
          "keywords": [
            "Molecular Graph Generation",
            "Discrete Flow Matching",
            "Fragment-Based Drug Discovery",
            "Natural Product"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "We introduce FragFM, a novel hierarchical framework employing fragment‐level discrete flow matching for efficient molecular graph generation, along with a new molecular generative benchmark focused on natural products.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=WOXyOiVd4B",
          "pdf_link": "https://openreview.net/pdf?id=WOXyOiVd4B"
        },
        "paper_internal_id": "WOXyOiVd4B",
        "category": "reject",
        "embedding_score": 0.6640031337738037,
        "final_score": 0.09451903402805328
      },
      "spotlight": {
        "paper": {
          "id": "PZaxCfLGLA",
          "title": "3D Interaction Geometric Pre-training for Molecular Relational Learning",
          "abstract": "Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. \nDespite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive.\nThis paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. \nWith the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the global and local 3D geometric information of molecular interaction.\nExtensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93% improvement in performance across 40 tasks.\nOur code is publicly available at https://github.com/Namkyeong/3DMRL.",
          "keywords": [
            "molecular science",
            "molecular relational learning"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=PZaxCfLGLA",
          "pdf_link": "https://openreview.net/pdf?id=PZaxCfLGLA"
        },
        "paper_internal_id": "PZaxCfLGLA",
        "category": "spotlight",
        "embedding_score": 0.6573716402053833,
        "final_score": 0.39538800716400146
      },
      "oral": {
        "paper": {
          "id": "RF3miSqdXa",
          "title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
          "abstract": "Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes\nof neural networks, wherein independently trained models have been observed to\nbe connected—up to permutation symmetries—by linear paths in parameter space\nalong which the loss remains consistently low. This observation challenges classical\nviews of non-convex optimization and has implications for model ensembling,\ngeneralization, and our understanding of neural loss geometry. Inspired by recent\nstudies on LMC in standard neural networks, we systematically investigate this\nphenomenon within Mixture-of-Experts (MoE) architectures—a class of models\nknown for their scalability and computational efficiency, which combine traditional\nneural networks—referred to as experts—through a learnable gating mechanism.\nWe begin by conducting a comprehensive analysis of both dense and sparse gating\nregimes, demonstrating that the symmetries inherent to MoE architectures are\nfully characterized by permutations acting on both the expert components and the\ngating function. Building on these foundational findings, we propose a matching\nalgorithm that enables alignment between independently trained MoEs, thereby\nfacilitating the discovery of LMC. Finally, we empirically validate the presence of\nLMC using our proposed algorithm across diverse MoE configurations—including\ndense, sparse, and shared-expert variants—under a wide range of model settings\nand datasets of varying scales and modalities. Our results confirm the existence\nof LMC in MoE architectures and offer fundamental insights into the functional\nlandscape and optimization dynamics of deep learning models.",
          "keywords": [
            "linear mode connectivity",
            "mixture-of-experts"
          ],
          "primary_area": "theory",
          "TLDR": "We investigate Linear Mode Connectivity (LMC) in Mixture-of-Experts (MoE) architectures by analyzing their underlying permutation symmetries and proposing expert-matching algorithms that align independently trained MoEs to reveal LMC.",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=RF3miSqdXa",
          "pdf_link": "https://openreview.net/pdf?id=RF3miSqdXa"
        },
        "paper_internal_id": "RF3miSqdXa",
        "category": "oral",
        "embedding_score": 0.7136059999465942,
        "final_score": 0.06318153440952301
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "n4V3MSqK77",
      "title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents",
      "abstract": "LLM-based agent applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs and latency due to extensive planning and reasoning requirements. \nExisting LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agent applications where outputs depend on external data and environmental contexts. \nWe propose **Agentic Plan Caching (APC)**, a novel **test-time memory** that extracts, stores, adapts, and reuses structured plan templates from planning stages of agent applications across semantically similar tasks to reduce the cost and latency of serving. \nUnlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. \nEvaluation across multiple real-world agent applications shows that our system can reduce costs by 50.31\\% and latency by 27.28\\% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.",
      "keywords": "['Caching', 'Memory', 'Serving', 'LLM Agents']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "n4V3MSqK77",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "obXGSmmG70",
          "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06% on APP, while maintaining high performance on complex tasks. This substantial token decrease directly translates to a significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering a practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications.",
          "keywords": [
            "Adaptive Reasoning",
            "Chain-of-Thought",
            "Large Language Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "LLMs using Chain-of-Thought (CoT) for everything is wasteful. We built AdaCoT, a smart system that teaches LLMs when to use CoT based on clear principles, saving compute and improving user experience without sacrificing performance on hard tasks.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=obXGSmmG70",
          "pdf_link": "https://openreview.net/pdf?id=obXGSmmG70"
        },
        "paper_internal_id": "obXGSmmG70",
        "category": "reject",
        "embedding_score": 0.699112057685852,
        "final_score": 0.9573623538017273
      },
      "spotlight": {
        "paper": {
          "id": "Fcs90Rwm8j",
          "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
          "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading,  underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents.",
          "keywords": [
            "Machine Learning",
            "Gaming",
            "Model Compression",
            "LLM Agents"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We present the first systematic study of lossy latency–quality trade-offs in LLM agents, introducing HFTBench and StreetFighter benchmarks, and proposing an adaptive mixed-precision framework for real-world latency-sensitive tasks.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=Fcs90Rwm8j",
          "pdf_link": "https://openreview.net/pdf?id=Fcs90Rwm8j"
        },
        "paper_internal_id": "Fcs90Rwm8j",
        "category": "spotlight",
        "embedding_score": 0.7406929731369019,
        "final_score": 0.917561948299408
      },
      "oral": {
        "paper": {
          "id": "q2VpjD7k1V",
          "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch",
          "abstract": "LLM‑based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications.\nTo assess the quality of the generated websites, we generate test cases targeting each functionality described in the instructions. These test cases are then manually filtered, refined, and organized to ensure accuracy, resulting in a total of 647 test cases. Each test case specifies an operation to be performed on the website and the expected outcome of the operation.\nTo automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute test cases on the generated websites and determine whether the observed responses align with the expected results.\nWe evaluate three high-performance code-agent frameworks—Bolt.diy, OpenHands, and Aider—using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark.\nAdditionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of the training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.\nWe release our data-generation, training, and testing code, along with both the datasets and model weights at https://github.com/mnluzimu/WebGen-Bench.",
          "keywords": [
            "Code Agent",
            "Website Generation"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "",
          "creation_date": "2025-04-29",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track oral",
          "forum_link": "https://openreview.net/forum?id=q2VpjD7k1V",
          "pdf_link": "https://openreview.net/pdf?id=q2VpjD7k1V"
        },
        "paper_internal_id": "q2VpjD7k1V",
        "category": "oral",
        "embedding_score": 0.7571942806243896,
        "final_score": 0.9344986081123352
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Ejcn7IDkzT",
      "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks",
      "abstract": "We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.",
      "keywords": "['robust satisficing', 'regret minimization', 'adversarial attacks']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Ejcn7IDkzT",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "nLWhcCs9Dp",
          "title": "O-MMGP: Optimal Mesh Morphing Gaussian Process Regression for Solving PDEs with non-Parametric Geometric Variations",
          "abstract": "We address the computational challenges of solving parametric PDEs with non parametrized geometric variations and non-reducible problems, such as those involving shocks and discontinuities of variable positions. Traditional dimensionality reduction methods like POD struggle with these scenarios due to slowly decaying Kolmogorov widths. To overcome this, we propose a novel non-linear dimensionality reduction technique to reduce the required modes for representation. The non-linear reduction is obtained through a POD after applying a transformation on the fields, which we call optimal mappings, and is a solution to an optimization problem in infinite dimension. The proposed learning framework combines morphing techniques, non-linear dimensionality reduction, and Gaussian Process Regression (GPR). The problem is reformulated on a reference geometry before applying the dimensionality reduction. Our method learns both the optimal mapping, and the solution fields, using a series of GPR models, enabling efficient and accurate modeling of complex parametric PDEs with geometrical variability. The results obtained concur with current state-of-the-art models. We mainly compare our method with the winning solution of the ML4CFD NeurIPS 2024 competition.",
          "keywords": [
            "Gaussian process",
            "Mesh morphing",
            "Reduced order modeling"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=nLWhcCs9Dp",
          "pdf_link": "https://openreview.net/pdf?id=nLWhcCs9Dp"
        },
        "paper_internal_id": "nLWhcCs9Dp",
        "category": "reject",
        "embedding_score": 0.7039874792098999,
        "final_score": 0.5426084995269775
      },
      "spotlight": {
        "paper": {
          "id": "FEugj28qhC",
          "title": "BayeSQP: Bayesian Optimization through Sequential Quadratic Programming",
          "abstract": "We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.",
          "keywords": [
            "Bayesian optimization",
            "sequential quadratic programming",
            "constrained optimization"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "We introduce BayeSQP, a novel black-box optimization algorithm that combines sequential quadratic programming with Bayesian optimization for high-dimensional constrained problems..",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=FEugj28qhC",
          "pdf_link": "https://openreview.net/pdf?id=FEugj28qhC"
        },
        "paper_internal_id": "FEugj28qhC",
        "category": "spotlight",
        "embedding_score": 0.8094871044158936,
        "final_score": 0.34895971417427063
      },
      "oral": {
        "paper": {
          "id": "gxfusMqPIs",
          "title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
          "abstract": "This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). \nUnder a Mat\\'ern kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\\tilde{O}(\\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\\sqrt{T \\ln^2 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner.",
          "keywords": [
            "Gaussian process bandits",
            "regret analysis",
            "Bayesian optimization"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=gxfusMqPIs",
          "pdf_link": "https://openreview.net/pdf?id=gxfusMqPIs"
        },
        "paper_internal_id": "gxfusMqPIs",
        "category": "oral",
        "embedding_score": 0.8323161602020264,
        "final_score": 0.9862491488456726
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "8C8F4NmHfz",
      "title": "Tail-Optimized Caching for LLM Inference",
      "abstract": "Prompt caching is critical for reducing latency and cost in LLM inference---OpenAI and Anthropic report up to 50–90\\% cost savings through prompt reuse. Despite its widespread success, little is known about what constitutes an optimal prompt caching policy, particularly when optimizing tail latency—a metric of central importance to practitioners. The widely used Least Recently Used (LRU) policy can perform arbitrarily poor on this metric, as it is oblivious to the heterogeneity of conversation lengths. To address this gap, we propose Tail-Optimized LRU, a simple two-line modification that reallocates KV cache capacity to prioritize high-latency conversations by evicting cache entries that are unlikely to affect future turns. Though the implementation is simple, we prove its optimality under a natural stochastic model of conversation dynamics, providing the first theoretical justification for LRU in this setting---a result that may be of independent interest to the caching community. \nExperimentally, on real conversation data WildChat~\\citep{zhao2024wildchat}, Tail-Optimized LRU achieves up to 27.5\\% reduction in P90 tail Time to First Token latency and 23.9\\% in P95 tail latency compared to LRU, along with up to 38.9\\% decrease in SLO violations of 200ms. \nWe believe this provides a practical and theoretically grounded option for practitioners seeking to optimize tail latency in real-world LLM deployments.",
      "keywords": "['prompt caching', 'large language models', 'tail latency', 'KV‑cache eviction', 'Least‑Recently‑Used']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "8C8F4NmHfz",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "YtsX7irxbq",
          "title": "When recalling in-context, Transformers are not SSMs",
          "abstract": "Despite the advantageous subquadratic complexity of modern recurrent deep learning models -- such as state-space models (SSMs) -- recent studies have highlighted their potential shortcomings compared to transformers on reasoning and memorization tasks. In this paper, we dive deeper into one of such benchmarks: associative recall (AR), which has been shown to correlate well with language modeling performance, and inspect in detail the effects of scaling and optimization issues in recently proposed token mixing strategies. We first demonstrate that, unlike standard transformers, the choice of learning rate plays a critical role in the performance of modern recurrent models: an issue that can severely affect reported performance in previous works and suggests further research is needed to stabilize training. Next, we show that recurrent and attention-based models exhibit contrasting benefits when scaling in width as opposed to depth, with attention being notably unable to solve AR when limited to a single layer. We then further inspect 1-layer transformers, revealing that despite their poor performance, their training dynamics surprisingly resemble the formation of induction heads, a phenomenon previously observed only in their 2-layer counterparts. Finally, through architectural ablations, we study how components affects Transformer and Mamba’s performance and optimization stability.",
          "keywords": [
            "SSMs",
            "Attention",
            "In-Context Learning",
            "Language Modeling",
            "Mamba"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=YtsX7irxbq",
          "pdf_link": "https://openreview.net/pdf?id=YtsX7irxbq"
        },
        "paper_internal_id": "YtsX7irxbq",
        "category": "reject",
        "embedding_score": 0.6720535755157471,
        "final_score": 0.8362787961959839
      },
      "spotlight": {
        "paper": {
          "id": "cECo8tetzF",
          "title": "Restoring Pruned Large Language Models via Lost Component Compensation",
          "abstract": "Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models.",
          "keywords": [
            "large language models",
            "pruning",
            "performance restoration"
          ],
          "primary_area": "deep_learning",
          "TLDR": "A targeted restoration method to recover pruned models while preserving their low cost and high efficiency",
          "creation_date": "2025-05-06",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=cECo8tetzF",
          "pdf_link": "https://openreview.net/pdf?id=cECo8tetzF"
        },
        "paper_internal_id": "cECo8tetzF",
        "category": "spotlight",
        "embedding_score": 0.6950945258140564,
        "final_score": 0.9472206830978394
      },
      "oral": {
        "paper": {
          "id": "JFygzwx8SJ",
          "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
          "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\\times$ and FlashAttention decoding latency by approximately $2\\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios.",
          "keywords": [
            "Large Language Models",
            "Efficient Inference",
            "Long-Context Processing",
            "KV Cache Compression"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose a novel query-agnostic KV cache eviction method for multi-query scenario.",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=JFygzwx8SJ",
          "pdf_link": "https://openreview.net/pdf?id=JFygzwx8SJ"
        },
        "paper_internal_id": "JFygzwx8SJ",
        "category": "oral",
        "embedding_score": 0.756659209728241,
        "final_score": 0.8674472570419312
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "k38Th3x4d9",
      "title": "Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery",
      "abstract": "Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies.",
      "keywords": "['root cause analysis', 'Granger causality', 'multivariate time series']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "k38Th3x4d9",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "v5BouOktUP",
          "title": "Multivariate Time-series Forecasting with SPACE: Series Prediction Augmented by Causality Estimation",
          "abstract": "The analysis of multivariate time series (MTS) presents a complex yet crucial task with substantial applications in areas such as weather forecasting, policy formulation, and stock market prediction. It is important to highlight three key characteristics of MTS that contribute to the challenging and multifaceted nature of their analysis: (i) their interrelationships are represented through causal relationships rather than mere similarities; (ii) they convey information across multiple independent factors; and (iii) their dynamics often arise from inherent temporal dependencies. While conventional time series analysis frameworks often fail to capture one or more of these aspects, resulting in incomplete or even misleading conclusions, we propose an end-to-end trainable $\\textbf{S}$eries $\\textbf{P}$rediction model $\\textbf{A}$ugmented by $\\textbf{C}$ausality $\\textbf{E}$stimation (SPACE) to address these limitations. This model effectively incorporates temporal dependencies and causal relationships, featuring a temporal embedding and a transfer entropy-based Cross-TE module designed to enhance predictions through causality-augmented mechanisms. Experiments demonstrate that SPACE achieves state-of-the-art results on challenging real-world time series prediction tasks, showing its effectiveness and versatility.",
          "keywords": [
            "Time Series Forecasting",
            "Causal Learning",
            "Transfer Entropy",
            "Graph Based Learning"
          ],
          "primary_area": "causal reasoning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=v5BouOktUP",
          "pdf_link": "https://openreview.net/pdf?id=v5BouOktUP"
        },
        "paper_internal_id": "v5BouOktUP",
        "category": "reject",
        "embedding_score": 0.7749016880989075,
        "final_score": 0.9984738230705261
      },
      "poster": {
        "paper": {
          "id": "m08aK3xxdJ",
          "title": "CATCH: Channel-Aware Multivariate Time Series Anomaly Detection via Frequency Patching",
          "abstract": "Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning normal patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising results, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 10 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available at https://github.com/decisionintelligence/CATCH.",
          "keywords": [
            "Multivariate Time Series",
            "Anomaly Detection"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-08",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=m08aK3xxdJ",
          "pdf_link": "https://openreview.net/pdf?id=m08aK3xxdJ"
        },
        "paper_internal_id": "m08aK3xxdJ",
        "category": "poster",
        "embedding_score": 0.7248094081878662,
        "final_score": 0.9986067414283752
      },
      "spotlight": {
        "paper": {
          "id": "3fl1SENSYO",
          "title": "DiffPuter: Empowering Diffusion Models for Missing Data Imputation",
          "abstract": "Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10\\% in MAE and 5.64\\% in RMSE compared to the most competitive existing method.",
          "keywords": [
            "Diffusion models",
            "missing data imputation"
          ],
          "primary_area": "generative models",
          "TLDR": "This paper combines EM algorithm and a Diffusion model for missing data imputation",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-15",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=3fl1SENSYO",
          "pdf_link": "https://openreview.net/pdf?id=3fl1SENSYO"
        },
        "paper_internal_id": "3fl1SENSYO",
        "category": "spotlight",
        "embedding_score": 0.6376352310180664,
        "final_score": 0.9873099327087402
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "N8Oj1XhtYZ",
      "title": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers",
      "abstract": "We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4)  Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.",
      "keywords": "['Efficient AI', 'Diffusion Models', 'Text to Image generation']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "N8Oj1XhtYZ",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "Yd5MHVIKLk",
          "title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion",
          "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control.\nMuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users.",
          "keywords": [
            "Diffusion models",
            "Controllable generation",
            "multi-modal agent"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=Yd5MHVIKLk",
          "pdf_link": "https://openreview.net/pdf?id=Yd5MHVIKLk"
        },
        "paper_internal_id": "Yd5MHVIKLk",
        "category": "reject",
        "embedding_score": 0.7073664665222168,
        "final_score": 0.7978372573852539
      },
      "poster": {
        "paper": {
          "id": "mr2icR6dpD",
          "title": "TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models",
          "abstract": "How humans can effectively and efficiently acquire images has always been a perennial question. A classic solution is *text-to-image retrieval* from an existing database; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in *text-to-image generation* have made it possible to produce attractive and counterfactual visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval, proposing a *unified* framework for both tasks with one single Large Multimodal Model (LMM). Specifically, we first explore the intrinsic discriminative abilities of LMMs and introduce an efficient generative retrieval method for text-to-image retrieval in a training-free manner. Subsequently, we unify generation and retrieval autoregressively and propose an autonomous decision mechanism to choose the best-matched one between generated and retrieved images as the response to the text prompt. To standardize the evaluation of unified text-to-image generation and retrieval, we construct TIGeR-Bench, a benchmark spanning both creative and knowledge-intensive domains. Extensive experiments on TIGeR-Bench and two retrieval benchmarks, *i.e.*, Flickr30K and MS-COCO, demonstrate the superiority of our proposed framework.",
          "keywords": [
            "Multimodal Large Language Models",
            "Text-to-Image Generation",
            "Cross-Modal Retrieval"
          ],
          "primary_area": "generative models",
          "TLDR": "This paper explores the intrinsic discriminative ability of multimodal foundation models, and proposes a unified framework combining text-to-image generation and retrieval.",
          "creation_date": "2024-09-16",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=mr2icR6dpD",
          "pdf_link": "https://openreview.net/pdf?id=mr2icR6dpD"
        },
        "paper_internal_id": "mr2icR6dpD",
        "category": "poster",
        "embedding_score": 0.72119140625,
        "final_score": 0.9599166512489319
      },
      "spotlight": {
        "paper": {
          "id": "cD1kl2QKv1",
          "title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt",
          "abstract": "Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined $\\textit{context consistency}$, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent $\\textit{context consistency}$, we propose a novel $\\textit{training-free}$ method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" ($\\textit{1Prompt1Story}$). Our approach $\\textit{1Prompt1Story}$ concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: $\\textit{Singular-Value\nReweighting}$ and $\\textit{Identity-Preserving Cross-Attention}$, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.",
          "keywords": [
            "diffusion model; consistent T2I image generation; storytelling"
          ],
          "primary_area": "generative models",
          "TLDR": "We propose a training-free approach named 1Prompt1Story for consistent text-to-image generations with a single concatenated prompt. Our method is built on the inherent context consistency propoerty of language models.",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=cD1kl2QKv1",
          "pdf_link": "https://openreview.net/pdf?id=cD1kl2QKv1"
        },
        "paper_internal_id": "cD1kl2QKv1",
        "category": "spotlight",
        "embedding_score": 0.7048956155776978,
        "final_score": 0.8258877396583557
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Y6aHdDNQYD",
      "title": "MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection",
      "abstract": "LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging $\\textit{cross-corruption}$ scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS.",
      "keywords": "['Test-Time Adaptation', '3D Object Detection']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Y6aHdDNQYD",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "HCJ7B6dhYK",
          "title": "Radon Implicit Field Transform (RIFT): Learning Scenes from Radar Signals",
          "abstract": "Data acquisition in array signal processing (ASP) is costly because achieving high angular and range resolutions necessitates large antenna apertures and wide frequency bandwidths, respectively. The data requirements for ASP problems grow multiplicatively with the number of viewpoints and frequencies, significantly increasing the burden of data collection, even for simulation. Implicit Neural Representations (INRs) — neural network-based models of 3D objects and scenes — offer compact and continuous representations with minimal radar data. They can interpolate to unseen viewpoints and potentially address the sampling cost in ASP problems. In this work, we select Synthetic Aperture Radar (SAR) as a case from ASP and propose the \\textit{\\textbf{R}adon \\textbf{I}mplicit \\textbf{F}ield \\textbf{T}ransform} (RIFT). RIFT consists of two components: a classical forward model for radar (Generalized Radon Transform, GRT), and an INR based scene representation learned from radar signals. This method can be extended to other ASP problems by replacing the GRT with appropriate algorithms corresponding to different data modalities. In our experiments, we first synthesize radar data using the GRT. We then train the INR model on this synthetic data by minimizing the reconstruction error of the radar signal. After training, we render the scene using the trained INR and evaluate our scene representation against the ground truth scene. Due to the lack of existing benchmarks, we introduce two main new error metrics: \\textit{\\textbf{p}hase-\\textbf{R}oot \\textbf{M}ean \\textbf{S}quare \\textbf{E}rror} (p-RMSE) for radar signal interpolation, and \\textit{\\textbf{m}agnitude-\\textbf{S}tructural \\textbf{S}imilarity \\textbf{I}ndex \\textbf{M}easure} (m-SSIM) for scene reconstruction. These metrics adapt traditional error measures to account for the complex nature of radar signals. Compared to traditional scene models in radar signal processing, with only 10\\% data footprint, our RIFT model achieves up to 188\\% improvement in scene reconstruction. Using the same amount of data, RIFT is up to $3\\times$ better at reconstruction and shows a 10\\% improvement generalizing to unseen viewpoints.",
          "keywords": [
            "AI for Science",
            "Representation Learning",
            "Scene Rendering",
            "Implicit Neural Representation",
            "3D Reconstruction",
            "Inverse Problems"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We combine implicit neural representation and signal processing algorithms to create a model which can reconstruct scenes by learning radar signals.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=HCJ7B6dhYK",
          "pdf_link": "https://openreview.net/pdf?id=HCJ7B6dhYK"
        },
        "paper_internal_id": "HCJ7B6dhYK",
        "category": "reject",
        "embedding_score": 0.7149161696434021,
        "final_score": 0.14869976043701172
      },
      "poster": {
        "paper": {
          "id": "9xHlhKLu1h",
          "title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection",
          "abstract": "While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from environmen-\ntal and intrinsic disturbances. Poor lighting or adverse weather conditions de-\ngrade camera performance, while radar suffers from noise and positional ambigu-\nity. Achieving robust radar-camera 3D object detection requires consistent perfor-\nmance across varying conditions, a topic that has not yet been fully explored. In\nthis work, we first conduct a systematic analysis of robustness in radar-camera de-\ntection on five kinds of noises and propose RobuRCDet, a robust object detection\nmodel in bird’s eye view (BEV). Specifically, we design a 3D Gaussian Expan-\nsion (3DGE) module to mitigate inaccuracies in radar points, including position,\nRadar Cross-Section (RCS), and velocity. The 3DGE uses RCS and velocity priors\nto generate a deformable kernel map and variance for kernel size adjustment and\nvalue distribution. Additionally, we introduce a weather-adaptive fusion module,\nwhich adaptively fuses radar and camera features based on camera signal confi-\ndence. Extensive experiments on the popular benchmark, nuScenes, show that\nour RobuRCDet achieves competitive results in regular and noisy conditions. The\nsource codes and trained models will be made available.",
          "keywords": [
            "3D Vision， Radar Camera 3D Object Detection"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-27",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=9xHlhKLu1h",
          "pdf_link": "https://openreview.net/pdf?id=9xHlhKLu1h"
        },
        "paper_internal_id": "9xHlhKLu1h",
        "category": "poster",
        "embedding_score": 0.7468962669372559,
        "final_score": 0.5646325349807739
      },
      "spotlight": {
        "paper": {
          "id": "HqLHY4TzGj",
          "title": "Union-over-Intersections: Object Detection beyond Winner-Takes-All",
          "abstract": "This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.",
          "keywords": [
            "localization based feature representation",
            "intersection over union",
            "object detection."
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=HqLHY4TzGj",
          "pdf_link": "https://openreview.net/pdf?id=HqLHY4TzGj"
        },
        "paper_internal_id": "HqLHY4TzGj",
        "category": "spotlight",
        "embedding_score": 0.719308614730835,
        "final_score": 0.25958251953125
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "zBbZ2vdLzH",
      "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
      "abstract": "When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to **j**ointly **d**enoise the features and **r**ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.",
      "keywords": "['GNNs', 'Rewiring', 'Denoising', 'Spectral Resonance', 'cSBM']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "zBbZ2vdLzH",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "Vszt1FDElj",
          "title": "Coarsening to Conceal: Enabling Privacy-Preserving Federated Learning for Graph Data",
          "abstract": "With the escalating demand for privacy-preserving machine learning, federated learning (FL) stands out by enabling collaboration among decentralized entities. Utilizing graph representations of data enhances learning for graph-level tasks, crucial for FL with data distributed across local repositories. Despite its benefits, stringent privacy regulations often compromise FL's performance. Previous methods aimed at ensuring privacy introduce performance degradation and computational overhead. In response to these challenges, we propose using graph coarsening—a simple yet effective method—to enhance the security and privacy of FL on graph data. Our approach posits that graph coarsening alone can suffice for privacy guarantees, as model parameters obtained from training on the coarsened graph effectively conceal sensitive information susceptible to privacy attacks. Through comprehensive application and analysis, we demonstrate the efficacy of graph coarsening within an FL setup, taking both the graph matrix and node features as input, and jointly learning the coarsened graph matrix and feature matrix while ensuring desired properties. The resultant coarsened graph representations are then utilized to train model parameters, subsequently communicated within an FL framework for downstream tasks such as classification. Extensive experimentation across various datasets confirms that graph coarsening ensures privacy while enhancing performance with minimal trade-offs compared to traditional differential privacy (DP) methods without adding extra complexity overhead.",
          "keywords": [
            "Federated Learning",
            "Privacy-Preserving Machine Learning",
            "Graph Neural Networks",
            "Graph Coarsening",
            "Data Privacy and Security"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We propose a novel approach to enhancing privacy in federated learning by leveraging graph coarsening, demonstrating its effectiveness in concealing sensitive information while maintaining robust collaborative learning across decentralized entities.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=Vszt1FDElj",
          "pdf_link": "https://openreview.net/pdf?id=Vszt1FDElj"
        },
        "paper_internal_id": "Vszt1FDElj",
        "category": "reject",
        "embedding_score": 0.7204545736312866,
        "final_score": 0.8353144526481628
      },
      "poster": {
        "paper": {
          "id": "h51mpl8Tyx",
          "title": "BANGS: Game-theoretic Node Selection for Graph Self-Training",
          "abstract": "Graph self-training is a semi-supervised learning method that iteratively selects a set of unlabeled data to retrain the underlying graph neural network (GNN) model and improve its prediction performance. While selecting highly confident nodes has proven effective for self-training, this pseudo-labeling strategy ignores the combinatorial dependencies between nodes and suffers from a local view of the distribution.\nTo overcome these issues, we propose BANGS, a novel framework that unifies the labeling strategy with conditional mutual information as the objective of node selection. Our approach---grounded in game theory---selects nodes in a combinatorial fashion and provides theoretical guarantees for robustness under noisy objective. More specifically, unlike traditional methods that rank and select nodes independently, BANGS considers nodes as a collective set in the self-training process. Our method demonstrates superior performance and robustness across various datasets, base models, and hyperparameter settings, outperforming existing techniques. The codebase is available on https://github.com/fangxin-wang/BANGS.",
          "keywords": [
            "Graph Semi-supervised Learning",
            "Graph Self-training",
            "Game Theory Application"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We propose a game-theoretic node selection framework for graph self-training.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=h51mpl8Tyx",
          "pdf_link": "https://openreview.net/pdf?id=h51mpl8Tyx"
        },
        "paper_internal_id": "h51mpl8Tyx",
        "category": "poster",
        "embedding_score": 0.7783781290054321,
        "final_score": 0.7743505239486694
      },
      "spotlight": {
        "paper": {
          "id": "aX7X9z3vQS",
          "title": "Recovering Manifold Structure Using Ollivier Ricci Curvature",
          "abstract": "We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.",
          "keywords": [
            "Manifold Learning",
            "Persistent Homology",
            "Ollivier-Ricci Curvature",
            "Pruning",
            "Nearest-Neighbor Graphs"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We present and test a theoretically grounded method that uses discrete graph curvature to prune nearest-neighbor graphs.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-08",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=aX7X9z3vQS",
          "pdf_link": "https://openreview.net/pdf?id=aX7X9z3vQS"
        },
        "paper_internal_id": "aX7X9z3vQS",
        "category": "spotlight",
        "embedding_score": 0.7191430926322937,
        "final_score": 0.6519126892089844
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "6EUtjXAvmj",
      "title": "Variational Diffusion Posterior Sampling with Midpoint Guidance",
      "abstract": "Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis.",
      "keywords": "['Diffusion models', 'Inverse problems', 'posterior sampling']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "6EUtjXAvmj",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "Z9Odi09Rv9",
          "title": "Fast and Noise-Robust Diffusion Solvers for Inverse Problems: A Frequentist Approach",
          "abstract": "Diffusion models have been firmly established as principled zero-shot solvers for linear and nonlinear inverse problems, owing to their powerful image prior and ease of formulation as Bayesian posterior samplers. However, many existing solvers struggle in the noisy measurement regime, either overfitting or underfitting to the measurement constraint, resulting in poor sample quality and inconsistent performance across noise levels. Moreover, existing solvers rely on approximating $x_0$ via Tweedie's formula, where an intractable \\textit{conditional} score is replaced by an \\textit{unconditional} score network, introducing a fundamental source of error in the resulting solution. In this work, we propose a novel frequentist's approach to diffusion-based inverse solvers, where each diffusion step can be seen as the maximum likelihood solution to a simple single-parameter conditional likelihood model, derived by an adjusted application of Tweedie's formula to the forward measurement model. We demonstrate that this perspective is not only scalable and fast, but also allows for a noise-aware maximization scheme with a likelihood-based stopping criterion that promotes the proper noise-adapted fit given knowledge of the measurement noise $\\sigma_\\mathbf{y}$. Finally, we demonstrate comparable or improved performance against a wide selection of contemporary inverse solvers across multiple datasets, tasks, and noise levels.",
          "keywords": [
            "diffusion models",
            "inverse problems",
            "maximum likelihood"
          ],
          "primary_area": "generative models",
          "TLDR": "We propose an improved solver for inverse problems by adjusting a known intractable term in Tweedie's formula and applying a noise-aware MLE framework. The resulting algorithm is fast, avoiding costly backpropagations through the score network.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=Z9Odi09Rv9",
          "pdf_link": "https://openreview.net/pdf?id=Z9Odi09Rv9"
        },
        "paper_internal_id": "Z9Odi09Rv9",
        "category": "reject",
        "embedding_score": 0.8083146214485168,
        "final_score": 0.8338679075241089
      },
      "poster": {
        "paper": {
          "id": "kRBQwlkFSP",
          "title": "Diffusion State-Guided Projected Gradient for Inverse Problems",
          "abstract": "Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at https://github.com/Anima-Lab/DiffStateGrad.",
          "keywords": [
            "Diffusion models",
            "Inverse problems",
            "Robustness",
            "Subspace",
            "Projection",
            "Box inpainting",
            "Phase retrieval"
          ],
          "primary_area": "generative models",
          "TLDR": "We can improve the performance and robustness of diffusion-based models in solving inverse problems by adding a Diffusion State-Guided Projected Gradient step.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-01",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=kRBQwlkFSP",
          "pdf_link": "https://openreview.net/pdf?id=kRBQwlkFSP"
        },
        "paper_internal_id": "kRBQwlkFSP",
        "category": "poster",
        "embedding_score": 0.8157957792282104,
        "final_score": 0.8737558126449585
      },
      "spotlight": {
        "paper": {
          "id": "TtUh0TOlGX",
          "title": "Regularization by Texts for Latent Diffusion Inverse Solvers",
          "abstract": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.",
          "keywords": [
            "Inverse problem",
            "Text regularization",
            "Diffusion model"
          ],
          "primary_area": "generative models",
          "TLDR": "We propose text regularization for inverse problem solving.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=TtUh0TOlGX",
          "pdf_link": "https://openreview.net/pdf?id=TtUh0TOlGX"
        },
        "paper_internal_id": "TtUh0TOlGX",
        "category": "spotlight",
        "embedding_score": 0.7468389272689819,
        "final_score": 0.7627066969871521
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "vRvVVb0NAz",
      "title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
      "abstract": "Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).",
      "keywords": "['Task arithmetic', 'generalization', 'nonlinear Transformers', 'deep learning theory', 'machine unlearning']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "vRvVVb0NAz",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "DzKdjWe59v",
          "title": "Hint Marginalization for Improved Reasoning in Large Language Models",
          "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode the most likely answer. Empirical evaluation on several benchmark datasets for arithmetic reasoning demonstrates the superiority of the proposed approach.",
          "keywords": [
            "reasoning",
            "large language models"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=DzKdjWe59v",
          "pdf_link": "https://openreview.net/pdf?id=DzKdjWe59v"
        },
        "paper_internal_id": "DzKdjWe59v",
        "category": "reject",
        "embedding_score": 0.698380708694458,
        "final_score": 0.8804348707199097
      },
      "poster": {
        "paper": {
          "id": "TDyE2iuvyc",
          "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
          "abstract": "Task arithmetic has emerged as a promising approach for editing models by representing task-specific knowledge as composable task vectors. However, existing methods rely on network linearization to derive task vectors, leading to computational bottlenecks during training and inference. Moreover, linearization alone does not ensure weight disentanglement, the key property that enables conflict-free composition of task vectors. To address this, we propose TaLoS which allows to build sparse task vectors with minimal interference without requiring explicit linearization and sharing information across tasks. We find that pre-trained models contain a subset of parameters with consistently low gradient sensitivity across tasks, and that sparsely updating only these parameters allows for promoting weight disentanglement during fine-tuning. Our experiments prove that TaLoS improves training and inference efficiency while outperforming current methods in task addition and negation. By enabling modular parameter editing, our approach fosters practical deployment of adaptable foundation models in real-world applications.",
          "keywords": [
            "task arithmetic",
            "parameter-efficient fine-tuning"
          ],
          "primary_area": "transfer learning, meta learning, and lifelong learning",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-03",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=TDyE2iuvyc",
          "pdf_link": "https://openreview.net/pdf?id=TDyE2iuvyc"
        },
        "paper_internal_id": "TDyE2iuvyc",
        "category": "poster",
        "embedding_score": 0.8211691975593567,
        "final_score": 0.9955116510391235
      },
      "spotlight": {
        "paper": {
          "id": "OZVTqoli2N",
          "title": "A Second-Order Perspective on Model Compositionality and Incremental Learning",
          "abstract": "The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts concentrating mainly on linearized networks. We conduct a theoretical study that attempts to demystify compositionality in standard non-linear networks through the second-order Taylor approximation of the loss function. The proposed formulation highlights the importance of staying within the pre-training basin to achieve composable modules. Moreover, it provides the basis for two dual incremental training algorithms: the one from the perspective of multiple models trained individually, while the other aims to optimize the composed model as a whole. We probe their application in incremental classification tasks and highlight some valuable skills. In fact, the pool of incrementally learned modules not only supports the creation of an effective multi-task model but also enables unlearning and specialization in certain tasks. Code available at <https://github.com/aimagelab/mammoth>",
          "keywords": [
            "Continual Learning",
            "Model Compositionality",
            "Ensemble Learning",
            "Task Arithmetic"
          ],
          "primary_area": "transfer learning, meta learning, and lifelong learning",
          "TLDR": "We explore compositionality in fine-tuning non-linear deep models, revealing that staying within the pre-training basin is key to creating effective incremental learners.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=OZVTqoli2N",
          "pdf_link": "https://openreview.net/pdf?id=OZVTqoli2N"
        },
        "paper_internal_id": "OZVTqoli2N",
        "category": "spotlight",
        "embedding_score": 0.7042179107666016,
        "final_score": 0.6895578503608704
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "je3GZissZc",
      "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
      "abstract": "Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks.",
      "keywords": "['In-context Imitation Learning', 'Robotic Manipulation', 'Graph Neural Networks', 'Diffusion Models']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "je3GZissZc",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "COdUNtjMEp",
          "title": "On the Training Convergence of Transformers for In-Context Classification",
          "abstract": "While transformers have demonstrated impressive capacities for in-context learning (ICL) in practice, theoretical understanding of the underlying mechanism enabling transformers to perform ICL is still in its infant stage. This work aims to theoretically study the training dynamics of transformers for in-context classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain assumptions, a single-layer transformer trained via gradient descent converges to a globally optimal model at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL inference error of the trained transformer. We show that when the lengths of training and testing prompts are sufficiently large, the prediction of the trained transformer approaches the Bayes-optimal classifier. Experimental results corroborate the theoretical findings.",
          "keywords": [
            "In-context learning",
            "Transformer"
          ],
          "primary_area": "learning theory",
          "TLDR": "Transformers trained via gradient descent can provably perform in-context classification of Gaussian mixtures.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=COdUNtjMEp",
          "pdf_link": "https://openreview.net/pdf?id=COdUNtjMEp"
        },
        "paper_internal_id": "COdUNtjMEp",
        "category": "reject",
        "embedding_score": 0.7347037196159363,
        "final_score": 0.9962385892868042
      },
      "poster": {
        "paper": {
          "id": "xing7dDGh3",
          "title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
          "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.",
          "keywords": [
            "large language models",
            "in-context learning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We discover that large language models can effectively process and in-context learn from continuous representations from various domains, often outperforming regular ICL and domain-specific models across diverse tasks and modalities.",
          "creation_date": "2024-09-17",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-20",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=xing7dDGh3",
          "pdf_link": "https://openreview.net/pdf?id=xing7dDGh3"
        },
        "paper_internal_id": "xing7dDGh3",
        "category": "poster",
        "embedding_score": 0.7274579405784607,
        "final_score": 0.8811731338500977
      },
      "spotlight": {
        "paper": {
          "id": "XgH1wfHSX8",
          "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning",
          "abstract": "In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.",
          "keywords": [
            "In-Context Learning",
            "Circuit Competition",
            "Markov Chains",
            "Training Dynamics",
            "Generalization"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "In-context learning consists of phases of multiple algorithmic solutions, many phenomena are explained by this decomposition.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=XgH1wfHSX8",
          "pdf_link": "https://openreview.net/pdf?id=XgH1wfHSX8"
        },
        "paper_internal_id": "XgH1wfHSX8",
        "category": "spotlight",
        "embedding_score": 0.7826623320579529,
        "final_score": 0.24407978355884552
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "2efNHgYRvM",
      "title": "On the Identification of Temporal Causal Representation with Instantaneous Dependence",
      "abstract": "Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.",
      "keywords": "['Causal Representation Learning', 'Instantaneous Dependency', 'Identification']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "2efNHgYRvM",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "Rkpdfia4Sz",
          "title": "Learning Discrete Latent Models from Discrete Observations",
          "abstract": "A central challenge in machine learning is discovering meaningful representations of high-dimensional data, commonly referred to as representation learning. However, many existing methods lack a theoretical foundation, leading to unreliable representations and limited inferential capabilities. In approaches where certain uniqueness of representation is guaranteed, such as nonlinear ICA, variables are typically assumed to be continuous. While recent work has extended identifiability to binarized observed variables, no principled method has been developed for scenarios involving discrete latent variables. In this paper, we show how multi-domain information can be leveraged to achieve identifiability when both latent and observed variables are discrete. We propose general identification conditions that do not depend on specific data distributional assumptions or parametric model forms. The effectiveness of our approach is validated through experiments on both simulated and real-world datasets.",
          "keywords": [
            "Latent Variable Identification",
            "Nonlinear Independent Component Analysis (ICA)"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=Rkpdfia4Sz",
          "pdf_link": "https://openreview.net/pdf?id=Rkpdfia4Sz"
        },
        "paper_internal_id": "Rkpdfia4Sz",
        "category": "reject",
        "embedding_score": 0.7806687951087952,
        "final_score": 0.9268910884857178
      },
      "poster": {
        "paper": {
          "id": "lk2Qk5xjeu",
          "title": "Unifying Causal Representation Learning with the Invariance Principle",
          "abstract": "Causal representation learning (CRL) aims at recovering latent causal variables from high-dimensional observations to solve causal downstream tasks, such as predicting the effect of new interventions or more robust classification. \n  A plethora of methods have been developed, each tackling carefully crafted problem settings that lead to different types of identifiability. \n  These different settings are widely assumed to be important because they are often linked to different rungs of Pearl's causal hierarchy, even though this correspondence is not always exact.\n    This work shows that instead of strictly conforming to this hierarchical mapping, *many causal representation learning approaches methodologically align their representations with inherent data symmetries.*\n  Identification of causal variables is guided by invariance principles that are not necessarily causal. \n  This result allows us to unify many existing approaches in a single method that can mix and match different assumptions, including non-causal ones, based on the invariance relevant to the problem at hand. \n  It also significantly benefits applicability, which we demonstrate by improving treatment effect estimation on real-world high-dimensional ecological data. Overall, this paper clarifies the role of causal assumptions in the discovery of causal variables and shifts the focus to preserving data symmetries.",
          "keywords": [
            "Causal representation learning",
            "Identifiability",
            "Invariance"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=lk2Qk5xjeu",
          "pdf_link": "https://openreview.net/pdf?id=lk2Qk5xjeu"
        },
        "paper_internal_id": "lk2Qk5xjeu",
        "category": "poster",
        "embedding_score": 0.7977816462516785,
        "final_score": 0.9902021288871765
      },
      "spotlight": {
        "paper": {
          "id": "k03mB41vyM",
          "title": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning",
          "abstract": "Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently.\nWe observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on \nexchangeable non--i.i.d. (independent and identically distributed) data.\nTo formalize this connection, \nwe propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning.\nWith the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data.\nWe term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results.",
          "keywords": [
            "causality",
            "ICA",
            "identifiability",
            "causal representation learning"
          ],
          "primary_area": "causal reasoning",
          "TLDR": "A unfiying frameworkt for identifiable causal structure and representation learning method under the lens of exchangeability",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=k03mB41vyM",
          "pdf_link": "https://openreview.net/pdf?id=k03mB41vyM"
        },
        "paper_internal_id": "k03mB41vyM",
        "category": "spotlight",
        "embedding_score": 0.7993590831756592,
        "final_score": 0.9563803672790527
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "EzjsoomYEb",
      "title": "Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity",
      "abstract": "Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information.",
      "keywords": "['Topological Deep Learning', 'Message Passing', 'Higher Order Message Passing', 'Expressivity', 'Graph Neural Networks', 'GNNs', 'Topology', 'Homology', 'Symmetry']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "EzjsoomYEb",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "2MqyCIxLSi",
          "title": "TopoTune: A Framework for Generalized Combinatorial Complex Neural Networks",
          "abstract": "Graph Neural Networks (GNNs) excel in learning from relational datasets, processing node and edge features in a way that preserves the symmetries of the graph domain. However, many complex systems---such as biological or social networks---involve multiway complex interactions that are more naturally represented by higher-order topological domains. The emerging field of Topological Deep Learning (TDL) aims to accommodate and leverage these higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly general TDL models, have been shown to be more expressive and better performing than GNNs. However, differently from the graph deep learning ecosystem, TDL lacks a principled and standardized framework for easily defining new architectures, restricting its accessibility and applicability. To address this issue, we introduce Generalized CCNNs (GCCNs), a novel simple yet powerful family of TDL models that can be used to systematically transform any (graph) neural network into its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show that these architectures consistently match or outperform CCNNs, often with less model complexity. In an effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight software for defining, building, and training GCCNs with unprecedented flexibility and ease.",
          "keywords": [
            "Topological Deep Learning",
            "Graph Neural Network",
            "Graph Expansion",
            "Combinatorial Complex",
            "Cellular Complex"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "TopoTune generalizes any architecture (Graph Neural Network, Transformer, etc.) into a Topological Neural Network that can process higher order structures on relational data.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=2MqyCIxLSi",
          "pdf_link": "https://openreview.net/pdf?id=2MqyCIxLSi"
        },
        "paper_internal_id": "2MqyCIxLSi",
        "category": "reject",
        "embedding_score": 0.8347077369689941,
        "final_score": 0.9427610635757446
      },
      "poster": {
        "paper": {
          "id": "QC2qE1tcmd",
          "title": "Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing",
          "abstract": "Topological deep learning (TDL) has emerged as a powerful tool for modeling higher-order interactions in relational data. However, phenomena such as oversquashing in topological message-passing remain understudied and lack theoretical analysis. We propose a unifying axiomatic framework that bridges graph and topological message-passing by viewing simplicial and cellular complexes and their message-passing schemes through the lens of relational structures. This approach extends graph-theoretic results and algorithms to higher-order structures, facilitating the analysis and mitigation of oversquashing in topological message-passing networks. Through theoretical analysis and empirical studies on simplicial networks, we demonstrate the potential of this framework to advance TDL.",
          "keywords": [
            "topological deep learning",
            "oversquashing",
            "rewiring",
            "relational graph neural networks",
            "simplicial complexes",
            "relational structures"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We propose a framework that unifies graph and higher-order message passing schemes using relational structures to analyze oversquashing in simplicial message-passing, and propose a higher-order generalization of rewiring.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-26",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=QC2qE1tcmd",
          "pdf_link": "https://openreview.net/pdf?id=QC2qE1tcmd"
        },
        "paper_internal_id": "QC2qE1tcmd",
        "category": "poster",
        "embedding_score": 0.8783160448074341,
        "final_score": 0.9713259339332581
      },
      "spotlight": {
        "paper": {
          "id": "L14sqcrUC3",
          "title": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks",
          "abstract": "Advances in machine learning research drive progress in real-world applications. \nTo ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep learning benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.\nFirst, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets often lack timestamp metadata to enable such evaluation.\nSecond, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.\nIn this work, we aim to understand how recent research advances in tabular deep learning transfer to these underrepresented conditions.\nTo this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. \nWe reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on both time-based data splits and richer feature sets leads to different methods ranking, compared to evaluation on random splits and smaller number of features, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.",
          "keywords": [
            "Tabular Data",
            "Benchmarks",
            "Reality Check",
            "Tabular Deep Learning",
            "Applications"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "We introduce TabReD, a collection of industry-grade tabular datasets, filling the gaps in academic benchmarks. Our evaluation reveals performance differences for various models and techniques in a new setting.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=L14sqcrUC3",
          "pdf_link": "https://openreview.net/pdf?id=L14sqcrUC3"
        },
        "paper_internal_id": "L14sqcrUC3",
        "category": "spotlight",
        "embedding_score": 0.6898399591445923,
        "final_score": 0.8075514435768127
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "OwpLQrpdwE",
      "title": "Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels",
      "abstract": "We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (or close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions.  We deduce a theoretical error bound for the proposed solver that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and travelling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics.",
      "keywords": "['Dynamics on manifolds', 'Operator-valued kernel', 'Geometry-preserving time integration', 'Ordinary differential equations']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "OwpLQrpdwE",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "4KKqHIb4iG",
          "title": "Backpropagation-free training of neural PDE solvers for time-dependent problems",
          "abstract": "Approximating solutions to time-dependent Partial Differential Equations (PDEs) is one of the most important problems in computational science. Neural PDE solvers have shown promise recently because they are mesh-free and easy to implement. However, backpropagation-based training often leads to poor approximation accuracy and long training time. In particular, capturing high-frequency temporal dynamics and solving over long time spans pose significant challenges. To address these, we present an approach to training neural PDE solvers without backpropagation by integrating two key ideas: separation of space and time variables and random sampling of weights and biases of the hidden layers. We reformulate the PDE as an Ordinary Differential Equation (ODE) using a neural network ansatz, construct neural basis functions only in the spatial domain, and solve the ODE leveraging classical ODE solvers from scientific computing. We demonstrate that our backpropagation-free algorithm outperforms the iterative, gradient-based optimization of physics-informed neural networks with respect to training time and accuracy, often by 1 to 5 orders of magnitude using different complicated PDEs characterized by high-frequency temporal dynamics, long time span, complex spatial domain, non-linearities, shocks, and high dimensionality.",
          "keywords": [
            "neural PDE solvers",
            "time-dependent partial differential equations",
            "random feature networks",
            "backpropagation-free training"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "We propose a backpropagation-free algorithm to train neural PDE solvers for time-dependent problems.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=4KKqHIb4iG",
          "pdf_link": "https://openreview.net/pdf?id=4KKqHIb4iG"
        },
        "paper_internal_id": "4KKqHIb4iG",
        "category": "reject",
        "embedding_score": 0.7676057815551758,
        "final_score": 0.9854128956794739
      },
      "poster": {
        "paper": {
          "id": "NPSZ7V1CCY",
          "title": "Zero-shot Imputation with Foundation Inference Models for Dynamical Systems",
          "abstract": "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for *zero-shot time series imputation*, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained *offline*, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that *one and the same* (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations — *without requiring any fine-tuning*. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets.\n\nOur pretrained model, repository and tutorials are available online.",
          "keywords": [
            "Zero-shot imputation",
            "foundation models",
            "time series imputation",
            "dynamical systems",
            "amortized inference",
            "zero-shot interpolation",
            "foundation models for time series"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "We introduce a framework for zero-shot imputation of missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Our foundation models often outperform state-of-the-art methods, which are trained on the target datasets.",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-14",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=NPSZ7V1CCY",
          "pdf_link": "https://openreview.net/pdf?id=NPSZ7V1CCY"
        },
        "paper_internal_id": "NPSZ7V1CCY",
        "category": "poster",
        "embedding_score": 0.7499801516532898,
        "final_score": 0.9833405613899231
      },
      "oral": {
        "paper": {
          "id": "AoraWUmpLU",
          "title": "Global Convergence in Neural ODEs: Impact of Activation Functions",
          "abstract": "Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functions—specifically smoothness and nonlinearity—are critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which not only support our analysis but also provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications.",
          "keywords": [
            "Neural ODEs",
            "Gradient Descent",
            "Neural Tangent Kernel (NTK)"
          ],
          "primary_area": "learning theory",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-04",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=AoraWUmpLU",
          "pdf_link": "https://openreview.net/pdf?id=AoraWUmpLU"
        },
        "paper_internal_id": "AoraWUmpLU",
        "category": "oral",
        "embedding_score": 0.7750526666641235,
        "final_score": 0.9943643808364868
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "44cMlQSreK",
      "title": "On Quantizing Neural Representation for Variable-Rate Video Coding",
      "abstract": "This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that variable-rate coding can be achieved by adjusting quantization parameters (QPs) of pre-trained weights. Our study reveals that traditional quantization methods, which assume inter-layer independence, are ineffective for non-generalized INR-VC models due to significant dependencies across layers. To address this, we redefine variable-rate INR-VC as a mixed-precision quantization problem and establish a theoretical framework for sensitivity criteria aimed at simplified, fine-grained rate control. Additionally, we propose network-wise calibration and channel-wise quantization strategies to minimize quantization-induced errors, arriving at a unified formula for representation-oriented PTQ calibration. Our experimental evaluations demonstrate that NeuroQuant significantly outperforms existing techniques in varying bitwidth quantization and compression efficiency, accelerating encoding by up to eight times and enabling quantization down to INT2 with minimal reconstruction loss. This work introduces variable-rate INR-VC for the first time and lays a theoretical foundation for future research in rate-distortion optimization, advancing the field of video coding technology. The materials\nwill be available at https://github.com/Eric-qi/NeuroQuant.",
      "keywords": "['Variable Rate', 'Video Coding', 'Quantization', 'Neural Representation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "44cMlQSreK",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "UKjAwMzX4m",
          "title": "BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM inference",
          "abstract": "Post-training quantization (PTQ) is a promising approach to reducing the storage and computational requirements of large language models (LLMs) without additional training cost. Recent PTQ studies have primarily focused on quantizing only weights to sub-8-bits while maintaining activations at 8-bits or higher. Accurate sub-8-bit quantization for both weights and activations without relying on quantization-aware training remains a significant challenge. In this work, we introduce a novel quantization method called block clustered quantization (BCQ) wherein each operand tensor is decomposed into blocks (a block is a group of contiguous scalars), blocks are clustered based on their statistics, and a dedicated optimal quantization codebook is designed for each cluster. We propose a PTQ algorithm called Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block clustering and codebook design to greedily minimize the quantization mean squared error. When weight and activation scalars are encoded to W4A4 format (with 0.5-bits of overhead for storing scaling factors and codebook selectors), we advance the current state-of-the-art by demonstrating <1% loss in inference accuracy across several LLMs and downstream tasks.",
          "keywords": [
            "Post-training Quantization",
            "Large Language Models",
            "Codebooks",
            "Clustering",
            "Block Clustered Quantization"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "LO-BCQ is a PTQ algorithm that minimizes quantization MSE of both weights and activations to achieve accurate W4A4 quantization for LLM inference.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=UKjAwMzX4m",
          "pdf_link": "https://openreview.net/pdf?id=UKjAwMzX4m"
        },
        "paper_internal_id": "UKjAwMzX4m",
        "category": "reject",
        "embedding_score": 0.7598919868469238,
        "final_score": 0.9898180365562439
      },
      "poster": {
        "paper": {
          "id": "868masI331",
          "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
          "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs)\nthat translate natural language text into sequences of discrete audio tokens have\ngained great research attention, with advances in neural audio codec (NAC) mod-\nels using residual vector quantization (RVQ). However, long-form speech synthe-\nsis remains a significant challenge due to the high frame rate, which increases the\nlength of audio tokens and makes it difficult for autoregressive language models\nto generate audio tokens for even a minute of speech. To address this challenge,\nthis paper introduces two novel post-training approaches: 1) Multi-Resolution Re-\nquantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame\nrate of pre-trained NAC models. Specifically, it incorporates multi-resolution\nresidual vector quantization (MRVQ) module that hierarchically reorganizes dis-\ncrete audio tokens through teacher-student distillation. HALL-E is an LLM-based\nTTS model designed to predict hierarchical tokens of MReQ. Specifically, it incor-\nporates the technique of using MRVQ sub-modules and continues training from a\npre-trained LLM-based TTS model. Furthermore, to promote TTS research, we\ncreate MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered\nspeech data for training and evaluating speech synthesis ranging from 3s up to\n180s. In experiments, we demonstrated the effectiveness of our approaches by ap-\nplying our post-training framework to VALL-E. We achieved the frame rate down\nto as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single\ninference step. Audio samples, dataset, codes and pre-trained models are available\nat https://yutonishimura-v2.github.io/HALL-E_DEMO.",
          "keywords": [
            "Text-to-speech synthesis",
            "LLM-based TTS",
            "neural audio codec",
            "long-form generation"
          ],
          "primary_area": "generative models",
          "TLDR": "We introduce MReQ and HALL-E, methods that reduce frame rates in TTS models to enable minute-long speech synthesis, and present the MinutesSpeech dataset.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=868masI331",
          "pdf_link": "https://openreview.net/pdf?id=868masI331"
        },
        "paper_internal_id": "868masI331",
        "category": "poster",
        "embedding_score": 0.7025977373123169,
        "final_score": 0.9522678256034851
      },
      "oral": {
        "paper": {
          "id": "kbjJ9ZOakb",
          "title": "Learning and aligning single-neuron invariance manifolds in visual cortex",
          "abstract": "Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons.",
          "keywords": [
            "neural invariances",
            "invariance manifold",
            "MEI",
            "implicit neural representations",
            "contrastive learning",
            "invariance alignment",
            "clustering",
            "visual cortex",
            "macaque V1",
            "primary visual cortex"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "Our method learns single-neuron invariances and aligns them, enabling population-level exploration of neural invariances.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-06",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=kbjJ9ZOakb",
          "pdf_link": "https://openreview.net/pdf?id=kbjJ9ZOakb"
        },
        "paper_internal_id": "kbjJ9ZOakb",
        "category": "oral",
        "embedding_score": 0.6767961382865906,
        "final_score": 0.5244985818862915
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "ZV7CLf0RHK",
      "title": "Fine-tuning with Reserved Majority for Noise Reduction",
      "abstract": "Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency.\nHowever, LoRA tuning not only injects knowledgeable features but also noisy hallucination during fine-tuning, which hinders the utilization of tunable parameters with the increasing LoRA rank.\nIn this work, we first investigate in-depth the redundancies among LoRA parameters with substantial empirical studies.\nAiming to resemble the learning capacity of high ranks from the findings, we set up a new fine-tuning framework, \\textbf{P}arameter-\\textbf{Re}dundant \\textbf{F}ine-\\textbf{T}uning (\\preft), which follows the vanilla LoRA tuning process but is required to reduce redundancies before merging LoRA parameters back to pre-trained models.\nBased on this framework, we propose \\textbf{No}ise reduction with \\textbf{R}eserved \\textbf{M}ajority~(\\norm), which decomposes the LoRA parameters into majority parts and redundant parts with random singular value decomposition.\nThe major components are determined by the proposed \\search method, specifically employing subspace similarity to confirm the parameter groups that share the highest similarity with the base weight.\nBy employing \\norm, we enhance both the learning capacity and benefits from larger ranks, which consistently outperforms both LoRA and other \\preft-based methods on various downstream tasks, such as general instruction tuning, math reasoning and code generation. \nCode is available at \\url{https://github.com/pixas/NoRM}.",
      "keywords": "['large language models', 'parameter redundancy fine-tuning', 'noisy reduction']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "ZV7CLf0RHK",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "d465apqCqc",
          "title": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models",
          "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs.",
          "keywords": [
            "supervised fine-tuning",
            "parameter efficient fine-tuning",
            "bias reduction"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=d465apqCqc",
          "pdf_link": "https://openreview.net/pdf?id=d465apqCqc"
        },
        "paper_internal_id": "d465apqCqc",
        "category": "reject",
        "embedding_score": 0.7848425507545471,
        "final_score": 0.9396332502365112
      },
      "poster": {
        "paper": {
          "id": "rWui9vLhOc",
          "title": "MoLEx: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling",
          "abstract": "Large-scale pre-training of deep models, followed by fine-tuning them to adapt to downstream tasks, has become the cornerstone of natural language processing (NLP). The prevalence of vast corpses of data coupled with computational resources has led to large models with a considerable number of parameters. While the massive size of these models has led to remarkable success in many NLP tasks, a detriment is the expense required to retrain all the base model's parameters for the adaptation to each task or domain. Parameter Efficient Fine-Tuning (PEFT) provides a highly effective solution for this challenge by minimizing the number of parameters required to be trained in adjusting to the new task while maintaining the quality of the model. While existing methods have achieved impressive results, they mainly focus on adapting a subset of parameters using adapters, weight reparameterization, and prompt engineering. In this paper, we study layers as extractors of different types of linguistic information that are valuable when used in conjunction with each other. We then propose the Mixture of Layer Experts (MoLEx), a novel Sparse Mixture of Experts (SMoE) whose experts are layers in the pre-trained model. In particular, MoLEx is applied at each layer of the pre-trained model. It performs a conditional computation of a mixture of layers during fine-tuning to provide the model with more structural knowledge about the data. By providing an avenue for information exchange between layers, MoLEx enables the model to make a more well-informed prediction for the downstream task, leading to better fine-tuning results with the same number of effective parameters. As experts can be processed in parallel, MoLEx introduces minimal additional computational overhead. We empirically corroborate the advantages of MoLEx when combined with popular PEFT baseline methods on a variety of downstream fine-tuning tasks, including the popular GLUE benchmark for natural language understanding (NLU) as well as the natural language generation (NLG) End-to-End Challenge (E2E).",
          "keywords": [
            "Parameter efficient fine-tuning",
            "mixture of experts",
            "sparse upcycling"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "we introduce a Mixture of Layer Experts (MoLEx) model that uses a lean upcycle of the layers in the pre-trained model to further improve performance of parameter efficient fine-tuning (PEFT)",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=rWui9vLhOc",
          "pdf_link": "https://openreview.net/pdf?id=rWui9vLhOc"
        },
        "paper_internal_id": "rWui9vLhOc",
        "category": "poster",
        "embedding_score": 0.7583857774734497,
        "final_score": 0.980701208114624
      },
      "oral": {
        "paper": {
          "id": "TwJrTz9cRS",
          "title": "HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models",
          "abstract": "We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code is available at https://github.com/hqsiswiliam/hira.",
          "keywords": [
            "Parametric-efficient fine-tuning",
            "Large Language Model"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-01",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=TwJrTz9cRS",
          "pdf_link": "https://openreview.net/pdf?id=TwJrTz9cRS"
        },
        "paper_internal_id": "TwJrTz9cRS",
        "category": "oral",
        "embedding_score": 0.802709698677063,
        "final_score": 0.9695843458175659
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "fZK6AQXlUU",
      "title": "Conformal Prediction Sets Can Cause Disparate Impact",
      "abstract": "Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.",
      "keywords": "['Conformal Prediction', 'Fairness', 'Uncertainty Quantification', 'Trustworthy ML', 'Human Subject Experiments']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "fZK6AQXlUU",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "LxkgScfHKf",
          "title": "Conformal Training with Reduced Variance",
          "abstract": "Conformal prediction (CP) is a distribution-free framework for achieving probabilistic guarantees on black-box models. {CP} is generally applied to a model post-training. Conformal training is an approach that aims to optimize the CP efficiency during training. In this direction, ConfTr (Stutz et al, 2022) is a technique that seeks to minimize the expected prediction set size of a model by simulating {CP} in-between training updates. Despite its potential, we identify a strong source of sample inefficiency in ConfTr that leads to overly noisy estimated gradients, introducing training instability and limiting practical use. To address this challenge, we propose variance-reduced conformal training (VR-ConfTr), a method that incorporates a variance reduction technique in the gradient estimation of the ConfTr objective function. Through extensive experiments on various benchmark datasets, we demonstrate that VR-ConfTr consistently achieves faster convergence and smaller prediction sets compared to baselines.",
          "keywords": [
            "Conformal Training",
            "Conformal Prediction",
            "Optimization",
            "Quantile",
            "Deep Learning",
            "Uncertainty Quantification"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We improve performance of conformal training by tackling optimization challenges.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=LxkgScfHKf",
          "pdf_link": "https://openreview.net/pdf?id=LxkgScfHKf"
        },
        "paper_internal_id": "LxkgScfHKf",
        "category": "reject",
        "embedding_score": 0.7165089249610901,
        "final_score": 0.998542308807373
      },
      "poster": {
        "paper": {
          "id": "xiQNfYl33p",
          "title": "A Generic Framework for Conformal Fairness",
          "abstract": "Conformal Prediction (CP) is a popular method for uncertainty quantification with machine learning models. While conformal prediction provides probabilistic guarantees regarding the coverage of the true label, these guarantees are agnostic to the presence of sensitive attributes within the dataset. In this work, we formalize \\textit{Conformal Fairness}, a notion of fairness using conformal predictors, and provide a theoretically well-founded algorithm and associated framework to control for the gaps in coverage between different sensitive groups. Our framework leverages the exchangeability assumption (implicit to CP) rather than the typical IID assumption, allowing us to apply the notion of Conformal Fairness to data types and tasks that are not IID, such as graph data. Experiments were conducted on graph and tabular datasets to demonstrate that the algorithm can control fairness-related gaps in addition to coverage aligned with theoretical expectations.",
          "keywords": [
            "Fairness",
            "Conformal Prediction",
            "Graph Neural Networks"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-17",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=xiQNfYl33p",
          "pdf_link": "https://openreview.net/pdf?id=xiQNfYl33p"
        },
        "paper_internal_id": "xiQNfYl33p",
        "category": "poster",
        "embedding_score": 0.8196759223937988,
        "final_score": 0.9993032217025757
      },
      "oral": {
        "paper": {
          "id": "st77ShxP1K",
          "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
          "abstract": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BenchForm, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs’ behavior in collaborative scenarios. Several representative LLMs are evaluated on BenchForm, using metrics such as conformity rate and independence rate to quantify conformity’s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs’ conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code are available at BenchForm.",
          "keywords": [
            "Large Language Models",
            "Conformity",
            "Multi-agent System"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=st77ShxP1K",
          "pdf_link": "https://openreview.net/pdf?id=st77ShxP1K"
        },
        "paper_internal_id": "st77ShxP1K",
        "category": "oral",
        "embedding_score": 0.6137658953666687,
        "final_score": 0.18084381520748138
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "jXvwJ51vcK",
      "title": "Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation",
      "abstract": "Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code is available at github.com/ZhaochongAn/Multimodality-3D-Few-Shot.",
      "keywords": "['3D segmentation', '3D point cloud', 'few-shot segmentation', 'multimodality', 'few-shot point cloud semantic segmentation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "jXvwJ51vcK",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "PD8JVDg8mB",
          "title": "Annotation Bootstrapping: Reinforcing Visual Pre-Training using Unlabelled Images",
          "abstract": "A common approach to learning from unlabeled images is to train models to satisfy invariances on these images, such as consistency under augmentations or crops. Despite successes on Imagenet, these approaches struggle to learn from larger uncurated datasets like web crawls or video, where such inductive biases only weakly hold. How can we more effectively learn from broader datasets? Instead of training models to be invariant across views, we study an alternative approach encouraging model representations to be \\textit{predictive} of important semantics of adjacent views of an image. We concurrently train a model to predict semantic annotations from images (generated either self-supervised, or from auxiliary datasets); and bootstrap the model's semantics by predicting, given a cropped view of an image and the coordinates for a nearby crop, the model's annotation distribution for the neighboring view.  A core strength of this approach is the ability to extract information universally from both unlabelled and labelled image data, incorporating captions, bounding boxes, and other annotations when they are present. Our experiments show that annotation propagation improves pre-training on unlabelled datasets in the wild, including video datasets like EpicKitchens, scene datasets like COCO, and uncurated web-scale image datasets like CC12M.",
          "keywords": [
            "visual pretraining",
            "self supervised learning",
            "bootstrapping"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "A RL-like approach for pre-training on all types of image data (labeled and unlabeled), focused on learning ``web-scale'' datasets like web-crawls and videos.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=PD8JVDg8mB",
          "pdf_link": "https://openreview.net/pdf?id=PD8JVDg8mB"
        },
        "paper_internal_id": "PD8JVDg8mB",
        "category": "reject",
        "embedding_score": 0.6974970102310181,
        "final_score": 0.6438257694244385
      },
      "poster": {
        "paper": {
          "id": "yXCTDhZDh6",
          "title": "Point-SAM: Promptable 3D Segmentation Model for Point Clouds",
          "abstract": "The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains a challenge due to issues such as non-unified data formats, poor model scalability, and the scarcity of labeled data with diverse masks. To this end, we propose a 3D promptable segmentation model Point-SAM, focusing on point clouds. We employ an efficient transformer-based architecture tailored for point clouds, extending SAM to the 3D domain. We then distill the rich knowledge from 2D SAM for Point-SAM training by introducing a data engine to generate part-level and object-level pseudo-labels at scale from 2D SAM. Our model outperforms state-of-the-art 3D segmentation models on several indoor and outdoor benchmarks and demonstrates a variety of applications, such as interactive 3D annotation and zero-shot 3D instance proposal.",
          "keywords": [
            "3D vision",
            "promptable segmentation",
            "point cloud segmentation"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-12",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=yXCTDhZDh6",
          "pdf_link": "https://openreview.net/pdf?id=yXCTDhZDh6"
        },
        "paper_internal_id": "yXCTDhZDh6",
        "category": "poster",
        "embedding_score": 0.744667649269104,
        "final_score": 0.9795573949813843
      },
      "oral": {
        "paper": {
          "id": "CRmiX0v16e",
          "title": "Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation",
          "abstract": "Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP. Consequently, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a novel open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that efficiently leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. \n We demonstrate that our proposed Multi-View Prompt Distribution (MVPDist) method makes use of multi-view information to account for misclassification from the object detector to predict a reliable label for 3D instance masks. Furthermore, since projections of 3D object instances are already contained within the 2D bounding boxes, we show that our proposed low granularity label maps, which require only a 2D object detector to construct, are sufficient and very fast to predict prompt IDs for 3D instance masks when used with our proposed MVPDist.\n We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, \n under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network.\n Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7% while operating at 22 seconds per scene. github.com/aminebdj/OpenYOLO3D",
          "keywords": [
            "Open Vocabulary",
            "3D point cloud instance segmentation"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=CRmiX0v16e",
          "pdf_link": "https://openreview.net/pdf?id=CRmiX0v16e"
        },
        "paper_internal_id": "CRmiX0v16e",
        "category": "oral",
        "embedding_score": 0.7325003743171692,
        "final_score": 0.9443075656890869
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "SG1R2H3fa1",
      "title": "Revisiting Random Walks for Learning on Graphs",
      "abstract": "We revisit a simple model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We call these stochastic machines random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walks guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where 3-WL test fails, and transductive classification on arXiv citation network. Code is available at https://github.com/jw9730/random-walk.",
      "keywords": "['Graph machine learning', 'random walk', 'invariance', 'universal approximation', 'markov chain']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "SG1R2H3fa1",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "0Th6bCZwKt",
          "title": "Gaussian Mixture Models Based Augmentation Enhances GNN Generalization",
          "abstract": "Graph Neural Networks (GNNs) have shown great promise in many learning tasks, notably including node and graph classification, but they face difficulties when tested on new or unseen data. These challenges are exacerbated when training data is limited in size or diversity. To address this issue, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GMM-GDA, a new, efficient graph data augmentation (GDA) algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques but also offers improved time complexity, making it highly suitable for real-world applications.",
          "keywords": [
            "Graph Neural Networks",
            "Data Augmentation"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=0Th6bCZwKt",
          "pdf_link": "https://openreview.net/pdf?id=0Th6bCZwKt"
        },
        "paper_internal_id": "0Th6bCZwKt",
        "category": "reject",
        "embedding_score": 0.7430991530418396,
        "final_score": 0.5135065913200378
      },
      "poster": {
        "paper": {
          "id": "8sSqNntaMr",
          "title": "RouteLLM: Learning to Route LLMs from Preference Data",
          "abstract": "Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a training framework for learning efficient router models that dynamically select between a stronger and weaker LLM during inference. Our framework leverages human preference data and employs data augmentation techniques to enhance performance. Evaluations on public benchmarks show that our approach can reduce costs by over 2 times without sacrificing response quality. Moreover, our routers exhibit strong generalization capabilities, maintaining performance even when routing between LLMs not included in training. This highlights the potential of our framework to deliver cost-effective, high-performance LLM solutions.",
          "keywords": [
            "Large language models",
            "query routing"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-23",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=8sSqNntaMr",
          "pdf_link": "https://openreview.net/pdf?id=8sSqNntaMr"
        },
        "paper_internal_id": "8sSqNntaMr",
        "category": "poster",
        "embedding_score": 0.719003438949585,
        "final_score": 0.4455563426017761
      },
      "oral": {
        "paper": {
          "id": "zBbZ2vdLzH",
          "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
          "abstract": "When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to **j**ointly **d**enoise the features and **r**ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.",
          "keywords": [
            "GNNs",
            "Rewiring",
            "Denoising",
            "Spectral Resonance",
            "cSBM"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We introduce joint denoising and rewiring (JDR)—an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification GNNs.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-08",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=zBbZ2vdLzH",
          "pdf_link": "https://openreview.net/pdf?id=zBbZ2vdLzH"
        },
        "paper_internal_id": "zBbZ2vdLzH",
        "category": "oral",
        "embedding_score": 0.7405905723571777,
        "final_score": 0.6728708744049072
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "DC8bsa9bzY",
      "title": "Estimating the Probabilities of Rare Outputs in Language Models",
      "abstract": "We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.",
      "keywords": "['low probabilities', 'adversarial training', 'importance sampling']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "DC8bsa9bzY",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "0F1rIKppTf",
          "title": "Through the Looking Glass: Mirror Schrödinger Bridges",
          "abstract": "Resampling from a target measure whose density is unknown is a fundamental problem in mathematical statistics and machine learning. A setting that dominates the machine learning literature consists of learning a map from an easy-to-sample prior, such as the Gaussian distribution, to a target measure. Under this model, samples from the prior are pushed forward to generate a new sample on the target measure, which is often difficult to sample from directly. In this paper, we propose a new model for conditional resampling called mirror Schrödinger bridges. Our key observation is that solving the Schrödinger bridge problem between a distribution and itself provides a natural way to produce new samples from conditional distributions, giving in-distribution variations of an input data point. We show how to efficiently solve this largely overlooked version of the Schrödinger bridge problem. We prove that our proposed method leads to significant algorithmic simplifications over existing alternatives, in addition to providing control over conditioning. Empirically, we demonstrate how these benefits can be leveraged to produce proximal samples in a number of application domains.",
          "keywords": [
            "entropic optimal transport",
            "schrödinger bridge",
            "stochastic differential equations",
            "sampling"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "Conditional resampling by solving the Schrödinger bridge problem between a distribution and itself",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=0F1rIKppTf",
          "pdf_link": "https://openreview.net/pdf?id=0F1rIKppTf"
        },
        "paper_internal_id": "0F1rIKppTf",
        "category": "reject",
        "embedding_score": 0.7271206378936768,
        "final_score": 0.8922545909881592
      },
      "poster": {
        "paper": {
          "id": "UgPoHhYQ2U",
          "title": "Uncertainty Herding: One Active Learning Method for All Label Budgets",
          "abstract": "Most active learning research has focused on methods which perform well when many labels are available, but can be dramatically worse than random selection when label budgets are small.\nOther methods have focused on the low-budget regime, but do poorly as label budgets increase.\nAs the line between \"low\" and \"high\" budgets varies by problem,\nthis is a serious issue in practice.\nWe propose *uncertainty coverage*,\nan objective which generalizes a variety of low- and high-budget objectives,\nas well as natural, hyperparameter-light methods to smoothly interpolate between low- and high-budget regimes.\nWe call greedy optimization of the estimate Uncertainty Herding;\nthis simple method is computationally fast,\nand we prove that it nearly optimizes the distribution-level coverage.\nIn experimental validation across a variety of active learning tasks,\nour proposal matches or beats state-of-the-art performance in essentially all cases;\nit is the only method of which we are aware that reliably works well in both low- and high-budget settings.",
          "keywords": [
            "Active learning"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "Most active learning methods struggle to perform well across both low- and high-budget settings. We propose Uncertainty Herding, a fast and flexible method that adapts to all regimes, consistently matching or surpassing state-of-the-art performance.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=UgPoHhYQ2U",
          "pdf_link": "https://openreview.net/pdf?id=UgPoHhYQ2U"
        },
        "paper_internal_id": "UgPoHhYQ2U",
        "category": "poster",
        "embedding_score": 0.7132894992828369,
        "final_score": 0.9089376330375671
      },
      "oral": {
        "paper": {
          "id": "EUSkm2sVJ6",
          "title": "How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning",
          "abstract": "How much of my data was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized usage of their data to train models. However, previous work mistakenly treats this as a binary problem—inferring whether all-or-none or any-or-none of the data was used—which is fragile when faced with real, non-binary data usage risks. To address this, we propose a fine-grained analysis called Dataset Usage Cardinality Inference (DUCI), which estimates the exact proportion of data used. Our algorithm, leveraging debiased membership guesses, matches the performance of the optimal MLE approach (with a maximum error <0.1) but with significantly lower (e.g., $300 \\times$ less) computational cost.",
          "keywords": [
            "Machine Learning",
            "Privacy",
            "Dataset Usage Inference",
            "Dataset Ownership",
            "Membership Inference Attack",
            "Dataset Copyright"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "The first method to quantitatively and non-binarily answer the question ``How much has a dataset been used in the training of a given model?''",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=EUSkm2sVJ6",
          "pdf_link": "https://openreview.net/pdf?id=EUSkm2sVJ6"
        },
        "paper_internal_id": "EUSkm2sVJ6",
        "category": "oral",
        "embedding_score": 0.7049731016159058,
        "final_score": 0.835561990737915
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "dYTtGFuD3S",
      "title": "Adaptive Drug Interaction Prediction via Enhanced Graph Representation Learning",
      "abstract": "This paper presents a groundbreaking theoretical framework for drug-drug interaction (DDI) prediction that seamlessly integrates domain adaptation (DA) techniques with advanced mathematical concepts. We introduce GraphPharmNet, a novel architecture that operates on DDI-DA bundles, leveraging gauge-equivariant geometric deep learning to capture the intricate structure of drug interactions across domains. Our approach reformulates the DDI prediction problem using the language of differential geometry, optimal transport, and symplectic geometry, viewing domain adaptation as a Hamiltonian flow on a statistical manifold. We develop a cohomological interpretation of domain invariance, characterizing robust DDI prediction features through the lens of persistent homology and sheaf theory. The domain adaptation process is analyzed using a geometric renormalization group framework, revealing a profound connection between the DDI-DA bundle's geometry and the emergence of domain-invariant predictive features. We further elucidate the spectral properties of the DDI-DA Laplacian, providing insights into the topological stability of domain adaptation in DDI prediction. Extensive experiments on benchmark datasets demonstrate that GraphPharmNet significantly outperforms existing methods, particularly in scenarios with limited data or when transferring knowledge across disparate domains. Our results highlight the power of this unified mathematical framework in capturing complex drug interactions and adapting to new domains, paving the way for more accurate, robust, and interpretable DDI prediction models. This work not only advances the field of computational drug discovery but also establishes a rigorous theoretical foundation for domain adaptation in graph-structured data, with potential applications across a wide range of scientific disciplines. Our anonymous github link: \\textbf{https://anonymous.4open.science/r/GraphPharmNet-C9D9}",
      "keywords": "['Domain-Aligned，Transfer Learning，Drug-Target Interaction']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "dYTtGFuD3S",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "g3VCIM94ke",
          "title": "Multi-domain Distribution Learning for De Novo Drug Design",
          "abstract": "We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules.",
          "keywords": [
            "Drug Discovery",
            "Flow Matching",
            "Markov Bridge",
            "Equivariance"
          ],
          "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=g3VCIM94ke",
          "pdf_link": "https://openreview.net/pdf?id=g3VCIM94ke"
        },
        "paper_internal_id": "g3VCIM94ke",
        "category": "poster",
        "embedding_score": 0.7480159997940063,
        "final_score": 0.8280112147331238
      },
      "spotlight": {
        "paper": {
          "id": "v9EjwMM55Y",
          "title": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery",
          "abstract": "Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-\nlearning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in ∆AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark.",
          "keywords": [
            "Few-shot molecular representation learning",
            "maching learning"
          ],
          "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
          "TLDR": "We introduce HierMatch, which performs matching across multiple levels, from atoms to tasks, to enhance molecular property predic- tions in few-shot learning scenarios.",
          "creation_date": "2024-09-13",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=v9EjwMM55Y",
          "pdf_link": "https://openreview.net/pdf?id=v9EjwMM55Y"
        },
        "paper_internal_id": "v9EjwMM55Y",
        "category": "spotlight",
        "embedding_score": 0.7126766443252563,
        "final_score": 0.22722472250461578
      },
      "oral": {
        "paper": {
          "id": "zBbZ2vdLzH",
          "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
          "abstract": "When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to **j**ointly **d**enoise the features and **r**ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.",
          "keywords": [
            "GNNs",
            "Rewiring",
            "Denoising",
            "Spectral Resonance",
            "cSBM"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We introduce joint denoising and rewiring (JDR)—an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification GNNs.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-08",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=zBbZ2vdLzH",
          "pdf_link": "https://openreview.net/pdf?id=zBbZ2vdLzH"
        },
        "paper_internal_id": "zBbZ2vdLzH",
        "category": "oral",
        "embedding_score": 0.6574230194091797,
        "final_score": 0.6193419694900513
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "dML3XGvWmy",
      "title": "Gödel Agent: A Self-Referential Framework Helps for Recursively Self-Improvement",
      "abstract": "The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce Gödel Agent, a self-evolving framework inspired by the Gödel machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. Gödel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of Gödel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.",
      "keywords": "['Agent', 'Large Language Model', 'Reasoning', 'Self-Improvement']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "dML3XGvWmy",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "K3KrOsR6y9",
          "title": "LLMs Can Plan Only If We Tell Them",
          "abstract": "Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously.",
          "keywords": [
            "large language models",
            "decision-making",
            "planning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Investigating what enables autonomous planning in large language models",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=K3KrOsR6y9",
          "pdf_link": "https://openreview.net/pdf?id=K3KrOsR6y9"
        },
        "paper_internal_id": "K3KrOsR6y9",
        "category": "poster",
        "embedding_score": 0.7520791292190552,
        "final_score": 0.9497106671333313
      },
      "spotlight": {
        "paper": {
          "id": "o1Et3MogPw",
          "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
          "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.",
          "keywords": [
            "llm agent",
            "multi-agent"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We propose IoA, a novel framework inspired by the Internet for effective collaboration among diverse LLM agents. IoA enables autonomous conversation flow, integration of heterogeneous agents, etc. It outperforms SoTA baselines in various tasks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=o1Et3MogPw",
          "pdf_link": "https://openreview.net/pdf?id=o1Et3MogPw"
        },
        "paper_internal_id": "o1Et3MogPw",
        "category": "spotlight",
        "embedding_score": 0.7843632102012634,
        "final_score": 0.9746679067611694
      },
      "oral": {
        "paper": {
          "id": "YrycTjllL0",
          "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
          "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.",
          "keywords": [
            "Code Generation",
            "Tool Use",
            "Instruction Following",
            "Benchmark"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-12",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YrycTjllL0",
          "pdf_link": "https://openreview.net/pdf?id=YrycTjllL0"
        },
        "paper_internal_id": "YrycTjllL0",
        "category": "oral",
        "embedding_score": 0.7226390838623047,
        "final_score": 0.9261044859886169
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "zbIS2r0t0F",
      "title": "Allostatic Control of Persistent States in Spiking Neural Networks for Perception and Computation",
      "abstract": "We introduce a novel model for updating perceptual beliefs about the environment\nby extending the concept of Allostasis to the control of internal representations.\nAllostasis is a fundamental regulatory mechanism observed in animal physiology\nthat orchestrates responses to maintain a dynamic equilibrium in bodily needs and\ninternal states. In this paper, we focus on an application in numerical cognition,\nwhere a bump of activity in an attractor network is used as a spatial-numerical\nrepresentation. While existing neural networks can maintain persistent states, to\ndate, there is no unified framework for dynamically controlling spatial changes in\nneuronal activity in response to enviromental changes. To address this, we couple\na well-known allostatic microcircuit, the Hammel model, with a ring attractor, re-\nsulting in a Spiking Neural Network architecture that can modulate the location of\nthe bump as a function of some reference input. This localised activity in turn is\nused as a perceptual belief in a simulated subitization task – a quick enumeration\nprocess without counting. We provide a general procedure to fine-tune the model\nand demonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare\nit with biological data. Finally, we analyze the dynamics of the network to un-\nderstand the selectivity and specificity of different neurons to different categories\npresent in the input. The results of this paper, particularly the mechanism for mov-\ning persistent states, are not limited to numerical cognition but can be applied to a\nwide range of tasks involving similar representations.",
      "keywords": "['Allostatic', 'Dynamic', 'Attractors']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "zbIS2r0t0F",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "We5z3UEnUY",
          "title": "Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning",
          "abstract": "Effective decision-making in partially observable environments demands robust memory management. Despite their success in supervised learning, current deep-learning memory models struggle in reinforcement learning environments that are partially observable and long-term. They fail to efficiently capture relevant past information, adapt flexibly to changing observations, and maintain stable updates over long episodes. We theoretically analyze the limitations of existing memory models within a unified framework and introduce the Stable Hadamard Memory, a novel memory model for reinforcement learning agents. Our model dynamically adjusts memory by erasing no longer needed experiences and reinforcing crucial ones computationally efficiently. To this end, we leverage the Hadamard product for calibrating and updating memory, specifically designed to enhance memory capacity while mitigating numerical and learning challenges. Our approach significantly outperforms state-of-the-art memory-based methods on challenging partially observable benchmarks, such as meta-reinforcement learning, long-horizon credit assignment, and POPGym, demonstrating superior performance in handling long-term and evolving contexts.",
          "keywords": [
            "Reinforcement Learning",
            "Memory",
            "POMDP"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "Stable Hadamard Memory introduces a novel, computationally efficient memory model for reinforcement learning, dynamically adjusting to erase irrelevant experiences and reinforce crucial ones.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=We5z3UEnUY",
          "pdf_link": "https://openreview.net/pdf?id=We5z3UEnUY"
        },
        "paper_internal_id": "We5z3UEnUY",
        "category": "poster",
        "embedding_score": 0.6996177434921265,
        "final_score": 0.8736401796340942
      },
      "spotlight": {
        "paper": {
          "id": "60i0ksMAhd",
          "title": "BlendRL: A Framework for Merging Symbolic and Neural Policy Learning",
          "abstract": "Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’ capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. \n\nTo overcome this challenge, we introduce *BlendRL*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. \nWe empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.",
          "keywords": [
            "Neuro-Symbolic AI",
            "Differentiable Reasoning",
            "Reinforcement Learning",
            "Interpretable AI",
            "First-order logic"
          ],
          "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
          "TLDR": "We propose a framework that jointly learns symbolic and neural policies for reinforcement learning.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=60i0ksMAhd",
          "pdf_link": "https://openreview.net/pdf?id=60i0ksMAhd"
        },
        "paper_internal_id": "60i0ksMAhd",
        "category": "spotlight",
        "embedding_score": 0.6518368124961853,
        "final_score": 0.39344578981399536
      },
      "oral": {
        "paper": {
          "id": "kbjJ9ZOakb",
          "title": "Learning and aligning single-neuron invariance manifolds in visual cortex",
          "abstract": "Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons.",
          "keywords": [
            "neural invariances",
            "invariance manifold",
            "MEI",
            "implicit neural representations",
            "contrastive learning",
            "invariance alignment",
            "clustering",
            "visual cortex",
            "macaque V1",
            "primary visual cortex"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "Our method learns single-neuron invariances and aligns them, enabling population-level exploration of neural invariances.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-06",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=kbjJ9ZOakb",
          "pdf_link": "https://openreview.net/pdf?id=kbjJ9ZOakb"
        },
        "paper_internal_id": "kbjJ9ZOakb",
        "category": "oral",
        "embedding_score": 0.7197316884994507,
        "final_score": 0.4154900312423706
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "Thnk4ez3wN",
      "title": "On Learning Representations for Tabular Dataset Distillation",
      "abstract": "Dataset distillation generates a small set of information-rich instances from a large dataset, resulting in reduced storage requirements, privacy or copyright risks, and computational costs for downstream modeling, though much of the research has focused on the image data modality. We study tabular data distillation, which brings in novel challenges such as the inherent feature heterogeneity and the common use of non-differentiable learning models (such as decision tree ensembles and nearest-neighbor predictors). To mitigate these challenges, we present TDColER, a tabular data distillation framework via column embeddings-based representation learning. To evaluate this framework, we also present a tabular data distillation benchmark, TDBench. Based on an elaborate evaluation on TDBench, resulting in 226,200 distilled datasets and 541,980 models trained on them, we demonstrate that TDColER is able to boost the distilled data quality of off-the-shelf distillation schemes by 0.5-143% across 7 different tabular learning models.",
      "keywords": "['Dataset Distillation', 'Tabular Data', 'Representation Learning', 'Autoencoders']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "Thnk4ez3wN",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "c61unr33XA",
          "title": "Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-training of Deep Networks",
          "abstract": "Dataset distillation (DD) generates small synthetic datasets that can efficiently train deep networks with a limited amount of memory and compute. Despite the success of DD methods for supervised learning, DD for self-supervised pre-training of deep models has remained unaddressed. Pre-training on unlabeled data is crucial for efficiently generalizing to downstream tasks with limited labeled data. In this work, we propose the first effective DD method for SSL pre-training. First, we show, theoretically and empirically, that naiive application of supervised DD methods to SSL fails, due to the high variance of the SSL gradient. Then, we address this issue by relying on insights from knowledge distillation (KD) literature. Specifically, we train a small student model to match the representations of a larger teacher model trained with SSL. Then, we generate a small synthetic dataset by matching the training trajectories of the student models. As the KD objective has considerably lower variance than SSL, our approach can generate synthetic datasets that can successfully pre-train high-quality encoders. Through extensive experiments, we show that our distilled sets lead to up to 13% higher accuracy than prior work, on a variety of downstream tasks, in the presence of limited labeled data. Code at https://github.com/BigML-CS-UCLA/MKDT.",
          "keywords": [
            "dataset distillation",
            "self-supervised learning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We present the first effective method for dataset distillation (i.e. creating a small synthetic dataset to summarize a large real dataset) for self-supervised learning.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-01",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=c61unr33XA",
          "pdf_link": "https://openreview.net/pdf?id=c61unr33XA"
        },
        "paper_internal_id": "c61unr33XA",
        "category": "poster",
        "embedding_score": 0.7835348844528198,
        "final_score": 0.8919016718864441
      },
      "spotlight": {
        "paper": {
          "id": "9bMZ29SPVx",
          "title": "A CLIP-Powered Framework for Robust and Generalizable Data Selection",
          "abstract": "Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets inevitably incurs substantial storage and computational overhead. \nMeanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance.\nData selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. \nExisting works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples.\nTo address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. \nSpecifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. \nExtensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality.\n The implementation is available at https://github.com/Jackbrocp/clip-powered-data-selection.",
          "keywords": [
            "Data selection",
            "generalization",
            "multimodal"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-22",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=9bMZ29SPVx",
          "pdf_link": "https://openreview.net/pdf?id=9bMZ29SPVx"
        },
        "paper_internal_id": "9bMZ29SPVx",
        "category": "spotlight",
        "embedding_score": 0.7079721689224243,
        "final_score": 0.22454597055912018
      },
      "oral": {
        "paper": {
          "id": "SctfBCLmWo",
          "title": "A Decade's Battle on Dataset Bias: Are We There Yet?",
          "abstract": "We revisit the ``dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias.",
          "keywords": [
            "Vision datasets",
            "Dataset bias",
            "Deep learning"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "Modern large-scale vision datasets that are supposedly very general and diverse, are in fact still very biased",
          "creation_date": "2024-09-13",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=SctfBCLmWo",
          "pdf_link": "https://openreview.net/pdf?id=SctfBCLmWo"
        },
        "paper_internal_id": "SctfBCLmWo",
        "category": "oral",
        "embedding_score": 0.6853429675102234,
        "final_score": 0.16524602472782135
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "GqGoa44obw",
      "title": "RLHF with Inconsistent Multi-Agent Feedback Under General Function Approximation: A Theoretical Perspective",
      "abstract": "Reinforcement learning from human feedback (RLHF) has been widely studied, as a method for leveraging feedback from human evaluators to guide the learning process. However, existing theoretical analyses typically assume that the human feedback is generated by the ground-truth reward function. This may not be true in practice, because the reward functions in human minds for providing feedback are usually different from the ground-truth reward function, e.g., due to diverse personal experiences and inherent biases. Such inconsistencies could lead to undesirable outcomes when applying existing algorithms, particularly when considering feedback from heterogeneous agents. Therefore, in this paper, we make the first effort to investigate a more practical and general setting of RLHF, where feedback could be generated by multiple agents with reward functions differing from the ground truth. To address this challenge, we develop a new algorithm with novel ideas for handling inconsistent multi-agent feedback, including a Steiner-Point-based confidence set to exploit the benefits of *multi-agent* feedback and a new weighted importance sampling method to manage complexity issues arising from *inconsistency*. Our theoretical analysis develops new methods to demonstrate the optimality of our algorithm. This result is the first of its kind to demonstrate the fundamental impact and potential of inconsistent multi-agent feedback in RLHF.",
      "keywords": "['RLHF theory', 'inconsistent multi-agent feedback', 'regret analysis']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "GqGoa44obw",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "WWXjMYZxfH",
          "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
          "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to preferred outcomes. This hinders learning efficiency and slows convergence.In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions --- sequences of tokens or higher-level language constructs --- into the learning process. By operating at higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30\\% in text summarization and code generation, 18\\% in dialogue, and 8\\% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF $1.7 \\sim 2$ times faster in terms of training time and continues to outperform it with further training. We make our code and data publicly available at \\url{https://github.com/ernie-research/MA-RLHF}.",
          "keywords": [
            "Human Alignment",
            "Large Language Models",
            "Reinforcement Learning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "This paper introduces MA-RLHF, a framework that incorporates macro actions into RLHF for large language models, addressing the credit assignment problem and significantly improving learning efficiency and performance across various tasks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-15",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=WWXjMYZxfH",
          "pdf_link": "https://openreview.net/pdf?id=WWXjMYZxfH"
        },
        "paper_internal_id": "WWXjMYZxfH",
        "category": "poster",
        "embedding_score": 0.7763577699661255,
        "final_score": 0.9993415474891663
      },
      "spotlight": {
        "paper": {
          "id": "cfKZ5VrhXt",
          "title": "Online Preference Alignment for Language Models via Count-based Exploration",
          "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.",
          "keywords": [
            "Reinforcement Learning from Human Feedback",
            "RLHF",
            "Preference Alignment",
            "Exploration",
            "LLMs"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose count-based online preference optimization for LLM alignment that leverages coin-flip counting to encourage exploration in online RLHF.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-22",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=cfKZ5VrhXt",
          "pdf_link": "https://openreview.net/pdf?id=cfKZ5VrhXt"
        },
        "paper_internal_id": "cfKZ5VrhXt",
        "category": "spotlight",
        "embedding_score": 0.7632477879524231,
        "final_score": 0.9869510531425476
      },
      "oral": {
        "paper": {
          "id": "FpiCLJrSW8",
          "title": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness",
          "abstract": "The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness.",
          "keywords": [
            "Large Language Model",
            "Trustworthy ML",
            "Data Attribution"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "Evaluating the Impact of RLHF on Trustworthiness Aspects",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=FpiCLJrSW8",
          "pdf_link": "https://openreview.net/pdf?id=FpiCLJrSW8"
        },
        "paper_internal_id": "FpiCLJrSW8",
        "category": "oral",
        "embedding_score": 0.7666694521903992,
        "final_score": 0.995574951171875
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "pwNIOcr8fU",
      "title": "Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions",
      "abstract": "Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. Additionally, as a data-based approach, SynDR-IQA can be coupled with model-based methods without increasing inference costs. The source code will be publicly available.",
      "keywords": "['Blind Image Quality Assessment; Data Distribution Reshaping; Synthetic Data']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "pwNIOcr8fU",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "GySIAKEwtZ",
          "title": "Geometry of Long-Tailed Representation Learning: Rebalancing Features for Skewed Distributions",
          "abstract": "Deep learning has achieved significant success by training on balanced datasets. However, real-world data often exhibit long-tailed distributions. Empirical studies have revealed that long-tailed data skew data representations, where head classes dominate the feature space. Many methods have been proposed to empirically rectify the skewed representations. However, a clear understanding of the underlying cause and extent of this skew remains lacking. In this study, we provide a comprehensive theoretical analysis to elucidate how long-tailed data affect feature distributions, deriving the conditions under which centers of tail classes shrink together or even collapse into a single point. This results in overlapping feature distributions of tail classes, making features in the overlapping regions inseparable. Moreover, we demonstrate that merely empirically correcting the skewed representations of the training data is insufficient to separate the overlapping features due to distribution shifts between the training and real data. To address these challenges, we propose a novel long-tailed representation learning method, FeatRecon. It reconstructs the feature space in order to arrange features from different classes into symmetricial and linearly separable regions. This, in turn, enhances the model’s robustness to long-tailed data. We validate the effectiveness of our method through extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 datasets.",
          "keywords": [
            "contrastive learning",
            "representation learning",
            "long-tail recgonition",
            "theory",
            "neural collapse"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=GySIAKEwtZ",
          "pdf_link": "https://openreview.net/pdf?id=GySIAKEwtZ"
        },
        "paper_internal_id": "GySIAKEwtZ",
        "category": "poster",
        "embedding_score": 0.7216678857803345,
        "final_score": 0.47066226601600647
      },
      "spotlight": {
        "paper": {
          "id": "9bMZ29SPVx",
          "title": "A CLIP-Powered Framework for Robust and Generalizable Data Selection",
          "abstract": "Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets inevitably incurs substantial storage and computational overhead. \nMeanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance.\nData selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. \nExisting works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples.\nTo address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. \nSpecifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. \nExtensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality.\n The implementation is available at https://github.com/Jackbrocp/clip-powered-data-selection.",
          "keywords": [
            "Data selection",
            "generalization",
            "multimodal"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-22",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=9bMZ29SPVx",
          "pdf_link": "https://openreview.net/pdf?id=9bMZ29SPVx"
        },
        "paper_internal_id": "9bMZ29SPVx",
        "category": "spotlight",
        "embedding_score": 0.7346253395080566,
        "final_score": 0.5649409294128418
      },
      "oral": {
        "paper": {
          "id": "N8Oj1XhtYZ",
          "title": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers",
          "abstract": "We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4)  Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.",
          "keywords": [
            "Efficient AI",
            "Diffusion Models",
            "Text to Image generation"
          ],
          "primary_area": "generative models",
          "TLDR": "Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed.",
          "creation_date": "2024-09-19",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=N8Oj1XhtYZ",
          "pdf_link": "https://openreview.net/pdf?id=N8Oj1XhtYZ"
        },
        "paper_internal_id": "N8Oj1XhtYZ",
        "category": "oral",
        "embedding_score": 0.7205867767333984,
        "final_score": 0.21254666149616241
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "viQ1bLqKY0",
      "title": "EXecution-Eval: Can language models execute real-world code?",
      "abstract": "As Large Language Models (LLMs) advance, traditional benchmarks face challenges of dataset saturation and disconnection from real-world performance, limiting our understanding of true model capabilities. We introduce EXecution-Eval (EXE), a benchmark designed to assess LLMs' ability to execute code and predict program states. EXE attempts to address key limitations in existing evaluations: difficulty scaling, task diversity, training data contamination, and cost-effective scalability.\nComprising over 30,000 tasks derived from 1,000 popular Python repositories on GitHub, EXE spans a range of context lengths and algorithmic complexities. Tasks require models to execute code, necessitating various operations including mathematical reasoning, logical inference, bit manipulation, string operations, loop execution, and maintaining multiple internal variable states during computation. Our methodology involves: (a) selecting and preprocessing GitHub repositories, (b) generating diverse inputs for functions, (c) executing code to obtain ground truth outputs, and (d) formulating tasks that require models to reason about code execution. This approach allows for continuous new task generation for as few as 1,200 tokens, significantly reducing the risk of models \"training on the test set.\"\nWe evaluate several state-of-the-art LLMs on EXE, revealing insights into their code comprehension and execution capabilities. Our results show that even the best-performing models struggle with complex, multi-step execution tasks, highlighting specific computational concepts that pose the greatest challenges for today's LLMs. Furthermore, we review EXE's potential for finding and predicting errors to aid in assessing a model's cybersecurity capabilities. We propose EXE as a sustainable and challenging testbed for evaluating frontier models, offering potential insights into their internal mechanistic advancement",
      "keywords": "['large language model', 'evaluation', 'benchmark', 'code execution']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "viQ1bLqKY0",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "XLMAMmowdY",
          "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
          "abstract": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs",
          "keywords": [
            "Agent",
            "Tool Learning",
            "Virtual Token"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "Unified tool retrieval and calling by transforming tools into virtual tokens",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=XLMAMmowdY",
          "pdf_link": "https://openreview.net/pdf?id=XLMAMmowdY"
        },
        "paper_internal_id": "XLMAMmowdY",
        "category": "poster",
        "embedding_score": 0.7394343614578247,
        "final_score": 0.9898709654808044
      },
      "spotlight": {
        "paper": {
          "id": "yVQcr4qjD6",
          "title": "Robust Function-Calling for On-Device Language Model via Function Masking",
          "abstract": "Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models’ sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance.",
          "keywords": [
            "language models",
            "function-calling",
            "mobile assistant",
            "tool-using"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-16",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=yVQcr4qjD6",
          "pdf_link": "https://openreview.net/pdf?id=yVQcr4qjD6"
        },
        "paper_internal_id": "yVQcr4qjD6",
        "category": "spotlight",
        "embedding_score": 0.7472328543663025,
        "final_score": 0.9832496047019958
      },
      "oral": {
        "paper": {
          "id": "YrycTjllL0",
          "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
          "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.",
          "keywords": [
            "Code Generation",
            "Tool Use",
            "Instruction Following",
            "Benchmark"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-12",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YrycTjllL0",
          "pdf_link": "https://openreview.net/pdf?id=YrycTjllL0"
        },
        "paper_internal_id": "YrycTjllL0",
        "category": "oral",
        "embedding_score": 0.8461626768112183,
        "final_score": 0.9714411497116089
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "TLgDQ0Rr2Z",
      "title": "Principle Counterfactual Fairness",
      "abstract": "Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on the protected attributes. Nevertheless, the question of \"which attributes and individuals should be protected\" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally take into this factor for the purpose of selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds, and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods.",
      "keywords": "['Counterfactual Fairness']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "TLgDQ0Rr2Z",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "rPkCVSsoM4",
          "title": "A Causal Lens for Learning Long-term Fair Policies",
          "abstract": "Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic decision-making systems while simultaneously considering instantaneous fairness requirements. In the context of reinforcement learning, we propose a general framework where long-term fairness is measured by the difference in the average expected qualification gain that individuals from different groups could obtain. Then, through a causal lens, we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect the policy has on the qualification gain. We analyze the intrinsic connection between these components and an emerging fairness notion called benefit fairness that aims to control the equity of outcomes in decision-making. Finally, we develop a simple yet effective approach for balancing various fairness notions.",
          "keywords": [
            "long-term fairness",
            "fair reinforcement learning",
            "causal decomposition"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-22",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=rPkCVSsoM4",
          "pdf_link": "https://openreview.net/pdf?id=rPkCVSsoM4"
        },
        "paper_internal_id": "rPkCVSsoM4",
        "category": "poster",
        "embedding_score": 0.7551818490028381,
        "final_score": 0.6620836853981018
      },
      "spotlight": {
        "paper": {
          "id": "fZK6AQXlUU",
          "title": "Conformal Prediction Sets Can Cause Disparate Impact",
          "abstract": "Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.",
          "keywords": [
            "Conformal Prediction",
            "Fairness",
            "Uncertainty Quantification",
            "Trustworthy ML",
            "Human Subject Experiments"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We demonstrate that providing conformal prediction sets to human decision makers can increase the unfairness of outcomes, and that applying Equalized Coverage increases unfairness more than marginal coverage.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-14",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=fZK6AQXlUU",
          "pdf_link": "https://openreview.net/pdf?id=fZK6AQXlUU"
        },
        "paper_internal_id": "fZK6AQXlUU",
        "category": "spotlight",
        "embedding_score": 0.7069796919822693,
        "final_score": 0.6384480595588684
      },
      "oral": {
        "paper": {
          "id": "stUKwWBuBm",
          "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
          "abstract": "A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \\emph{all} $n$-player matrix and finite-horizon Markov games.  In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality.  To validate the expressivity of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model. We validate our findings on a simple multi-agent reinforcement learning benchmark. Our results open the doors for to the principled development of new decentralized multi-agent reinforcement learning algorithms.",
          "keywords": [
            "behavioral economics",
            "risk-aversion",
            "multi-agent reinforcement learning",
            "quantal response",
            "bounded rationality"
          ],
          "primary_area": "learning theory",
          "TLDR": "By incorporating risk aversion and bounded rationality into agents' decision-making processes, we introduced a computationally tractable equilibria class for matrix and Markov games which aligns with observed human behaviors.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-30",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=stUKwWBuBm",
          "pdf_link": "https://openreview.net/pdf?id=stUKwWBuBm"
        },
        "paper_internal_id": "stUKwWBuBm",
        "category": "oral",
        "embedding_score": 0.6267061233520508,
        "final_score": 0.35981032252311707
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "VRlihVklCL",
      "title": "MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data",
      "abstract": "Large language models (LLMs), like ChatGPT, have shown that even trained with noisy prior data,  they can generalize effectively to new tasks through in-context learning (ICL) and pre-training techniques.\nMotivated by this, we explore whether a similar approach can be applied to scientific foundation models (SFMs). Our methodology is structured as follows: (i) we collect low-cost physics-informed neural network (PINN)-based approximated prior data in the form of solutions to partial differential equations (PDEs) constructed through an arbitrary linear combination of mathematical dictionaries; (ii) we utilize Transformer architectures with self and cross-attention mechanisms to predict PDE solutions without knowledge of the governing equations in a zero-shot setting; (iii) we provide experimental evidence on the one-dimensional convection-diffusion-reaction equation, which demonstrate that pre-training remains robust even with approximated prior data, with only marginal impacts on test accuracy. Notably, this finding opens the path to pre-training SFMs with realistic, low-cost data instead of (or in conjunction with) numerical high-cost data. These results support the conjecture that SFMs can improve in a manner similar to LLMs, where fully cleaning the vast set of sentences crawled from the Internet is nearly impossible.",
      "keywords": "['in-context learning', 'scientific foundation model', 'zero-shot', 'PINN-prior']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "VRlihVklCL",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "SMK0f8JoKF",
          "title": "Language Models Are Implicitly Continuous",
          "abstract": "Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators.\nIn this work, we show that Transformer-based language models implicitly learn to represent sentences as continuous-time functions defined over a continuous input space. \nThis phenomenon occurs in most state-of-the-art Large Language Models (LLMs), including Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that LLMs reason about language in ways that fundamentally differ from humans.\nOur work formally extends Transformers to capture the nuances of time and space continuity in both input and output space.\nOur results challenge the traditional interpretation of how LLMs understand language, with several linguistic and engineering implications.",
          "keywords": [
            "llm",
            "continuity",
            "spatiotemporal transformers",
            "linguistics"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "LLMs implicitly behave like continuous models, even when trained in a discrete fashion.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-13",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=SMK0f8JoKF",
          "pdf_link": "https://openreview.net/pdf?id=SMK0f8JoKF"
        },
        "paper_internal_id": "SMK0f8JoKF",
        "category": "poster",
        "embedding_score": 0.7355301380157471,
        "final_score": 0.9739511609077454
      },
      "spotlight": {
        "paper": {
          "id": "qtWjSboqfe",
          "title": "DEEM: Diffusion models serve as the eyes of large language models for image perception",
          "abstract": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's  visual perception performance to a large extent (e.g., 4\\% ↑ on RobustVQA, 6.5\\% ↑ on MMVP and 12.8 \\% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM  exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis.",
          "keywords": [
            "MLLM; Diffusion Model;"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "Diffusion Model Can help MLLM see better",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-21",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=qtWjSboqfe",
          "pdf_link": "https://openreview.net/pdf?id=qtWjSboqfe"
        },
        "paper_internal_id": "qtWjSboqfe",
        "category": "spotlight",
        "embedding_score": 0.7280415296554565,
        "final_score": 0.8205915689468384
      },
      "oral": {
        "paper": {
          "id": "QWunLKbBGF",
          "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
          "abstract": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and\nproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. \nOur benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular,  in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.",
          "keywords": [
            "personalization",
            "benchmark",
            "Large language models",
            "conversational llm",
            "chatbots"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=QWunLKbBGF",
          "pdf_link": "https://openreview.net/pdf?id=QWunLKbBGF"
        },
        "paper_internal_id": "QWunLKbBGF",
        "category": "oral",
        "embedding_score": 0.7299467921257019,
        "final_score": 0.9501538276672363
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "RcNzwKrjTo",
      "title": "Conformal Prediction Sets with Improved Conditional Coverage using Trust Scores",
      "abstract": "Standard conformal prediction offers a marginal guarantee on coverage, but for prediction sets to be truly useful, they should ideally ensure coverage conditional on each test point. However, it is impossible to achieve exact, distribution-free conditional coverage in finite samples. In this work, we propose an alternative conformal prediction algorithm that targets coverage where it matters most---in instances where a classifier is overconfident in its incorrect predictions. We start by dissecting miscoverage events in marginally-valid conformal prediction, and show that miscoverage rates vary based on the classifier's confidence and its deviation from the Bayes optimal classifier. Motivated by this insight, we develop a variant of conformal prediction that targets coverage conditional on a reduced set of two variables: the classifier's confidence in a prediction and a nonparametric trust score that measures its deviation from the Bayes classifier. Empirical evaluation on multiple image datasets shows that our method generally improves conditional coverage properties compared to standard conformal prediction, including class-conditional coverage, coverage over arbitrary subgroups, and coverage over demographic groups.",
      "keywords": "['uncertainty quantification', 'conformal prediction', 'conditional guarantees']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "RcNzwKrjTo",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "Nfd7z9d6Bb",
          "title": "Probabilistic Conformal Prediction with Approximate Conditional Validity",
          "abstract": "We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $\\textup{P}_{Y \\mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications.",
          "keywords": [
            "Conformal Prediction",
            "Conditional coverage",
            "Probabilistic method",
            "Uncertainty Quantification"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "We introduce a method that effectively integrates conformal approaches with an estimate of the conditional distribution to ensure the approximate conditional validity.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=Nfd7z9d6Bb",
          "pdf_link": "https://openreview.net/pdf?id=Nfd7z9d6Bb"
        },
        "paper_internal_id": "Nfd7z9d6Bb",
        "category": "poster",
        "embedding_score": 0.839959979057312,
        "final_score": 0.9798566699028015
      },
      "spotlight": {
        "paper": {
          "id": "fZK6AQXlUU",
          "title": "Conformal Prediction Sets Can Cause Disparate Impact",
          "abstract": "Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.",
          "keywords": [
            "Conformal Prediction",
            "Fairness",
            "Uncertainty Quantification",
            "Trustworthy ML",
            "Human Subject Experiments"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We demonstrate that providing conformal prediction sets to human decision makers can increase the unfairness of outcomes, and that applying Equalized Coverage increases unfairness more than marginal coverage.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-14",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=fZK6AQXlUU",
          "pdf_link": "https://openreview.net/pdf?id=fZK6AQXlUU"
        },
        "paper_internal_id": "fZK6AQXlUU",
        "category": "spotlight",
        "embedding_score": 0.7893874645233154,
        "final_score": 0.9050468802452087
      },
      "oral": {
        "paper": {
          "id": "UHPnqSTBPO",
          "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
          "abstract": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this *selective evaluation* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce *Simulated Annotators*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose *Cascaded Selective Evaluation*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, *guarantees* over 80% human agreement with almost 80% test coverage.",
          "keywords": [
            "Large Language Model",
            "LLM",
            "LLM Judge",
            "Evaluation",
            "Alignment"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We propose Cascaded Selective Evaluation, an LLM-as-Judge framework that dynamically selects when to trust different judge models to reduce evaluation overhead, while providing a provable guarantee of human-judge agreement.",
          "creation_date": "2024-09-22",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=UHPnqSTBPO",
          "pdf_link": "https://openreview.net/pdf?id=UHPnqSTBPO"
        },
        "paper_internal_id": "UHPnqSTBPO",
        "category": "oral",
        "embedding_score": 0.6815863847732544,
        "final_score": 0.7037734985351562
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "uaKBM9sGEm",
      "title": "Towards Off-Road Autonomous Driving via Planner Guided Policy Optimization",
      "abstract": "Off-road autonomous driving poses significant challenges such as navigating diverse terrains, avoiding obstacles, and maneuvering through ditches. Addressing these challenges requires effective planning and adaptability, making it a long-horizon planning and control problem. Traditional model-based control techniques like Model Predictive Path Integral (MPPI) require dense sampling and accurate modeling of the vehicle-terrain interaction, both of which are computationally expensive, making effective long-horizon planning in real-time intractable. Reinforcement learning (RL) methods operate without this limitation and are computationally cheaper at deployment. However, exploration in obstacle-dense and challenging terrains is difficult, and typical RL techniques struggle to navigate in these terrains. To alleviate the limitations of MPPI, we propose a hierarchical autonomy pipeline with a low-frequency high-level MPPI planner and a high-frequency low-level RL controller. To tackle RL's exploration challenge, we propose a teacher-student paradigm to learn an end-to-end RL policy, capable of real-time execution and traversal through challenging terrains. The teacher policy is trained using dense planning information from an MPPI planner while the student policy learns to navigate using visual inputs and sparse planning information. In this framework, we introduce a new policy gradient formulation that extends Proximal Policy Optimization (PPO), leveraging off-policy trajectories for teacher guidance and on-policy trajectories for student exploration. We demonstrate our performance in a realistic off-road simulator against various RL and imitation learning methods.",
      "keywords": "['Reinforcement learning', 'Learning from Demonstrations', 'Autonomous driving', 'Off-road driving']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "uaKBM9sGEm",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "8NdNniulYE",
          "title": "STAMP: Scalable Task- And Model-agnostic Collaborative Perception",
          "abstract": "Perception is a crucial component of autonomous driving systems. However, single-agent setups often face limitations due to sensor constraints, especially under challenging conditions like severe occlusion, adverse weather, and long-range object detection. Multi-agent collaborative perception (CP) offers a promising solution that enables communication and information sharing between connected vehicles. Yet, the heterogeneity among agents—in terms of sensors, models, and tasks—significantly hinders effective and efficient cross-agent collaboration. To address these challenges, we propose STAMP, a scalable task- and model-agnostic collaborative perception framework tailored for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific domains and a shared protocol domain, facilitating efficient feature sharing and fusion while minimizing computational overhead. Moreover, our approach enhances scalability, preserves model security, and accommodates a diverse range of agents. Extensive experiments on both simulated (OPV2V) and real-world (V2V4Real) datasets demonstrate that STAMP achieves comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As the first-of-its-kind task- and model-agnostic collaborative perception framework, STAMP aims to advance research in scalable and secure mobility systems, bringing us closer to Level 5 autonomy. Our project page is at https://xiangbogaobarry.github.io/STAMP and the code is available at https://github.com/taco-group/STAMP.",
          "keywords": [
            "Autonomous Driving",
            "Collaborative Perception",
            "Domain Adaptation"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We propose STAMP, a scalable, task- and model-agnostic collaborative perception framework that enables efficient feature sharing and fusion among heterogeneous agents in autonomous driving",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-23",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=8NdNniulYE",
          "pdf_link": "https://openreview.net/pdf?id=8NdNniulYE"
        },
        "paper_internal_id": "8NdNniulYE",
        "category": "poster",
        "embedding_score": 0.7267186641693115,
        "final_score": 0.7954948544502258
      },
      "spotlight": {
        "paper": {
          "id": "8UFG9D8xeU",
          "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations",
          "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies.",
          "keywords": [
            "Efficient Post-training Preference Alignment",
            "Alignment from demonstrations",
            "Multi-agent Motion Generation"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We propose an efficient post-training alignment approach that significantly improves the pre-trained motion generation model’s quality without requiring additional post-training human preference annotation or expansive compute.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-26",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=8UFG9D8xeU",
          "pdf_link": "https://openreview.net/pdf?id=8UFG9D8xeU"
        },
        "paper_internal_id": "8UFG9D8xeU",
        "category": "spotlight",
        "embedding_score": 0.693928599357605,
        "final_score": 0.52943354845047
      },
      "oral": {
        "paper": {
          "id": "wM2sfVgMDH",
          "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
          "abstract": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.",
          "keywords": [
            "diffusion planning",
            "autonomous driving"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=wM2sfVgMDH",
          "pdf_link": "https://openreview.net/pdf?id=wM2sfVgMDH"
        },
        "paper_internal_id": "wM2sfVgMDH",
        "category": "oral",
        "embedding_score": 0.7902950048446655,
        "final_score": 0.5966985821723938
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "TkbjqexD8w",
      "title": "Invariant Spatiotemporal Representation Learning for Cross-patient Seizure Classification",
      "abstract": "Automatic seizure type classification from electroencephalogram (EEG) data can help clinicians to better diagnose epilepsy. Although many previous studies have focused on the classification problem of seizure EEG data, most of these methods require that there is no distribution shift between training data and test data, which greatly limits the applicability in real-world scenarios. In this paper, we propose an invariant spatiotemporal representation learning method for cross-patient seizure classification. Specifically, we first split the spatiotemporal EEG data into different environments based on heterogeneous risk minimization to reflect the spurious correlations. We then learn invariant spatiotemporal representations and train the seizure classification model based on the learned representations to achieve accurate seizure-type classification across various environments. The experiments are conducted on the largest public EEG dataset, the Temple University Hospital Seizure Corpus (TUSZ) dataset, and the experimental results demonstrate the effectiveness of our method.",
      "keywords": "['electroencephalogram data', 'spatiotemporal data', 'invariant representation learning']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "TkbjqexD8w",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "bwOndfohRK",
          "title": "Neural networks on Symmetric Spaces of Noncompact Type",
          "abstract": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.",
          "keywords": [
            "geometric deep learning",
            "symmetric spaces",
            "hyperbolic spaces",
            "SPD manifolds"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=bwOndfohRK",
          "pdf_link": "https://openreview.net/pdf?id=bwOndfohRK"
        },
        "paper_internal_id": "bwOndfohRK",
        "category": "poster",
        "embedding_score": 0.69272780418396,
        "final_score": 0.7511955499649048
      },
      "spotlight": {
        "paper": {
          "id": "fGdF8Bq1FV",
          "title": "Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors",
          "abstract": "We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and \"test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the \"structure\" and \"simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).",
          "keywords": [
            "Representation learning algorithm",
            "Gaussian-Mixture",
            "regularizer",
            "rate-disotortion"
          ],
          "primary_area": "learning theory",
          "TLDR": "We derive generalization bounds for the representation learning algorithms and, inspired by the bounds, propose a regularizer with data-dependent Gaussian mixture priors.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-07",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=fGdF8Bq1FV",
          "pdf_link": "https://openreview.net/pdf?id=fGdF8Bq1FV"
        },
        "paper_internal_id": "fGdF8Bq1FV",
        "category": "spotlight",
        "embedding_score": 0.6576583385467529,
        "final_score": 0.58524090051651
      },
      "oral": {
        "paper": {
          "id": "1CLzLXSFNn",
          "title": "TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis",
          "abstract": "Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce TimeMixer++, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.",
          "keywords": [
            "time series",
            "pattern machine",
            "predictive analysis"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "TimeMixer++ is a time series pattern machine that employs multi-scale and multi-resolution pattern extraction to deliver SOTA performance across 8 diverse analytical tasks, including forecasting, classification, anomaly detection, and imputation.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=1CLzLXSFNn",
          "pdf_link": "https://openreview.net/pdf?id=1CLzLXSFNn"
        },
        "paper_internal_id": "1CLzLXSFNn",
        "category": "oral",
        "embedding_score": 0.6598263382911682,
        "final_score": 0.5407130718231201
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "BOQpRtI4F5",
      "title": "Towards Bridging Generalization and Expressivity of Graph Neural Networks",
      "abstract": "Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of graph-structured data.\nIn this work, we address the intricate relationship between expressivity and generalization in GNNs. Theoretical studies conjecture a trade-off between the two: highly expressive models risk overfitting, while those focused on generalization may sacrifice expressivity. However, empirical evidence often contradicts this assumption, with expressive GNNs frequently demonstrating strong generalization. We explore this contradiction by introducing a novel framework that connects GNN generalization to the variance in graph structures they can capture. This leads us to propose a $k$-variance margin-based generalization bound that characterizes the structural properties of graph embeddings in terms of their upper-bounded expressive power. Our analysis does not rely on specific GNN architectures, making it broadly applicable across GNN models. We further uncover a trade-off between intra-class concentration and inter-class separation, both of which are crucial for effective generalization. Through case studies and experiments on real-world datasets, we demonstrate that our theoretical findings align with empirical results, offering a deeper understanding of how expressivity can enhance GNN generalization.",
      "keywords": "['gnn', 'expressivity', 'generalization']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "BOQpRtI4F5",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "TVwD2zIQ1F",
          "title": "Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoors",
          "abstract": "Generalization of machine learning models can be severely compromised by data poisoning, where adversarial changes are applied to the training data. This vulnerability has led to interest in certifying (i.e., proving) that such changes up to a certain magnitude do not affect test predictions. We, for the first time, certify Graph Neural Networks (GNNs) against poisoning attacks, including backdoors, targeting the node features of a given graph. Our certificates are white-box and based upon (i) the neural tangent kernel, which characterizes the training dynamics of sufficiently wide networks; and (ii) a novel reformulation of the bilevel optimization problem describing poisoning as a mixed-integer linear program. Consequently, we leverage our framework to provide fundamental insights into the role of graph structure and its connectivity on the worst-case robustness behavior of convolution-based and PageRank-based GNNs. We note that our framework is more general and constitutes the first approach to derive white-box poisoning certificates for NNs, which can be of independent interest beyond graph-related tasks.",
          "keywords": [
            "graph neural networks",
            "provable robustness",
            "certificates",
            "poisoning",
            "data poisoning",
            "backdoor attacks",
            "neural tangent kernel",
            "adversarial robustness",
            "mixed-integer linear programming",
            "support vector machines"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "First deterministic & white-box certificates for (graph) neural networks against data poisoning & backdoors based on (i) the neural tangent kernel, and (ii) a novel reformulation of the bilevel poisoning problem as a mixed-integer linear program.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=TVwD2zIQ1F",
          "pdf_link": "https://openreview.net/pdf?id=TVwD2zIQ1F"
        },
        "paper_internal_id": "TVwD2zIQ1F",
        "category": "reject",
        "embedding_score": 0.7186906933784485,
        "final_score": 0.9804449081420898
      },
      "spotlight": {
        "paper": {
          "id": "sZQRUrvLn4",
          "title": "Graph Neural Networks Can (Often) Count Substructures",
          "abstract": "Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs.\nBecause of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism problem), a task that is of great importance for several types of graph-structured data. \nHowever, we observe that GNNs are in fact able to count graph patterns quite accurately across several real-world graph datasets.\nMotivated by this observation, we provide an analysis of the subgraph-counting capabilities of GNNs beyond the worst case, deriving several sufficient conditions for GNNs to be able to count subgraphs and, more importantly, to be able to sample-efficiently learn to count subgraphs. \nMoreover, we develop novel dynamic programming algorithms for solving the subgraph isomorphism problem on restricted classes of pattern and target graphs, and show that message-passing GNNs can efficiently simulate these dynamic programs. \nFinally, we empirically validate that our sufficient conditions for GNNs to count subgraphs hold on many real-world datasets, providing a theoretically-grounded explanation to our motivating observations.",
          "keywords": [
            "graph neural networks",
            "subgraphs",
            "expressivity"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We provide a theoretical analysis of the subgraph-counting capabilities of graph neural networks beyond the worst case.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=sZQRUrvLn4",
          "pdf_link": "https://openreview.net/pdf?id=sZQRUrvLn4"
        },
        "paper_internal_id": "sZQRUrvLn4",
        "category": "spotlight",
        "embedding_score": 0.7698301076889038,
        "final_score": 0.9687498211860657
      },
      "oral": {
        "paper": {
          "id": "OIvg3MqWX2",
          "title": "A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules",
          "abstract": "Graph neural networks (GNNs) -- learn graph representations by exploiting the graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine the general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks. Code is available at https://github.com/shihhsinwang0214/SCHull.",
          "keywords": [
            "Graph representation",
            "sparsity",
            "connectivity",
            "rigidity",
            "molecules",
            "learning"
          ],
          "primary_area": "learning on graphs and other geometries & topologies",
          "TLDR": "We introduce a new sparse, connected, and rigid graph representation for molecules.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-05",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=OIvg3MqWX2",
          "pdf_link": "https://openreview.net/pdf?id=OIvg3MqWX2"
        },
        "paper_internal_id": "OIvg3MqWX2",
        "category": "oral",
        "embedding_score": 0.7244705557823181,
        "final_score": 0.9824731349945068
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "cFu7ze7xUm",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges.\nCaching all Key and Value (KV) states across all attention heads consumes substantial memory.\nExisting KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements.\nIn this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens.\nIn contrast, all other heads, which primarily focus on recent tokens and attention sinks—referred to as Streaming Heads—do not require full attention.\nBased on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately.\nOur method significantly reduces long-context inference memory by up to 2.55$\\times$ for MHA and 1.67$\\times$ for GQA models while speeding up decoding by up to 2.18$\\times$ and 1.50$\\times$ and accelerating pre-filling by up to 1.73$\\times$ and 1.63$\\times$ for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention.\nNotably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.33 million context length measured on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "keywords": "['Large Language Models; Long Context; Efficiency;']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "cFu7ze7xUm",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "LlE61BEYpB",
          "title": "FLARE: Fine-tuned Long-context Acceleration with ReLU-enhanced FIRE",
          "abstract": "Deploying large language models (LLMs) on resource-constrained edge devices is challenging due to computational bottlenecks, memory bottlenecks, and -- for long-contexts -- specifically the Softmax operation in the attention mechanism. While using ReLU in place of Softmax has been explored, and FIRE as an alternative to RoPE has been explored for models trained from scratch, there has been little work towards exploring fine-tuning models to utilize these efficient algorithms, or the combination of the two.\n\nIn this paper, we contribute FLARE, a method for fusing Rectified Linear Activations (ReLU) with Relative Encodings (specifically FIRE), and we share a particular recipe which allows these to be fine-tuned effectively into existing models and fused to create efficient long-context inference. Following this recipe yields markedly better validation loss, long-context inference speed, and successfully introduces the property of length-generalization -- the property where the model gains high accuracy for contexts lengths several times larger than trained -- unlike RoPE -- without further fine-tuning.   \n\nOnce FIRE and ReLU are both fine-tuned into a model, we show these can be mathematically fused into a single, more efficient operation, which on average was found to shave 98.9\\% of FIRE operations and produce a Probability matrix with 98.9\\% zeros in its lower-triangle.\n\nFinally, we benchmark inference speed improvements for custom hardware as well with custom CUDA kernels. Using Power, Performance, and Area (PPA) analysis, we show that FLARE operates at eight times the frequency of Softmax while consuming only 0.1\\% of the power and 0.11\\% of the energy per cycle. Our custom CUDA Kernel shows 3.8x faster operation than Softmax FlashAttention. We believe this shows the potential of fine-tuning new algorithms in pre-trained models, and we share our fine-tuning recipes, code and custom hardware designs at \\url{https://anonymous.4open.science/r/nanoGPTBD54}.",
          "keywords": [
            "FIRE",
            "Functional Interpolation for Relative Position Encoding",
            "fine-tune",
            "fine-tuning",
            "ReLU",
            "Softmax",
            "Softplus",
            "Softmax alternatives",
            "long context",
            "transformer",
            "large language model",
            "edge device",
            "Flash Attention"
          ],
          "primary_area": "optimization",
          "TLDR": "We fine-tune LLMs for edge hardware by replacing Softmax with ReLU and Softplus element-wise alternatives, combining FIRE and ReLU into a single more efficient operation, and showing significant efficiency improvements scaling with context length.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=LlE61BEYpB",
          "pdf_link": "https://openreview.net/pdf?id=LlE61BEYpB"
        },
        "paper_internal_id": "LlE61BEYpB",
        "category": "reject",
        "embedding_score": 0.7684521675109863,
        "final_score": 0.9909960627555847
      },
      "spotlight": {
        "paper": {
          "id": "FDnZFpHmU4",
          "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
          "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$ \\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \\textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.",
          "keywords": [
            "Model ensembling",
            "LLM"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "This study introduces a general model selection strategy for ensembling and proposes an efficient ensemble method that operates on the top-k candidate tokens.",
          "creation_date": "2024-09-16",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-26",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=FDnZFpHmU4",
          "pdf_link": "https://openreview.net/pdf?id=FDnZFpHmU4"
        },
        "paper_internal_id": "FDnZFpHmU4",
        "category": "spotlight",
        "embedding_score": 0.7019190788269043,
        "final_score": 0.9782880544662476
      },
      "oral": {
        "paper": {
          "id": "QWunLKbBGF",
          "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
          "abstract": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and\nproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. \nOur benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular,  in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.",
          "keywords": [
            "personalization",
            "benchmark",
            "Large language models",
            "conversational llm",
            "chatbots"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=QWunLKbBGF",
          "pdf_link": "https://openreview.net/pdf?id=QWunLKbBGF"
        },
        "paper_internal_id": "QWunLKbBGF",
        "category": "oral",
        "embedding_score": 0.7172940373420715,
        "final_score": 0.9847897887229919
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "09FiNmvNMw",
      "title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
      "abstract": "Complex logical reasoning tasks require a long sequence of reasoning, which a large language model (LLM) with chain-of-thought prompting still falls short. To alleviate this issue, neurosymbolic approaches incorporate a symbolic solver. Specifically, an LLM only translates a natural language problem into a satisfiability (SAT) problem that consists of first-order logic formulas, and a sound symbolic solver returns a mathematically correct solution. However, we discover that LLMs have difficulties to capture complex logical semantics hidden in the natural language during translation. To resolve this limitation, we propose a Compositional First-Order Logic Translation. An LLM first parses a natural language sentence into newly defined logical dependency structures that consist of an atomic subsentence and its dependents, then sequentially translate the parsed subsentences. Since multiple logical dependency structures and sequential translations are possible for a single sentence, we also introduce two Verification algorithms to ensure more reliable results. We utilize an SAT solver to rigorously compare semantics of generated first-order logic formulas and select the most probable one. We evaluate the proposed method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it outperforms the previous neurosymbolic approaches and achieves new state-of-the-art results.",
      "keywords": "['Logical Reasoning', 'Large Language Models', 'Neurosymbolic Approaches', 'Semantic Decomposition', 'Formal Language Verification']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "09FiNmvNMw",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "8QTpYC4smR",
          "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
          "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries.",
          "keywords": [
            "Large Language Models",
            "Systematic Review"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=8QTpYC4smR",
          "pdf_link": "https://openreview.net/pdf?id=8QTpYC4smR"
        },
        "paper_internal_id": "8QTpYC4smR",
        "category": "reject",
        "embedding_score": 0.737265944480896,
        "final_score": 0.8242861032485962
      },
      "spotlight": {
        "paper": {
          "id": "jkUp3lybXf",
          "title": "Preference Optimization for Reasoning with Pseudo Feedback",
          "abstract": "Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.\nIn this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \\emph{test cases}. \nWe explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.\nWe conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
          "keywords": [
            "Large Language Model",
            "Code Generation",
            "Natural Language Reasoning",
            "Reinforcement Learning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We develop a framework to continuously improve LLMs for reasoning and code generation through self-consistency-based pseudo feedback.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-17",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=jkUp3lybXf",
          "pdf_link": "https://openreview.net/pdf?id=jkUp3lybXf"
        },
        "paper_internal_id": "jkUp3lybXf",
        "category": "spotlight",
        "embedding_score": 0.7612543106079102,
        "final_score": 0.4145223796367645
      },
      "oral": {
        "paper": {
          "id": "YUYJsHOf3c",
          "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
          "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.",
          "keywords": [
            "LLM",
            "reasoning",
            "generalization",
            "self-improvement"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose ReGenesis, a method to self-synthesize reasoning paths as post-training data of LLMs by progressing from general reasoning structures to task-specific reasoning paths, to improve LLMs' generalization capability in reasoning.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YUYJsHOf3c",
          "pdf_link": "https://openreview.net/pdf?id=YUYJsHOf3c"
        },
        "paper_internal_id": "YUYJsHOf3c",
        "category": "oral",
        "embedding_score": 0.8106182813644409,
        "final_score": 0.3549400866031647
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "N5fVv6PZGz",
      "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
      "abstract": "Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.",
      "keywords": "['NLP in resource-constrained settings', 'inference methods']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "N5fVv6PZGz",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "ob7UrZOJve",
          "title": "Inheritune: Training Smaller Yet More Attentive Language Models",
          "abstract": "Large Language Models (LLMs) have achieved remarkable performance across various natural language processing tasks, primarily due to the transformer architecture and its self-attention mechanism. However, we observe that in standard decoder-style LLMs attention matrices degenerate to single-column for deeper layers. Layers in this state unable to learn anything meaningful and mostly redundant; we refer to these as lazy layers. The goal of this paper is to train smaller models by eliminating this structural inefficiency without compromising performance.\n\nMotivated by this observation, we propose Inheritune, a simple yet effective training recipe for developing smaller, high-performing language models. Smaller models trained with Inheritune inherits early transformer layers from a larger pre-trained model, then retrains and progressively expands the smaller model until it matches or exceeds the performance of the larger model. We demonstrate that Inheritune enables the training of various sizes of GPT-2 models on datasets like OpenWebText-9B and FineWeb\\_Edu. Models trained with Inheritune, despite having significantly fewer layers, match or even surpass the performance of their larger counterparts. For instance, our 16-layer GPT-2 medium variant achieves comparable performance to the standard 24-layer GPT-2 medium model.",
          "keywords": [
            "Large Language Models",
            "Small Language Models",
            "Attention degeneration",
            "Efficient training",
            "Model Initialization"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Attention degeneration reduces the effectiveness of later transformer blocks in deep LLMs. To address this, we create small language models by leveraging only a few blocks from larger LLMs.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=ob7UrZOJve",
          "pdf_link": "https://openreview.net/pdf?id=ob7UrZOJve"
        },
        "paper_internal_id": "ob7UrZOJve",
        "category": "reject",
        "embedding_score": 0.6820360422134399,
        "final_score": 0.9761306643486023
      },
      "spotlight": {
        "paper": {
          "id": "lgsyLSsDRe",
          "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
          "abstract": "Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time.  Also, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB.  We further provide the analysis of model compression techniques for generalist embedding models.  We open-source the model at: https://huggingface.co/nvidia/NV-Embed-v2 .",
          "keywords": [
            "LLM",
            "embedding model",
            "retriever"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=lgsyLSsDRe",
          "pdf_link": "https://openreview.net/pdf?id=lgsyLSsDRe"
        },
        "paper_internal_id": "lgsyLSsDRe",
        "category": "spotlight",
        "embedding_score": 0.6819076538085938,
        "final_score": 0.9601616859436035
      },
      "oral": {
        "paper": {
          "id": "QWunLKbBGF",
          "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
          "abstract": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and\nproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. \nOur benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular,  in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.",
          "keywords": [
            "personalization",
            "benchmark",
            "Large language models",
            "conversational llm",
            "chatbots"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=QWunLKbBGF",
          "pdf_link": "https://openreview.net/pdf?id=QWunLKbBGF"
        },
        "paper_internal_id": "QWunLKbBGF",
        "category": "oral",
        "embedding_score": 0.6638585329055786,
        "final_score": 0.9751601219177246
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "0Xt7uT04cQ",
      "title": "Uni-Sign: Toward Unified Sign Language Understanding at Scale",
      "abstract": "Sign language pre-training has gained increasing attention for its ability to enhance performance across various sign language understanding (SLU) tasks. However, existing methods often suffer from a gap between pre-training and fine-tuning, leading to suboptimal results. To address this, we propose Uni-Sign, a unified pre-training framework that eliminates the gap between pre-training and downstream SLU tasks through a large-scale generative pre-training strategy and a novel fine-tuning paradigm. First, we introduce CSL-News, a large-scale Chinese Sign Language (CSL) dataset containing 1,985 hours of video paired with textual annotations, which enables effective large-scale pre-training. Second, Uni-Sign unifies SLU tasks by treating downstream tasks as a single sign language translation (SLT) task during fine-tuning, ensuring seamless knowledge transfer between pre-training and fine-tuning. Furthermore, we incorporate a prior-guided fusion (PGF) module and a score-aware sampling strategy to efficiently fuse pose and RGB information, addressing keypoint inaccuracies and improving computational efficiency.  Extensive experiments across multiple SLU benchmarks demonstrate that Uni-Sign achieves state-of-the-art performance across multiple downstream SLU tasks. Dataset and code are available at github.com/ZechengLi19/Uni-Sign.",
      "keywords": "['Sign language understanding', 'Pre-training', 'Large-scale sign language dataset']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "0Xt7uT04cQ",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "GsCMKwyfWm",
          "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
          "abstract": "Counting is a fundamental skill for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs’ counting abilities using a divide-and conquer approach, breaking counting problems into sub-counting tasks. Unlike prior methods, which do not generalize well to counting datasets on which they have not been trained, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances counting capabilities across various datasets and benchmarks.",
          "keywords": [
            "Counting",
            "Large vision-language models"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose a method that enhances the counting ability of large vision-language models by dividing the image into subimages through a mechanism that does not bisect the target objects.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=GsCMKwyfWm",
          "pdf_link": "https://openreview.net/pdf?id=GsCMKwyfWm"
        },
        "paper_internal_id": "GsCMKwyfWm",
        "category": "reject",
        "embedding_score": 0.7028090953826904,
        "final_score": 0.9133405089378357
      },
      "spotlight": {
        "paper": {
          "id": "bMC1t7eLRc",
          "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
          "abstract": "Data selection is of great significance in  pretraining large language models, given the  variation in quality within the large-scale available training corpora. \nTo achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores.  However, this approach has several limitations. \n(1) Calculating the accurate influence of all available data is time-consuming.\n(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.\nIn this paper, we introduce $\\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.\nTo compute the influence ($i.e.,$ the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. \nFor the diversity, $\\texttt{Quad}$ clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.  Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that $\\texttt{Quad}$ significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.",
          "keywords": [
            "LLMs",
            "data selection",
            "influence function",
            "diversity"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=bMC1t7eLRc",
          "pdf_link": "https://openreview.net/pdf?id=bMC1t7eLRc"
        },
        "paper_internal_id": "bMC1t7eLRc",
        "category": "spotlight",
        "embedding_score": 0.6749110817909241,
        "final_score": 0.5708171725273132
      },
      "oral": {
        "paper": {
          "id": "SI2hI0frk6",
          "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
          "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.\nTransfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.\nWe pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.\nOur experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.\nBy introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.\nWe further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.",
          "keywords": [
            "multimodal foundation model",
            "multimodal generation and understanding",
            "diffusion",
            "next token prediction"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Transfusion is a recipe for training a multi-modal model over discrete and continuous data.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=SI2hI0frk6",
          "pdf_link": "https://openreview.net/pdf?id=SI2hI0frk6"
        },
        "paper_internal_id": "SI2hI0frk6",
        "category": "oral",
        "embedding_score": 0.7124509215354919,
        "final_score": 0.5240509510040283
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "VipcVxaTnG",
      "title": "Correlation and Navigation in the Vocabulary Key Representation Space of Language Models",
      "abstract": "Language model (LM) decoding is based on the next-token prediction (NTP) probability distribution. For neural LMs (e.g., Transformer-based), NTP distribution is\nessentially a softmax-regularized dot product between an encoded input context\n(query) and fixed vocabulary representations (keys). In this paper, we study the\neffect of the key distribution on the NTP distribution, with a focus on whether\nthe similarity between keys will trigger spurious correlations in NTP. Through\nknowledge-probing tasks, we show that in the NTP distribution, the few top-ranked\ntokens are typically accurate. However, the middle-ranked prediction is highly biased\ntowards the tokens that are distributionally (not necessarily semantically) similar to\nthese top ones. For instance, if “P” is predicted as the top-1 token, “A”-“Z” will all\nbe ranked high in NTP, no matter whether they can lead to correct decoding results.\nThis hurts the sampling diversity and makes the sampling of correct, long-tail\nresults hopeless and noisy. We attempt to alleviate this issue via a novel in-context\nmethod that iteratively pushes the query representation away from explored regions.\nSpecifically, we include the explored decoding results in the context and prompt\nthe LM to generate something else, which encourages the LM to produce a query\nrepresentation that has small dot products with explored keys. Experiments on\nknowledge-probing tasks show that our method leads to efficient navigation away\nfrom explored keys to correct new keys. We further extend our method to open-ended and chain-of-thought (for reasoning) generation. Experiment results show\nthat ICN contributes to better generation diversity and improved self-consistency\nvoting performance. Finally, we discuss potential training issues caused by the\nfixed key space together with the challenges and possible ways to address them in\nfuture research.",
      "keywords": "['Language Modeling', 'Next Token Prediction', 'Spurious Correlation', 'Generation Diversity']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "VipcVxaTnG",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "bFYST1MaGh",
          "title": "Communicating Activations Between Language Model Agents",
          "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via *activations*; concretely, we pause an LM $B$'s computation at an intermediate layer, combine its current activation with another LM $A$'s intermediate activation via some function $f$, then pass $f$'s output into the next layer of $B$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with *zero* additional parameters and data, and saves a *substantial amount of compute* over natural language communication. We test our method with various functional forms $f$ on two experimental setups—multi-player coordination games and reasoning benchmarks—and find that it achieves up to $27.0$% improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative \"language\" for communication between LMs.",
          "keywords": [
            "large language models",
            "multiagent communication",
            "embedding representation",
            "multiagent debate"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We propose a simple technique whereby language models communicate via activations.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=bFYST1MaGh",
          "pdf_link": "https://openreview.net/pdf?id=bFYST1MaGh"
        },
        "paper_internal_id": "bFYST1MaGh",
        "category": "reject",
        "embedding_score": 0.7295273542404175,
        "final_score": 0.9491833448410034
      },
      "spotlight": {
        "paper": {
          "id": "2c7pfOqu9k",
          "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
          "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose **Flattened Tree KV Splitting**, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation, DeFT achieves up to 2.23/3.59$\\times$ speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
          "keywords": [
            "LLM inference",
            "attention",
            "memory-efficiency",
            "tree-based decoding"
          ],
          "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
          "TLDR": "We propose DeFT, a  hardware-efficient tree attention algorithm to improve the tree-based decoding (e.g. multi-step reasoning, speculative decoding, etc) efficiency with IO-awareness for shared prefixes and load-balancing..",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=2c7pfOqu9k",
          "pdf_link": "https://openreview.net/pdf?id=2c7pfOqu9k"
        },
        "paper_internal_id": "2c7pfOqu9k",
        "category": "spotlight",
        "embedding_score": 0.7456060647964478,
        "final_score": 0.9459241032600403
      },
      "oral": {
        "paper": {
          "id": "mtSSFiqW6y",
          "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
          "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
          "keywords": [
            "LLM inference",
            "speculative decoding"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mtSSFiqW6y",
          "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y"
        },
        "paper_internal_id": "mtSSFiqW6y",
        "category": "oral",
        "embedding_score": 0.7683910131454468,
        "final_score": 0.9694504141807556
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "jVDPq9EdzT",
      "title": "UniDrive: Towards Universal Driving Perception Across Camera Configurations",
      "abstract": "Vision-centric autonomous driving has demonstrated excellent performance with economical sensors. As the fundamental step, 3D perception aims to infer 3D information from 2D images based on 3D-2D projection. This makes driving perception models susceptible to sensor configuration (e.g., camera intrinsics and extrinsics) variations. However, generalizing across camera configurations is important for deploying autonomous driving models on different car models. In this paper, we present UniDrive, a novel framework for vision-centric autonomous driving to achieve universal perception across camera configurations. We deploy a set of unified virtual cameras and propose a ground-aware projection method to effectively transform the original images into these unified virtual views. We further propose a virtual configuration optimization method by minimizing the expected projection error between original and virtual cameras. The proposed virtual camera projection can be applied to existing 3D perception methods as a plug-and-play module to mitigate the challenges posed by camera parameter variability, resulting in more adaptable and reliable driving perception models. To evaluate the effectiveness of our framework, we collect a dataset on CARLA by driving the same routes while only modifying the camera configurations. Experimental results demonstrate that our method trained on one specific camera configuration can generalize to varying configurations with minor performance degradation.",
      "keywords": "['Autonomous Driving', '3D Detection', 'Sensor Configuration']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "jVDPq9EdzT",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "SEvJfuCtPY",
          "title": "Phase-aware Training Schedule Simplifies Learning in Flow-Based Generative Models",
          "abstract": "We analyze the training of a two-layer autoencoder used to parameterize a flow-based generative model for sampling from a high-dimensional Gaussian mixture. Building on the work of Cui et al. (2024), we find that the phase where the high-level features are learnt during training disappears as the dimension goes to infinity without an appropriate time schedule. We introduce a time dilation that solves this problem. This enables us to characterize the learnt velocity field, finding a first phase where the high-level feature (asymmetry between modes) is learnt and a second phase where the low-level feature (distribution of each mode) is learnt. We find that the autoencoder representing the velocity field learns to simplify by estimating only the parameters relevant to the feature for each phase. Turning to real data, we propose a method that, for a given feature, finds intervals of time where training improves accuracy the most on that feature, and we provide an experiment on MNIST validating this approach.",
          "keywords": [
            "diffusion models",
            "phase transitions",
            "flow-based generative model",
            "high-dimensional gaussian mixtures",
            "denoising autoencoders",
            "training schedules"
          ],
          "primary_area": "generative models",
          "TLDR": "We introduce a time-dilated training schedule for flow-based generative models that allows the learning of high-level features in high-dimensional settings by overcoming gradient vanishing and enabling phase-specific parameter learning.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=SEvJfuCtPY",
          "pdf_link": "https://openreview.net/pdf?id=SEvJfuCtPY"
        },
        "paper_internal_id": "SEvJfuCtPY",
        "category": "reject",
        "embedding_score": 0.6586215496063232,
        "final_score": 0.5238301753997803
      },
      "spotlight": {
        "paper": {
          "id": "BL4WBIfyrz",
          "title": "Lightweight Neural App Control",
          "abstract": "This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC  takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution.  We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
          "keywords": [
            "vision-language model",
            "multi-modal",
            "android control",
            "app agent"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=BL4WBIfyrz",
          "pdf_link": "https://openreview.net/pdf?id=BL4WBIfyrz"
        },
        "paper_internal_id": "BL4WBIfyrz",
        "category": "spotlight",
        "embedding_score": 0.6531404256820679,
        "final_score": 0.5034826397895813
      },
      "oral": {
        "paper": {
          "id": "xDrFWUmCne",
          "title": "Learning to Discretize Denoising Diffusion ODEs",
          "abstract": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at https://github.com/vinhsuhi/LD3.",
          "keywords": [
            "Diffusion models",
            "Efficient Sampling",
            "Ordinary Differentiable Equations"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=xDrFWUmCne",
          "pdf_link": "https://openreview.net/pdf?id=xDrFWUmCne"
        },
        "paper_internal_id": "xDrFWUmCne",
        "category": "oral",
        "embedding_score": 0.6922323107719421,
        "final_score": 0.37325695157051086
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Extracting Symbolic Sequences from Visual Representations via Self-Supervised Learning",
      "abstract": "In this paper, we explore the potential of abstracting complex visual information into discrete, structured symbolic sequences using self-supervised learning (SSL). Inspired by how language abstracts and organizes information to enable better reasoning and generalization, we propose a novel approach for generating symbolic representations from visual data. To learn these sequences, we extend the DINO framework to handle both visual and symbolic information. Initial experiments suggest that the generated symbolic sequences capture a meaningful level of abstraction, though further refinement is required. An advantage of our method is its interpretability: the sequences are produced by a decoder transformer using cross-attention, allowing attention maps to be linked to specific symbols and offering insight into how these representations correspond to image regions. This approach lays the foundation for creating interpretable symbolic representations with potential applications in high-level scene understanding.",
      "keywords": [
        "Self-Supervised Learning",
        "Symbolic Representations",
        "Information Theory",
        "Knowledge Distillation",
        "Visual Abstraction",
        "Interpretability"
      ],
      "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
      "TLDR": "",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-05",
      "venue": "Submitted to ICLR 2025",
      "forum_link": "https://openreview.net/forum?id=MsUhByb3CM",
      "pdf_link": "https://openreview.net/pdf?id=MsUhByb3CM",
      "label": "reject",
      "conference": "ICLR",
      "paper_id": "MsUhByb3CM"
    },
    "query_internal_id": "MsUhByb3CM",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "md9qolJwLl",
          "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models",
          "abstract": "Pretrained masked language models (MLMs) have demonstrated an impressive capability to comprehend and encode conceptual knowledge, revealing a lattice structure among concepts. This raises a critical question: how does this conceptualization emerge from MLM pretraining? In this paper, we explore this problem from the perspective of Formal Concept Analysis (FCA), a mathematical framework that derives concept lattices from the observations of object-attribute relationships. We show that the MLM's objective implicitly learns a formal context that describes objects, attributes, and their dependencies, which enables the reconstruction of a concept lattice through FCA. We propose a novel framework for concept lattice construction from pretrained MLMs and investigate the origin of the inductive biases of MLMs in lattice structure learning. Our framework differs from previous work because it does not rely on human-defined concepts and allows for discovering \"latent\" concepts that extend beyond human definitions. We create three datasets for evaluation, and the empirical results verify our hypothesis.",
          "keywords": [
            "Masked Language Models",
            "Formal Concept Analysis",
            "Interpretability"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We investigate the conceptualization of language models from the pespective of formal concept analysis and discuss the inductive bias of  masked language models in lattice structure learning.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=md9qolJwLl",
          "pdf_link": "https://openreview.net/pdf?id=md9qolJwLl"
        },
        "paper_internal_id": "md9qolJwLl",
        "category": "poster",
        "embedding_score": 0.7435940504074097,
        "final_score": 0.6013675332069397
      },
      "spotlight": {
        "paper": {
          "id": "qtWjSboqfe",
          "title": "DEEM: Diffusion models serve as the eyes of large language models for image perception",
          "abstract": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's  visual perception performance to a large extent (e.g., 4\\% ↑ on RobustVQA, 6.5\\% ↑ on MMVP and 12.8 \\% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM  exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis.",
          "keywords": [
            "MLLM; Diffusion Model;"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "Diffusion Model Can help MLLM see better",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-21",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=qtWjSboqfe",
          "pdf_link": "https://openreview.net/pdf?id=qtWjSboqfe"
        },
        "paper_internal_id": "qtWjSboqfe",
        "category": "spotlight",
        "embedding_score": 0.737234354019165,
        "final_score": 0.11823701858520508
      },
      "oral": {
        "paper": {
          "id": "YUYJsHOf3c",
          "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
          "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.",
          "keywords": [
            "LLM",
            "reasoning",
            "generalization",
            "self-improvement"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose ReGenesis, a method to self-synthesize reasoning paths as post-training data of LLMs by progressing from general reasoning structures to task-specific reasoning paths, to improve LLMs' generalization capability in reasoning.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YUYJsHOf3c",
          "pdf_link": "https://openreview.net/pdf?id=YUYJsHOf3c"
        },
        "paper_internal_id": "YUYJsHOf3c",
        "category": "oral",
        "embedding_score": 0.7173325419425964,
        "final_score": 0.29947802424430847
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Inference of Evolving Mental States from Irregular Action Events to Understand Human Behaviors",
      "abstract": "Inference of latent human mental processes, such as belief, intention, or desire, is crucial for developing AI with human-like intelligence, enabling more effective and timely collaboration. In this paper, we introduce a versatile encoder-decoder model designed to infer  evolving mental processes based on irregularly observed action events and predict future occurrences. The primary challenges arise from two factors: both actions and mental processes are irregular events, and the observed action data is often limited. To address the irregularity of these events, we leverage a temporal point process model within the encoder-decoder framework, effectively capturing the dynamics of both action and mental events. Additionally, we implement a backtracking mechanism in the decoder to enhance the accuracy of predicting future actions and evolving mental states. To tackle the issue of limited data, our model incorporates logic rules as priors, enabling accurate inferences from just a few observed samples. These logic rules can be refined and updated as needed, providing flexibility to the model. Overall, our approach enhances the understanding of human behavior by predicting when actions will occur and how mental processes evolve. Experiments on both synthetic and real-world datasets demonstrate the strong performance of our model in inferring mental states and predicting future actions, contributing to the development of more human-centric AI systems.",
      "keywords": [
        "temporal point process",
        "logic rule",
        "human-AI collaboration"
      ],
      "primary_area": "interpretability and explainable AI",
      "TLDR": "",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-05",
      "venue": "Submitted to ICLR 2025",
      "forum_link": "https://openreview.net/forum?id=YSA0QeYnDd",
      "pdf_link": "https://openreview.net/pdf?id=YSA0QeYnDd",
      "label": "reject",
      "conference": "ICLR",
      "paper_id": "YSA0QeYnDd"
    },
    "query_internal_id": "YSA0QeYnDd",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "S9GyQUXzee",
          "title": "GROOT-2: Weakly Supervised Multimodal Instruction Following Agents",
          "abstract": "Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. \nTo address this issue, we frame the problem as a semi-supervised learning task and introduce \\agent, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. \\agent’s effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.",
          "keywords": [
            "Reinforcement Learning",
            "Open-world Agent",
            "Weakly Supervised Learning",
            "Goal-Conditioned Policy"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "",
          "creation_date": "2024-09-19",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=S9GyQUXzee",
          "pdf_link": "https://openreview.net/pdf?id=S9GyQUXzee"
        },
        "paper_internal_id": "S9GyQUXzee",
        "category": "poster",
        "embedding_score": 0.7187448740005493,
        "final_score": 0.3685760498046875
      },
      "spotlight": {
        "paper": {
          "id": "2iCIHgE8KG",
          "title": "Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA",
          "abstract": "Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at \\textit{each} time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the infinite GPFA model correctly infers dynamically changing activations of latent factors on a synthetic dataset. By fitting the infinite GPFA model to population activities of hippocampal place cells during spatial tasks with alternating random foraging and spatial memory phases, we identify novel non-trivial and behaviourally meaningful dynamics in the neural encoding process.",
          "keywords": [
            "Computational neuroscience",
            "neural data analysis",
            "Bayesian nonparametrics",
            "latent variable modelling;"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We propose a fully Bayesian nonparametric extension of GPFA that enables discovery of temporally compositional neural manifolds underlying high-dimensional population neuronal activities.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-26",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=2iCIHgE8KG",
          "pdf_link": "https://openreview.net/pdf?id=2iCIHgE8KG"
        },
        "paper_internal_id": "2iCIHgE8KG",
        "category": "spotlight",
        "embedding_score": 0.7483859062194824,
        "final_score": 0.07296807318925858
      },
      "oral": {
        "paper": {
          "id": "2efNHgYRvM",
          "title": "On the Identification of Temporal Causal Representation with Instantaneous Dependence",
          "abstract": "Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.",
          "keywords": [
            "Causal Representation Learning",
            "Instantaneous Dependency",
            "Identification"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-05",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=2efNHgYRvM",
          "pdf_link": "https://openreview.net/pdf?id=2efNHgYRvM"
        },
        "paper_internal_id": "2efNHgYRvM",
        "category": "oral",
        "embedding_score": 0.7367122769355774,
        "final_score": 0.1268538236618042
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "A New Look at Low-Rank Recurrent Neural Networks",
      "abstract": "Low-rank recurrent neural networks (RNNs) have recently gained prominence as a framework for understanding how neural systems solve complex cognitive tasks. However, fitting and interpreting these networks remains an important open problem.\nHere we address this challenge using a perspective from the ``neural engineering framework'', which shows how to embed an arbitrary ordinary differential equation (ODE) into a low-rank RNN using least-squares regression. Under this perspective, individual neurons in a low-rank RNN provide nonlinear basis functions for representing an ODE of interest. This clarifies limits on the expressivity of low-rank RNNs, such as the fact that with a $\\tanh$ non-linearity they can only capture odd-symmetric functions in the absence of per neuron inputs or biases. Building on this framework, we propose a method for finding the smallest low-rank RNN to implement a given dynamical system using a variant of orthogonal matching pursuit. We also show how to use regression-based fitting to obtain low-rank RNNs with time-varying dynamics. This allows for the rapid training of vastly different dynamical systems that nevertheless produce a given time-varying trajectory. Finally, we highlight the usefulness of our framework by comparing to RNNs trained using backprop-through-time on neuroscience-inspired tasks, showing that our method achieves faster and more accurate learning with smaller networks than gradient-based training.",
      "keywords": [
        "low-rank rnn",
        "computational neuroscience",
        "dynamical systems",
        "neural dynamics"
      ],
      "primary_area": "applications to neuroscience & cognitive science",
      "TLDR": "We introduce an approach to train low-rank RNNs to implement a desired dynamical system, clarify limits on expressivity of such models, and offer insights into how input-driven RNNs produce trajectories observed in neuroscience tasks.",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-05",
      "venue": "Submitted to ICLR 2025",
      "forum_link": "https://openreview.net/forum?id=fWXYD0ZCdd",
      "pdf_link": "https://openreview.net/pdf?id=fWXYD0ZCdd",
      "label": "reject",
      "conference": "ICLR",
      "paper_id": "fWXYD0ZCdd"
    },
    "query_internal_id": "fWXYD0ZCdd",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "1yJP5TVWih",
          "title": "Lambda-Skip Connections: the architectural component that prevents Rank Collapse",
          "abstract": "Rank collapse, a phenomenon where embedding vectors in sequence models\nrapidly converge to a uniform token or equilibrium state, has recently gained at-\ntention in the deep learning literature. This phenomenon leads to reduced expres-\nsivity and potential training instabilities due to vanishing gradients. Empirical ev-\nidence suggests that architectural components like skip connections, LayerNorm,\nand MultiLayer Perceptrons (MLPs) play critical roles in mitigating rank collapse.\nWhile this issue is well-documented for transformers, alternative sequence mod-\nels, such as State Space Models (SSMs), which have recently gained prominence,\nhave not been thoroughly examined for similar vulnerabilities. This paper extends\nthe theory of rank collapse from transformers to SSMs using a unifying frame-\nwork that captures both architectures. We introduce a modification in the skip\nconnection component, termed lambda-skip connections, that provides guaran-\ntees for rank collapse prevention. We present, via analytical results, a sufficient\ncondition to achieve the guarantee for all of the aforementioned architectures. We\nalso study the necessity of this condition via ablation studies and analytical exam-\nples. To our knowledge, this is the first study that provides a general guarantee to\nprevent rank collapse, and that investigates rank collapse in the context of SSMs,\noffering valuable understanding for both theoreticians and practitioners. Finally,\nwe validate our findings with experiments demonstrating the crucial role of archi-\ntectural components in preventing rank collapse.",
          "keywords": [
            "Rank Collapse",
            "Skip Connections",
            "Sequence Modeling Architectures"
          ],
          "primary_area": "learning theory",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-13",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=1yJP5TVWih",
          "pdf_link": "https://openreview.net/pdf?id=1yJP5TVWih"
        },
        "paper_internal_id": "1yJP5TVWih",
        "category": "poster",
        "embedding_score": 0.7388298511505127,
        "final_score": 0.971200704574585
      },
      "spotlight": {
        "paper": {
          "id": "UvfI4grcM7",
          "title": "Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics",
          "abstract": "The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks.\nCritically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd.",
          "keywords": [
            "Barrel cortex",
            "biophysical modeling",
            "sensory-motor integration",
            "recurrent spiking neural networks"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "Training a biologically constrained barrel cortex model and exploring its biological interpretability.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=UvfI4grcM7",
          "pdf_link": "https://openreview.net/pdf?id=UvfI4grcM7"
        },
        "paper_internal_id": "UvfI4grcM7",
        "category": "spotlight",
        "embedding_score": 0.7537448406219482,
        "final_score": 0.6936991214752197
      },
      "oral": {
        "paper": {
          "id": "UvTo3tVBk2",
          "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
          "abstract": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences. However, both Transformers and LRNNs struggle to perform state-tracking, which may impair performance in tasks such as code evaluation. In one forward pass, current architectures are unable to solve even parity, the simplest state-tracking task, which non-linear RNNs can handle effectively. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to $[0, 1]$ and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while non-triangular matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. We also show that state-tracking enabled LRNNs can be pretrained  stably and efficiently at scale (1.3B parameters), achieving competitive performance on language modeling and showing promise on code and math tasks.",
          "keywords": [
            "State Tracking",
            "State Space",
            "Mamba",
            "Linear RNN",
            "Linear Attention",
            "GLA",
            "DeltaNet",
            "Formal Languages",
            "Products of Householders"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We show that expanding the eigenvalue range of Linear RNN from [0, 1] to [-1,1] enhances their state-tracking capabilities, enabling them to solve complex tasks like parity and modular counting, while preserving their efficiency in language modeling.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-14",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=UvTo3tVBk2",
          "pdf_link": "https://openreview.net/pdf?id=UvTo3tVBk2"
        },
        "paper_internal_id": "UvTo3tVBk2",
        "category": "oral",
        "embedding_score": 0.7651464939117432,
        "final_score": 0.8054280281066895
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Advancing Algorithmic Trading with Large Language Models: A Reinforcement Learning Approach for Stock Market Optimization",
      "abstract": "In the fast-evolving landscape of financial markets, effective decision-making tools are essential for managing complexities driven by economic indicators and market dynamics. Algorithmic trading strategies have gained prominence for their ability to execute trades autonomously, with Deep Reinforcement Learning (DRL) emerging as a key approach for optimizing trading actions through continuous market interaction. However, RL-based systems face significant challenges, particularly in adapting to evolving time series data and incorporating unstructured textual information. In response to these limitations, recent advancements in Large Language Models (LLMs) offer new opportunities. LLMs possess the capacity to analyze vast volumes of data, providing enhanced insights that can complement traditional market analysis. This study proposes a novel approach that integrates six distinct LLMs into algorithmic trading frameworks, developing Stock-Evol-Instruct, an innovative instruction generation algorithm. This algorithm enables RL agents to fine-tune their trading strategies by leveraging LLM-driven insights for daily stock trading decisions. Empirical evaluation using real-world stock data from Silver and JPMorgan demonstrates the significant potential of this approach to outperform conventional trading models. By bridging the gap between LLMs and RL in algorithmic trading, this study contributes to a new frontier in financial technology, setting the stage for future advancements in autonomous trading systems.",
      "keywords": [
        "Algorithmic trading",
        "Stock market",
        "Large language models",
        "Deep reinforcement learning"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "A novel approach to algorithmic trading by integrating LLMs with Deep RL, for optimizing stock market trading strategies through the development of the Stock-Evol-Instruct algorithm.",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-05",
      "venue": "Submitted to ICLR 2025",
      "forum_link": "https://openreview.net/forum?id=w7BGq6ozOL",
      "pdf_link": "https://openreview.net/pdf?id=w7BGq6ozOL",
      "label": "reject",
      "conference": "ICLR",
      "paper_id": "w7BGq6ozOL"
    },
    "query_internal_id": "w7BGq6ozOL",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "Yqk7EyT52H",
          "title": "MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model",
          "abstract": "Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains under-explored. In financial markets, generative models can simulate complex market effects of participants with various behaviors, enabling interaction under different market conditions, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the domain-specific need for realistic, interactive and controllable order generation. Key observations include LMM's strong scalability across data size and model complexity, and MarS's robust and practicable realism in controlled generation with market impact. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment, thus demonstrating MarS's ``paradigm shift'' potential for a variety of financial applications. We release the code of MarS at https://github.com/microsoft/MarS/.",
          "keywords": [
            "Financial Market Simulation",
            "Generative Foundation Model",
            "Large Market Model (LMM)",
            "Controllable Simulation",
            "Interactive Simulation",
            "Market Impact",
            "Reinforcement Learning",
            "Forecasting",
            "Market Manipulation Detection",
            "Order-Level Data"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-15",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=Yqk7EyT52H",
          "pdf_link": "https://openreview.net/pdf?id=Yqk7EyT52H"
        },
        "paper_internal_id": "Yqk7EyT52H",
        "category": "poster",
        "embedding_score": 0.8062750101089478,
        "final_score": 0.09670375287532806
      },
      "spotlight": {
        "paper": {
          "id": "Acvo2RGSCy",
          "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
          "abstract": "The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling *inference-time reasoning*, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.",
          "keywords": [
            "large language models",
            "decision theory",
            "decision making under uncertainty"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We introduce an inference-time reasoning procedure for reliable decision making under uncertainty with LLMs, drawing upon principles from classical decision theory.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=Acvo2RGSCy",
          "pdf_link": "https://openreview.net/pdf?id=Acvo2RGSCy"
        },
        "paper_internal_id": "Acvo2RGSCy",
        "category": "spotlight",
        "embedding_score": 0.7383512258529663,
        "final_score": 0.07741992175579071
      },
      "oral": {
        "paper": {
          "id": "syThiTmWWm",
          "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
          "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **\"null model\"** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an $86.5\\\\%$ LC win rate on AlpacaEval 2.0; an $83.0$ score on Arena-Hard-Auto; and a $9.55$ score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., $805$ samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",
          "keywords": [
            "Large Language Models",
            "Cheating",
            "Automatic LLM Benchmarks"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We show that null models that always return the same cheating responses can achieve high win rates on automatic LLM benchmarks.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-13",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=syThiTmWWm",
          "pdf_link": "https://openreview.net/pdf?id=syThiTmWWm"
        },
        "paper_internal_id": "syThiTmWWm",
        "category": "oral",
        "embedding_score": 0.7124567627906799,
        "final_score": 0.0367603674530983
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "UniRestore3D: A Scalable Framework For General Shape Restoration",
      "abstract": "Shape restoration aims to recover intact 3D shapes from defective ones, such as those that are incomplete, noisy, and low-resolution. Previous works have achieved impressive results in shape restoration subtasks thanks to advanced generative models. While effective for specific shape defects, they are less applicable in real-world scenarios involving multiple defect types simultaneously. Additionally, training on limited subsets of defective shapes hinders knowledge transfer across restoration types and thus affects generalization. In this paper, we address the task of general shape restoration, which restores shapes with various types of defects through a unified model, thereby naturally improving the applicability and scalability. Our approach first standardizes the data representation across different restoration subtasks using high-resolution TSDF grids and constructs a large-scale dataset with diverse types of shape defects. Next, we design an efficient hierarchical shape generation model and a noise-robust defective shape encoder that enables effective impaired shape understanding and intact shape generation. Moreover, we propose a scalable training strategy for efficient model training. The capabilities of our proposed method are demonstrated across multiple shape restoration subtasks and validated on various datasets, including Objaverse, ShapeNet, GSO, and ABO.",
      "keywords": [
        "Shape Restoration",
        "3D Reconstruction",
        "Diffusion Model"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "A unified shape generative model with a scalable training strategy for restoring various forms of defective shapes.",
      "creation_date": "2024-09-23",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-14",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=xPO6fwvldG",
      "pdf_link": "https://openreview.net/pdf?id=xPO6fwvldG",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "xPO6fwvldG"
    },
    "query_internal_id": "xPO6fwvldG",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "BoRmf8wDZ7",
          "title": "Gaussian Masked Autoencoders",
          "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While mainstream self-supervised learning frameworks such as MAE operate on low-level pixels, the image synthesis community has evolved to use latent, mid-level representations for better generative visual data modeling. Our approach, named GMAE, aims to reconcile these two and get the benefits of both worlds. Like MAE, it reconstructs the image end-to-end in the pixel space; however, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities (e.g figure-ground segmentation, image layering, edge detection, etc) while preserving the high self-supervised representation quality from MAE. Notably, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data.",
          "keywords": [
            "Representation learning",
            "Gaussian Splatting"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=BoRmf8wDZ7",
          "pdf_link": "https://openreview.net/pdf?id=BoRmf8wDZ7"
        },
        "paper_internal_id": "BoRmf8wDZ7",
        "category": "reject",
        "embedding_score": 0.6808738112449646,
        "final_score": 0.9690115451812744
      },
      "spotlight": {
        "paper": {
          "id": "wXSshrxlP4",
          "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
          "abstract": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to group 3D\npoints as objects, existing unsupervised methods are usually limited to identifying\nsimple objects like cars or their segmented objects are often inferior due to the\nlack of objectness in pretrained features. In this paper, we propose a new two-\nstage pipeline called GrabS. The core concept of our method is to learn generative\nand discriminative object-centric priors as a foundation from object datasets in the\nfirst stage, and then design an embodied agent to learn to discover multiple ob-\njects by querying against the pretrained generative priors in the second stage. We\nextensively evaluate our method on two real-world datasets and a newly created\nsynthetic dataset, demonstrating remarkable segmentation performance, clearly\nsurpassing all existing unsupervised methods.",
          "keywords": [
            "3D scene object segmentation",
            "unsupervised learning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=wXSshrxlP4",
          "pdf_link": "https://openreview.net/pdf?id=wXSshrxlP4"
        },
        "paper_internal_id": "wXSshrxlP4",
        "category": "spotlight",
        "embedding_score": 0.6518657803535461,
        "final_score": 0.9841693639755249
      },
      "oral": {
        "paper": {
          "id": "8enWnd6Gp3",
          "title": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes",
          "abstract": "We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation.",
          "keywords": [
            "geometry representation",
            "3D modeling"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-20",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=8enWnd6Gp3",
          "pdf_link": "https://openreview.net/pdf?id=8enWnd6Gp3"
        },
        "paper_internal_id": "8enWnd6Gp3",
        "category": "oral",
        "embedding_score": 0.7291016578674316,
        "final_score": 0.9799404740333557
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
      "abstract": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup. Our code and checkpoints are available at https://www.github.com/alanbaade/SyllableLM",
      "keywords": [
        "Generative Spoken Language Modeling",
        "Audio",
        "Textless NLP",
        "Representation Learning"
      ],
      "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
      "TLDR": "We introduce a new self-supervised method to learn semantic speech units (pseudo-syllables) that dramatically lowers bitrate and improves spoken language modeling compared to prior work.",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-01",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=dGSOn7sdWg",
      "pdf_link": "https://openreview.net/pdf?id=dGSOn7sdWg",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "dGSOn7sdWg"
    },
    "query_internal_id": "dGSOn7sdWg",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "OW332Wh9S5",
          "title": "DC-Spin: A Speaker-invariant Speech Tokenizer For Spoken Language Models",
          "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
          "keywords": [
            "speech tokenizer",
            "self-supervised learning",
            "spoken language model",
            "speech language model",
            "speech resynthesis",
            "audio codec"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "DC-Spin extracts speaker-invariant tokens to improve spoken language models and speech resynthesis, and analyses offer insights for designing better speech tokenizers.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=OW332Wh9S5",
          "pdf_link": "https://openreview.net/pdf?id=OW332Wh9S5"
        },
        "paper_internal_id": "OW332Wh9S5",
        "category": "reject",
        "embedding_score": 0.7989768981933594,
        "final_score": 0.9564034342765808
      },
      "spotlight": {
        "paper": {
          "id": "oQ4igHyh3N",
          "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
          "abstract": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at {\\color{red}\\url{https://github.com/Haiyang-W/TokenFormer.git}}",
          "keywords": [
            "Fully Attention-based Neural Network",
            "Large Language Model",
            "Model Scaling",
            "Tokenized Model Parameters"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Designing a fully attention-based neural network for efficient model scaling through treating model parameters as tokens.",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-24",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=oQ4igHyh3N",
          "pdf_link": "https://openreview.net/pdf?id=oQ4igHyh3N"
        },
        "paper_internal_id": "oQ4igHyh3N",
        "category": "spotlight",
        "embedding_score": 0.7012676000595093,
        "final_score": 0.9399078488349915
      },
      "oral": {
        "paper": {
          "id": "mtSSFiqW6y",
          "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
          "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
          "keywords": [
            "LLM inference",
            "speculative decoding"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mtSSFiqW6y",
          "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y"
        },
        "paper_internal_id": "mtSSFiqW6y",
        "category": "oral",
        "embedding_score": 0.7387912273406982,
        "final_score": 0.8167599439620972
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
      "abstract": "Transformers have found extensive applications across various domains due to their powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the **optimal approximation rate**, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates.  Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
      "keywords": [
        "activation function",
        "transformer",
        "pre-training",
        "large language models"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-20",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=CbpWPbYHuv",
      "pdf_link": "https://openreview.net/pdf?id=CbpWPbYHuv",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "CbpWPbYHuv"
    },
    "query_internal_id": "CbpWPbYHuv",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "cnecLUNs6w",
          "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression",
          "abstract": "Transformers have demonstrated remarkable in-context learning capabilities across various domains, including statistical learning tasks. While previous work has shown that transformers can implement common learning algorithms, the adversarial robustness of these learned algorithms remains unexplored. This work investigates the vulnerability of in-context learning in transformers to _hijacking attacks_ focusing on the setting of linear regression tasks. Hijacking attacks are prompt-manipulation attacks in which the adversary's goal is to manipulate the prompt to force the transformer to generate a specific output. We first prove that single-layer linear transformers, known to implement gradient descent in-context, are non-robust and can be manipulated to output arbitrary predictions by perturbing\na single example in the in-context training set. While our experiments show these attacks succeed on linear transformers, we find they do not transfer to more complex transformers with GPT-2 architectures. Nonetheless, we show that these transformers can be hijacked using gradient-based adversarial attacks. We then demonstrate that adversarial training enhances transformers' robustness against hijacking attacks, even when just applied during finetuning.  Additionally, we find that in some settings, adversarial training against a weaker attack model can lead to robustness to a stronger attack model.  Lastly, we investigate the transferability of hijacking attacks across transformers of varying scales and initialization seeds, as well as between transformers and ordinary least squares (OLS). We find that while attacks transfer effectively between small-scale transformers, they show poor transferability in other scenarios (small-to-large scale, large-to-large scale, and between transformers and OLS).",
          "keywords": [
            "in-context learning",
            "transformers",
            "hijacking attacks",
            "linear regression",
            "linear transformers",
            "transfer of adversarial attacks"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-13",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=cnecLUNs6w",
          "pdf_link": "https://openreview.net/pdf?id=cnecLUNs6w"
        },
        "paper_internal_id": "cnecLUNs6w",
        "category": "reject",
        "embedding_score": 0.7202917337417603,
        "final_score": 0.6143267750740051
      },
      "spotlight": {
        "paper": {
          "id": "oQ4igHyh3N",
          "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
          "abstract": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at {\\color{red}\\url{https://github.com/Haiyang-W/TokenFormer.git}}",
          "keywords": [
            "Fully Attention-based Neural Network",
            "Large Language Model",
            "Model Scaling",
            "Tokenized Model Parameters"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Designing a fully attention-based neural network for efficient model scaling through treating model parameters as tokens.",
          "creation_date": "2024-09-15",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-24",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=oQ4igHyh3N",
          "pdf_link": "https://openreview.net/pdf?id=oQ4igHyh3N"
        },
        "paper_internal_id": "oQ4igHyh3N",
        "category": "spotlight",
        "embedding_score": 0.7662354111671448,
        "final_score": 0.4308234453201294
      },
      "oral": {
        "paper": {
          "id": "FVuqJt3c4L",
          "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
          "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
          "keywords": [
            "representation learning",
            "neuroscience",
            "self supervised learning"
          ],
          "primary_area": "applications to neuroscience & cognitive science",
          "TLDR": "Representation learning of neural data",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-28",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=FVuqJt3c4L",
          "pdf_link": "https://openreview.net/pdf?id=FVuqJt3c4L"
        },
        "paper_internal_id": "FVuqJt3c4L",
        "category": "oral",
        "embedding_score": 0.7247215509414673,
        "final_score": 0.06569036096334457
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction",
      "abstract": "3D facial reconstruction from a single in-the-wild image is a crucial task in human-centered computer vision tasks. While existing methods can recover accurate facial shapes, there remains significant space for improvement in fine-grained expression capture.  Current approaches struggle with irregular mouth shapes, exaggerated expressions, and asymmetrical facial movements. We present TEASER (Token EnhAnced Spatial modeling for Expressions Reconstruction), which addresses these challenges and enhances 3D facial geometry performance⁠⁠. TEASER tackles two main limitations of existing methods: insufficient photometric loss for self-reconstruction and inaccurate localization of subtle expressions. We introduce a multi-scale tokenizer to extract facial appearance information. Combined with a neural renderer, these tokens provide precise geometric guidance for expression reconstruction. Furthermore, TEASER incorporates a pose-dependent landmark loss to further improve geometric performance⁠. Our approach not only significantly enhances expression reconstruction quality but also offers interpretable tokens suitable for various downstream applications, such as photorealistic facial video driving, expression transfer, and identity swapping. Quantitative and qualitative experimental results across multiple datasets demonstrate that TEASER achieves state-of-the-art performance in precise expression reconstruction.",
      "keywords": [
        "Expression reconstruction",
        "Hybrid parameters"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "TEASER reconstructs precise 3D facial expression and generates high-fidelity face image through estimating hybrid parameters for 3D facial reconstruction.",
      "creation_date": "2024-09-18",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=pTeOOKnjGM",
      "pdf_link": "https://openreview.net/pdf?id=pTeOOKnjGM",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "pTeOOKnjGM"
    },
    "query_internal_id": "pTeOOKnjGM",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "VAvZ4oinpa",
          "title": "Video Generation with Learned Action Prior",
          "abstract": "Long-term stochastic video generation remains challenging, especially with moving cameras. This scenario introduces complex interactions between camera movement and observed pixels, resulting in intricate spatio-temporal dynamics and partial observability issues. Current approaches often focus on pixel-level image reconstruction, neglecting explicit modeling of camera motion dynamics. Our proposed solution incorporates camera motion or action as an extended part of the observed image state, employing a multi-modal learning framework to simultaneously model both image and action. We introduce three models: (i) Video Generation with Learning Action Prior (VG-LeAP) that treats the image-action pair as an augmented state generated from a single latent stochastic process and uses variational inference to learn the image-action latent prior; (ii) Causal-LeAP, which establishes a causal relationship between action and the observed image frame, and learns a seperate action prior, conditioned on the observed image states along with the image prior; and (iii) RAFI, which integrates the augmented image-action state concept with a conditional flow matching framework, demonstrating that this action-conditioned image generation concept can be extended to other transformer-based architectures. Through comprehensive empirical studies on robotic video dataset, RoAM, we highlight the importance of multi-modal training in addressing partially observable video generation problems.",
          "keywords": [
            "Stochastic Video Generation",
            "Variational Inference"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "We propose variational models for learning action priors for video generation tasks in situations where the camera is also moving like in autonomous cars or robots.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=VAvZ4oinpa",
          "pdf_link": "https://openreview.net/pdf?id=VAvZ4oinpa"
        },
        "paper_internal_id": "VAvZ4oinpa",
        "category": "reject",
        "embedding_score": 0.6600450873374939,
        "final_score": 0.5807457566261292
      },
      "spotlight": {
        "paper": {
          "id": "WKfb1xGXGx",
          "title": "Perm: A Parametric Representation for Multi-Style 3D Hair Modeling",
          "abstract": "We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures, termed guide textures and residual textures, respectively. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of Perm, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation. More details can be found on our project page: https://cs.yale.edu/homes/che/projects/perm/.",
          "keywords": [
            "Hair Modeling",
            "Parametric Models",
            "Generative Models"
          ],
          "primary_area": "generative models",
          "TLDR": "We present a parametric model of 3D human hair.",
          "creation_date": "2024-09-21",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-04",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=WKfb1xGXGx",
          "pdf_link": "https://openreview.net/pdf?id=WKfb1xGXGx"
        },
        "paper_internal_id": "WKfb1xGXGx",
        "category": "spotlight",
        "embedding_score": 0.6844159364700317,
        "final_score": 0.6204475164413452
      },
      "oral": {
        "paper": {
          "id": "8enWnd6Gp3",
          "title": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes",
          "abstract": "We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation.",
          "keywords": [
            "geometry representation",
            "3D modeling"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-20",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=8enWnd6Gp3",
          "pdf_link": "https://openreview.net/pdf?id=8enWnd6Gp3"
        },
        "paper_internal_id": "8enWnd6Gp3",
        "category": "oral",
        "embedding_score": 0.6609033346176147,
        "final_score": 0.7052169442176819
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix",
      "abstract": "Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our empirical results demonstrate the effectiveness of our non-linear pruning approach in maintaining model performance while significantly reducing computational costs, which is beyond the current state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.",
      "keywords": [
        "Weights Pruning",
        "Attention Approximation",
        "Gradient Descent Optimization"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-27",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=sgbI8Pxwie",
      "pdf_link": "https://openreview.net/pdf?id=sgbI8Pxwie",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "sgbI8Pxwie"
    },
    "query_internal_id": "sgbI8Pxwie",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "eks3dGnocX",
          "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis",
          "abstract": "Large language models (LLMs) have shown amazing performance on tasks that require planning and reasoning. Motivated by this, we investigate the internal mechanisms that underpin a network's ability to perform complex logical reasoning. We first construct a synthetic propositional logic problem that serves as a concrete test-bed for network training and evaluation. Crucially, this problem demands nontrivial planning to solve. We perform our study on two fronts. First, we pursue an understanding of precisely how a three-layer transformer, trained from scratch and attains perfect test accuracy, solves this problem. We are able to identify certain \"planning\" and \"reasoning\" circuits in the network that necessitate cooperation between the attention blocks to implement the desired logic. Second, we study how a pretrained LLM, Mistral 7B, solves this problem. Using activation patching, we characterize internal components that are critical in solving our logic problem. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason.",
          "keywords": [
            "Mechanistic Interpretability",
            "Language Models",
            "Transformers",
            "Logical Reasoning",
            "Learned Representations"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We mechanistically analyze how small and large language models solve synthetic propositional logic problems.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=eks3dGnocX",
          "pdf_link": "https://openreview.net/pdf?id=eks3dGnocX"
        },
        "paper_internal_id": "eks3dGnocX",
        "category": "reject",
        "embedding_score": 0.7450246810913086,
        "final_score": 0.9882522225379944
      },
      "spotlight": {
        "paper": {
          "id": "csbf1p8xUq",
          "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale",
          "abstract": "Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and \nwhile some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality responses for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages. We introduce **X-ALMA**, a model designed to ensure top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. After the final stage of training regimen, our proposed **A**daptive **R**ejection **P**reference **O**ptimization (**ARPO**) surpasses existing preference optimization methods in translation tasks.",
          "keywords": [
            "Large Language Model",
            "Machine Translation",
            "Multilingual"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We present X-ALMA, a multilingual machine translation model that prioritizes quality over quantity by delivering top-tier performance across 50 diverse languages, regardless of their resource levels",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=csbf1p8xUq",
          "pdf_link": "https://openreview.net/pdf?id=csbf1p8xUq"
        },
        "paper_internal_id": "csbf1p8xUq",
        "category": "spotlight",
        "embedding_score": 0.7673905491828918,
        "final_score": 0.9161998629570007
      },
      "oral": {
        "paper": {
          "id": "QWunLKbBGF",
          "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
          "abstract": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and\nproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. \nOur benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular,  in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.",
          "keywords": [
            "personalization",
            "benchmark",
            "Large language models",
            "conversational llm",
            "chatbots"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-18",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=QWunLKbBGF",
          "pdf_link": "https://openreview.net/pdf?id=QWunLKbBGF"
        },
        "paper_internal_id": "QWunLKbBGF",
        "category": "oral",
        "embedding_score": 0.7315423488616943,
        "final_score": 0.8920223712921143
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
      "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.",
      "keywords": [
        "large language models",
        "speech interaction",
        "speech-to-speech",
        "speech-language models"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "LLaMA-Omni is a low-latency and high-quality end-to-end speech interaction model, which can simultaneously generate both text and speech responses based on speech instructions.",
      "creation_date": "2024-09-23",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-28",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=PYmrUQmMEw",
      "pdf_link": "https://openreview.net/pdf?id=PYmrUQmMEw",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "PYmrUQmMEw"
    },
    "query_internal_id": "PYmrUQmMEw",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "5iUUorHeM3",
          "title": "CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs",
          "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs’ reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04\\% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45\\% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.",
          "keywords": [
            "Large Language Models (LLMs)",
            "benchmarking",
            "analog circuits",
            "dataset creation",
            "evaluation metrics"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "The CIRCUIT dataset introduces a new benchmark and a unique evaluation method to assess Large Language Models' understanding of analog circuits.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=5iUUorHeM3",
          "pdf_link": "https://openreview.net/pdf?id=5iUUorHeM3"
        },
        "paper_internal_id": "5iUUorHeM3",
        "category": "reject",
        "embedding_score": 0.7206588983535767,
        "final_score": 0.9337776899337769
      },
      "spotlight": {
        "paper": {
          "id": "BL4WBIfyrz",
          "title": "Lightweight Neural App Control",
          "abstract": "This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC  takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution.  We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
          "keywords": [
            "vision-language model",
            "multi-modal",
            "android control",
            "app agent"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=BL4WBIfyrz",
          "pdf_link": "https://openreview.net/pdf?id=BL4WBIfyrz"
        },
        "paper_internal_id": "BL4WBIfyrz",
        "category": "spotlight",
        "embedding_score": 0.687942385673523,
        "final_score": 0.9298402667045593
      },
      "oral": {
        "paper": {
          "id": "mtSSFiqW6y",
          "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
          "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
          "keywords": [
            "LLM inference",
            "speculative decoding"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mtSSFiqW6y",
          "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y"
        },
        "paper_internal_id": "mtSSFiqW6y",
        "category": "oral",
        "embedding_score": 0.72386234998703,
        "final_score": 0.9073816537857056
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Towards a Unified and Verified Understanding of Group-Operation Networks",
      "abstract": "A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.",
      "keywords": [
        "mechanistic interpretability",
        "verification",
        "proof",
        "guarantees",
        "interpretability",
        "equivariance",
        "group theory",
        "representation theory"
      ],
      "primary_area": "interpretability and explainable AI",
      "TLDR": "We investigate how neural networks compute group operations, finding an explanation that unifies those of previous works, then verify this explanation by translating it into a compact proof of model performance.",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-11",
      "venue": "ICLR 2025 Spotlight",
      "forum_link": "https://openreview.net/forum?id=8xxEBAtD7y",
      "pdf_link": "https://openreview.net/pdf?id=8xxEBAtD7y",
      "label": "spotlight",
      "conference": "ICLR",
      "paper_id": "8xxEBAtD7y"
    },
    "query_internal_id": "8xxEBAtD7y",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "sp9irsV1yq",
          "title": "Identifying Sub-networks in Neural Networks via Functionally Similar Representations",
          "abstract": "Mechanistic interpretability aims to provide human-understandable insights into the inner workings of neural network models by examining their internals. Existing approaches typically require significant manual effort and prior knowledge, with strategies tailored to specific tasks. In this work, we take a step toward automating the understanding of the network by investigating the existence of distinct  sub-networks. Specifically, we explore a novel automated and task-agnostic approach based on the notion of functionally similar representations within neural networks, reducing the need for human intervention. \nOur method identifies similar and dissimilar layers in the network, revealing potential sub-components. We achieve this by proposing, for the first time to our knowledge, the use of Gromov-Wasserstein distance, which overcomes challenges posed by varying distributions and dimensionalities across intermediate representations—issues that complicate direct layer-to-layer comparisons.\nThrough experiments on algebraic, language, and vision tasks, we observe the emergence of sub-groups within neural network layers corresponding to functional abstractions. Additionally, we find that different training strategies influence the positioning of these sub-groups. Our approach offers meaningful insights into the behavior of neural networks with minimal human and computational cost.",
          "keywords": [
            "mechanistic interpretability",
            "subnetworks"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We propose a novel approach to identify functionally similar subnetworks within neural networks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=sp9irsV1yq",
          "pdf_link": "https://openreview.net/pdf?id=sp9irsV1yq"
        },
        "paper_internal_id": "sp9irsV1yq",
        "category": "reject",
        "embedding_score": 0.8154499530792236,
        "final_score": 0.5215609669685364
      },
      "poster": {
        "paper": {
          "id": "5IWJBStfU7",
          "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
          "abstract": "As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms embedded within their structures to explain their behavior. This work systematically examines a fundamental question: for a fixed behavior to explain, and under the criteria that MI sets for itself, are we guaranteed a unique explanation? Drawing an analogy with the concept of identifiability in statistics, which ensures the uniqueness of parameters inferred from data under specific modeling assumptions, we speak about the identifiability of explanations produced by MI.\n\nWe identify two broad strategies to produce MI explanations: (i) \"where-then-what\", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) \"what-then-where\", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. \n\nWe systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons small enough to allow a complete enumeration of candidate explanations. Our experiments reveal overwhelming evidence of non-identifiability in all cases: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and a single algorithm can be causally aligned with different subspaces of the network.\n\nWe discuss whether the unicity intuition is necessary. One could adopt a pragmatic stance, requiring explanations only to meet predictive and/or manipulability standards. However, if unicity is considered essential, e.g., to provide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the inner interpretability framework that demands explanations to be validated by multiple complementary criteria. This work aims to contribute constructively to the ongoing effort to formalize what we expect from explanations in AI.",
          "keywords": [
            "AI interpretability",
            "mechanistic interpretability",
            "causal consistency",
            "explanatory algorithms",
            "circuits"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-08",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=5IWJBStfU7",
          "pdf_link": "https://openreview.net/pdf?id=5IWJBStfU7"
        },
        "paper_internal_id": "5IWJBStfU7",
        "category": "poster",
        "embedding_score": 0.8272968530654907,
        "final_score": 0.5610404014587402
      },
      "oral": {
        "paper": {
          "id": "Ozo7qJ5vZi",
          "title": "KAN: Kolmogorov–Arnold Networks",
          "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons''), KANs have learnable activation functions on edges (\"weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient.",
          "keywords": [
            "Kolmogorov-Arnold networks",
            "Kolmogorov-Arnold representation theorem",
            "learnable activation functions",
            "interpretability",
            "AI + Science"
          ],
          "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
          "TLDR": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-08",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=Ozo7qJ5vZi",
          "pdf_link": "https://openreview.net/pdf?id=Ozo7qJ5vZi"
        },
        "paper_internal_id": "Ozo7qJ5vZi",
        "category": "oral",
        "embedding_score": 0.7596794366836548,
        "final_score": 0.09905146807432175
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Error-quantified Conformal Inference for Time Series",
      "abstract": "Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal prediction provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a series of online conformal inference methods updated thresholds of prediction sets by performing online gradient descent on a sequence of quantile loss functions. A drawback of such methods is that they only use the information of revealed non-conformity scores via miscoverage indicators but ignore error quantification, namely the distance between the non-conformity score and the current threshold. To accurately leverage the dynamic of miscoverage error, we propose Error-quantified Conformal Inference (ECI) by smoothing the quantile loss function. ECI introduces a continuous and adaptive feedback scale with the miscoverage error, rather than simple binary feedback in existing methods. We establish a long-term coverage guarantee for ECI under arbitrary dependence and distribution shift. The extensive experimental results show that ECI can achieve valid miscoverage control and output tighter prediction sets than other baselines.",
      "keywords": [
        "Time Series; Uncertainty Quantification; Conformal Prediction; Distribution Shift"
      ],
      "primary_area": "learning on time series and dynamical systems",
      "TLDR": "We propose a new online conformal inference method ECI by quantifying the extent of under/over coverage, which can react quickly to distribution shift in time series.",
      "creation_date": "2024-09-25",
      "original_date": "2024-10-04",
      "modification_date": "2025-05-01",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=RD9q5vEe1Q",
      "pdf_link": "https://openreview.net/pdf?id=RD9q5vEe1Q",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "RD9q5vEe1Q"
    },
    "query_internal_id": "RD9q5vEe1Q",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "WFlLqUmb9v",
          "title": "Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation",
          "abstract": "Time-series forecasting is a long-standing challenge in statistics and machine learning, with one of the key difficulties being the ability to process sequences with long-range dependencies. A recent line of work has addressed this by applying the short-time Fourier transform (STFT), which partitions sequences into multiple subsequences and applies a Fourier transform to each separately.\nWe propose the Frequency Information Aggregation (FIA-Net), a model that can utilize two backbone architectures: the Window-Mixing MLP (WM-MLP), which aggregates adjacent window information in the frequency domain, and the Hyper-Complex MLP (HC-MLP), which treats the set of STFT windows as hyper-complex (HC) valued vectors. and employ HC algebra to efficiently combine information from all STFT windows altogether. Furthermore, due to the nature of HC operations, the HC-MLP uses up to three times fewer parameters than the equivalent standard window aggre- gation method. We evaluate the FIA-Net on various time-series benchmarks and show that the proposed methodologies outperform existing state-of-the-art meth- ods in terms of both accuracy and efficiency. Our code is publicly available on https://anonymous.4open.science/r/research-1803/",
          "keywords": [
            "time-series forecasting",
            "frequency models",
            "hyper-complex machine learning",
            "short-time Fourier transform"
          ],
          "primary_area": "learning on time series and dynamical systems",
          "TLDR": "We propose a novel time-series forecasting model that uses STFT window aggregation and hyper-complex models.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=WFlLqUmb9v",
          "pdf_link": "https://openreview.net/pdf?id=WFlLqUmb9v"
        },
        "paper_internal_id": "WFlLqUmb9v",
        "category": "reject",
        "embedding_score": 0.675125241279602,
        "final_score": 0.896452784538269
      },
      "spotlight": {
        "paper": {
          "id": "e1wDDFmlVu",
          "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
          "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available at https://github.com/Time-MoE/Time-MoE",
          "keywords": [
            "time series",
            "foundation model",
            "forecasting"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Time-MoE is a family of time-series foundation models with the mixture-of-experts architecture. For the first time, Time-MoE has scaled up to 2.4 billion parameters, resulting in substantially improved zero-shot/full-shot forecasting performance.",
          "creation_date": "2024-09-24",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=e1wDDFmlVu",
          "pdf_link": "https://openreview.net/pdf?id=e1wDDFmlVu"
        },
        "paper_internal_id": "e1wDDFmlVu",
        "category": "spotlight",
        "embedding_score": 0.6616389751434326,
        "final_score": 0.7289202213287354
      },
      "oral": {
        "paper": {
          "id": "k38Th3x4d9",
          "title": "Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery",
          "abstract": "Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies.",
          "keywords": [
            "root cause analysis",
            "Granger causality",
            "multivariate time series"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-20",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=k38Th3x4d9",
          "pdf_link": "https://openreview.net/pdf?id=k38Th3x4d9"
        },
        "paper_internal_id": "k38Th3x4d9",
        "category": "oral",
        "embedding_score": 0.6428059339523315,
        "final_score": 0.9524211287498474
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Learning Fine-Grained Representations through Textual Token Disentanglement in Composed Video Retrieval",
      "abstract": "With the explosive growth of video data, finding videos that meet detailed requirements in large datasets has become a challenge. To address this, the composed video retrieval task has been introduced, enabling users to retrieve videos using complex queries that involve both visual and textual information. However, the inherent heterogeneity between the modalities poses significant challenges. Textual data are highly abstract, while video content contains substantial redundancy. The modality gap in information representation makes existing methods struggle with the modality fusion and alignment required for fine-grained composed retrieval. To overcome these challenges, we first introduce FineCVR-1M, a fine-grained composed video retrieval dataset containing 1,010,071 video-text triplets with detailed textual descriptions. This dataset is constructed through an automated process that identifies key concept changes between video pairs to generate textual descriptions for both static and action concepts. For fine-grained retrieval methods, the key challenge lies in understanding the detailed requirements. Text description serves as clear expressions of intent, but it requires models to distinguish subtle differences in the description of video semantics. Therefore, we propose a textual Feature Disentanglement and Cross-modal Alignment framework (FDCA) that disentangles features at both the sentence and token levels. At the sequence level, we separate text features into retained and injected features. At the token level, an Auxiliary Token Disentangling mechanism is proposed to disentangle texts into retained, injected, and excluded tokens. The disentanglement at both levels extracts fine-grained features, which are aligned and fused with the reference video to extract global representations for video retrieval. Experiments on FineCVR-1M dataset demonstrate the superior performance of FDCA. Our code and dataset are available at: https://may2333.github.io/FineCVR/.",
      "keywords": [
        "Composed Video Retrieval; Fine-grained Representation; Feature Disentanglement"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "",
      "creation_date": "2024-09-18",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-26",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=wGa2plE8ka",
      "pdf_link": "https://openreview.net/pdf?id=wGa2plE8ka",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "wGa2plE8ka"
    },
    "query_internal_id": "wGa2plE8ka",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "GsCMKwyfWm",
          "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
          "abstract": "Counting is a fundamental skill for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs’ counting abilities using a divide-and conquer approach, breaking counting problems into sub-counting tasks. Unlike prior methods, which do not generalize well to counting datasets on which they have not been trained, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances counting capabilities across various datasets and benchmarks.",
          "keywords": [
            "Counting",
            "Large vision-language models"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose a method that enhances the counting ability of large vision-language models by dividing the image into subimages through a mechanism that does not bisect the target objects.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=GsCMKwyfWm",
          "pdf_link": "https://openreview.net/pdf?id=GsCMKwyfWm"
        },
        "paper_internal_id": "GsCMKwyfWm",
        "category": "reject",
        "embedding_score": 0.6847028732299805,
        "final_score": 0.8707904815673828
      },
      "spotlight": {
        "paper": {
          "id": "G6dMvRuhFr",
          "title": "Grounding Video Models to Actions through Goal Conditioned Exploration",
          "abstract": "Large video models, pretrained on massive quantities of amount of Internet video,  provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.\nHowever, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.\nTo tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. \nGathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.\nIn this paper, we investigate how to directly  ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.\nWe propose a framework that uses trajectory level action generation in combination with video guidance to\nenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.\nWe validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. \nWe show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.",
          "keywords": [
            "Embodied AI",
            "Decision Making",
            "Robotics",
            "Video Model"
          ],
          "primary_area": "applications to robotics, autonomy, planning",
          "TLDR": "We illustrate how we can ground video models to actions without using actions labels through goal conditioned exploration.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=G6dMvRuhFr",
          "pdf_link": "https://openreview.net/pdf?id=G6dMvRuhFr"
        },
        "paper_internal_id": "G6dMvRuhFr",
        "category": "spotlight",
        "embedding_score": 0.6897778511047363,
        "final_score": 0.706433892250061
      },
      "oral": {
        "paper": {
          "id": "Ha6RTeWMd0",
          "title": "SAM 2: Segment Anything in Images and Videos",
          "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code.",
          "keywords": [
            "computer vision",
            "video segmentation",
            "image segmentation"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-13",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-22",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=Ha6RTeWMd0",
          "pdf_link": "https://openreview.net/pdf?id=Ha6RTeWMd0"
        },
        "paper_internal_id": "Ha6RTeWMd0",
        "category": "oral",
        "embedding_score": 0.7137274146080017,
        "final_score": 0.8127578496932983
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Find A Winning Sign: Sign Is All We Need to Win the Lottery",
      "abstract": "The Lottery Ticket Hypothesis (LTH) posits the existence of a sparse subnetwork (a.k.a. winning ticket) that can generalize comparably to its over-parameterized counterpart when trained from scratch.\nThe common approach to finding a winning ticket is to preserve the original strong generalization through Iterative Pruning (IP) and transfer information useful for achieving the learned generalization by applying the resulting sparse mask to an untrained network.\nHowever, existing IP methods still struggle to generalize their observations beyond ad-hoc initialization and small-scale architectures or datasets, or they bypass these challenges by applying their mask to trained weights instead of initialized ones.\nIn this paper, we demonstrate that the parameter sign configuration plays a crucial role in conveying useful information for generalization to any randomly initialized network.\nThrough linear mode connectivity analysis, we observe that a sparse network trained by an existing IP method can retain its basin of attraction if its parameter signs and normalization layer parameters are preserved.\nTo take a step closer to finding a winning ticket, we alleviate the reliance on normalization layer parameters by preventing high error barriers along the linear path between the sparse network trained by our method and its counterpart with initialized normalization layer parameters.\nInterestingly, across various architectures and datasets, we observe that any randomly initialized network can be optimized to exhibit low error barriers along the linear path to the sparse network trained by our method by inheriting its sparsity and parameter sign information, potentially achieving performance comparable to the original.\nThe code is available at https://github.com/JungHunOh/AWS_ICLR2025.git.",
      "keywords": [
        "lottery ticket hypothesis",
        "network pruning",
        "linear mode connectivity"
      ],
      "primary_area": "other topics in machine learning (i.e., none of the above)",
      "TLDR": "We demonstrate that an effective signed mask can allow any randomly initialized network to win the lottery.",
      "creation_date": "2024-09-13",
      "original_date": "2024-10-04",
      "modification_date": "2025-04-01",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=cLtE4qoPlD",
      "pdf_link": "https://openreview.net/pdf?id=cLtE4qoPlD",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "cLtE4qoPlD"
    },
    "query_internal_id": "cLtE4qoPlD",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "PJjHILiQHC",
          "title": "Approaching Deep Learning through the Spectral Dynamics of Weights",
          "abstract": "We propose an empirical approach centered on the spectral dynamics of weights---the behavior of singular values and vectors during optimization---to unify and clarify several phenomena in deep learning. We identify a consistent bias in optimization across various experiments, from small-scale ``grokking'' to large-scale tasks like image classification with ConvNets, image generation with UNets, speech recognition with LSTMs, and language modeling with Transformers. We also demonstrate that weight decay enhances this bias beyond its role as a norm regularizer, even in practical systems. Moreover, we show that these spectral dynamics distinguish memorizing networks from generalizing ones, offering a novel perspective on this longstanding conundrum. Additionally, we leverage spectral dynamics to explore the emergence of well-performing sparse subnetworks (lottery tickets) and the structure of the loss surface through linear mode connectivity. Our findings suggest that spectral dynamics provide a coherent framework to better understand the behavior of neural networks across diverse settings.",
          "keywords": [
            "simplicity bias",
            "grokking",
            "lottery tickets",
            "linear mode connectivity"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We show that an optimization bias in the singular values and vectors of weight matrices links together many complex phenomena in deep learning.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=PJjHILiQHC",
          "pdf_link": "https://openreview.net/pdf?id=PJjHILiQHC"
        },
        "paper_internal_id": "PJjHILiQHC",
        "category": "reject",
        "embedding_score": 0.7445379495620728,
        "final_score": 0.7871547937393188
      },
      "spotlight": {
        "paper": {
          "id": "DC8bsa9bzY",
          "title": "Estimating the Probabilities of Rare Outputs in Language Models",
          "abstract": "We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.",
          "keywords": [
            "low probabilities",
            "adversarial training",
            "importance sampling"
          ],
          "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
          "TLDR": "We present methods for estimating the probability that a language model outputs a rare token on a given input distribution.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=DC8bsa9bzY",
          "pdf_link": "https://openreview.net/pdf?id=DC8bsa9bzY"
        },
        "paper_internal_id": "DC8bsa9bzY",
        "category": "spotlight",
        "embedding_score": 0.686262845993042,
        "final_score": 0.06315145641565323
      },
      "oral": {
        "paper": {
          "id": "25kAzqzTrz",
          "title": "Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning",
          "abstract": "Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize  better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in  FixMatch-like SSL applied to DNNs by taking  convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch.",
          "keywords": [
            "deep semi-supervised learning",
            "generalization error",
            "feature learning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=25kAzqzTrz",
          "pdf_link": "https://openreview.net/pdf?id=25kAzqzTrz"
        },
        "paper_internal_id": "25kAzqzTrz",
        "category": "oral",
        "embedding_score": 0.7240649461746216,
        "final_score": 0.6604621410369873
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Agents' Room:  Narrative Generation through Multi-step Collaboration",
      "abstract": "Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.",
      "keywords": [
        "fiction",
        "creative writing",
        "long-form generation",
        "LLMs",
        "agent",
        "collaboration",
        "multi-agent",
        "dataset",
        "evaluation"
      ],
      "primary_area": "applications to computer vision, audio, language, and other modalities",
      "TLDR": "We propose Agents' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents.",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-15",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=HfWcFs7XLR",
      "pdf_link": "https://openreview.net/pdf?id=HfWcFs7XLR",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "HfWcFs7XLR"
    },
    "query_internal_id": "HfWcFs7XLR",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "CFKZKjrQ5r",
          "title": "FCoReBench: Can Large Language Models Solve Challenging First-Order Combinatorial Reasoning Problems?",
          "abstract": "Can the large language models (LLMs) solve challenging first-order combinatorial\nreasoning problems such as graph coloring, knapsack, and cryptarithmetic? By\nfirst-order, we mean these problems can be instantiated with potentially an infinite\nnumber of problem instances of varying sizes. They are also challenging being\nNP-hard and requiring several reasoning steps to reach a solution. While existing\nwork has focused on coming up with datasets with hard benchmarks, there is\nlimited work which exploits the first-order nature of the problem structure. To\naddress this challenge, we present FCoReBench, a dataset of 40 such challenging\nproblems, along with scripts to generate problem instances of varying sizes and\nautomatically verify and generate their solutions. We first observe that LLMs, even\nwhen aided by symbolic solvers, perform rather poorly on our dataset, being unable\nto leverage the underlying structure of these problems. We specifically observe\na drop in performance with increasing problem size. In response, we propose a\nnew approach, SymPro-LM, which combines LLMs with both symbolic solvers\nand program interpreters, along with feedback from a few solved examples, to\nachieve huge performance gains. Our proposed approach is robust to changes in the\nproblem size, and has the unique characteristic of not requiring any LLM call during\ninference time, unlike earlier approaches. As an additional experiment, we also\ndemonstrate SymPro-LM’s effectiveness on other logical reasoning benchmarks.",
          "keywords": [
            "llms",
            "logical-reasoning",
            "first-order-reasoning",
            "neuro-symbolic"
          ],
          "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
          "TLDR": "We introduce a dataset for first-order combinatorial reasoning and propose a method to integrate LLMs with symbolic solvers through programs, we show significant performance improvements on our dataset and effectiveness on other benchmarks.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=CFKZKjrQ5r",
          "pdf_link": "https://openreview.net/pdf?id=CFKZKjrQ5r"
        },
        "paper_internal_id": "CFKZKjrQ5r",
        "category": "reject",
        "embedding_score": 0.6414691805839539,
        "final_score": 0.14073599874973297
      },
      "spotlight": {
        "paper": {
          "id": "h0ZfDIrj7T",
          "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
          "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni.",
          "keywords": [
            "Multi-Agent Inference",
            "Large Language Model"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "This paper presents a method that synergistically leverage multiple LLMs to significantly improve their performance.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=h0ZfDIrj7T",
          "pdf_link": "https://openreview.net/pdf?id=h0ZfDIrj7T"
        },
        "paper_internal_id": "h0ZfDIrj7T",
        "category": "spotlight",
        "embedding_score": 0.6621943712234497,
        "final_score": 0.06496894359588623
      },
      "oral": {
        "paper": {
          "id": "mtSSFiqW6y",
          "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
          "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
          "keywords": [
            "LLM inference",
            "speculative decoding"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mtSSFiqW6y",
          "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y"
        },
        "paper_internal_id": "mtSSFiqW6y",
        "category": "oral",
        "embedding_score": 0.6347788572311401,
        "final_score": 0.0464840792119503
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels",
      "abstract": "Collaboration is a key challenge in distributed multi-agent reinforcement learning (MARL) environments. Learning frameworks for these decentralized systems must weigh the benefits of explicit player coordination against the communication overhead and computational cost of sharing local observations and environmental data. Quantum computing has sparked a potential synergy between quantum entanglement and cooperation in multi-agent environments, which could enable more efficient distributed collaboration with minimal information sharing. This relationship is largely unexplored, however, as current state-of-the-art quantum MARL (QMARL) implementations rely on classical information sharing rather than entanglement over a quantum channel as a coordination medium. In contrast, in this paper, a novel framework dubbed entangled QMARL (eQMARL) is proposed. The proposed eQMARL is a distributed actor-critic framework that facilitates cooperation over a quantum channel and eliminates local observation sharing via a quantum entangled split critic. Introducing a quantum critic uniquely spread across the agents allows coupling of local observation encoders through entangled input qubits over a quantum channel, which requires no explicit sharing of local observations and reduces classical communication overhead. Further, agent policies are tuned through joint observation-value function estimation via joint quantum measurements, thereby reducing the centralized computational burden. Experimental results show that eQMARL with $\\Psi^{+}$ entanglement converges to a cooperative strategy up to $17.8\\\\%$ faster and with a higher overall score compared to split classical and fully centralized classical and quantum baselines. The results also show that eQMARL achieves this performance with a constant factor of $25$-times fewer centralized parameters compared to the split classical baseline.",
      "keywords": [
        "quantum machine learning",
        "multi-agent reinforcement learning",
        "quantum entanglement"
      ],
      "primary_area": "reinforcement learning",
      "TLDR": "",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-20",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=cR5GTis5II",
      "pdf_link": "https://openreview.net/pdf?id=cR5GTis5II",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "cR5GTis5II"
    },
    "query_internal_id": "cR5GTis5II",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "kBybSUskz7",
          "title": "Reinforcement Learning and Heuristics for Hardware-Efficient Constrained Code Design",
          "abstract": "Constrained codes enhance reliability in high-speed communication systems and optimize bit efficiency when working with non-binary data representations (e.g., three-level ternary symbols).  A key challenge in their design is minimizing the hardware complexity of the translation logic that encodes and decodes data. We introduce a reinforcement learning (RL)-based framework, augmented by a custom L1 similarity-based heuristic, to design hardware-efficient translation logic, navigating the vast solution space of codeword assignments. By modeling the task as a bipartite graph matching problem and using logic synthesis tools to evaluate hardware complexity, our RL approach outperforms human-derived solutions and generalizes to various code types. Finally, we analyze the learned policies to extract insights into high-performing strategies.",
          "keywords": [
            "reinforcement learning",
            "bipartite matching",
            "GNN",
            "combinatorial optimization",
            "feature engineering",
            "hardware design optimization",
            "logic synthesis"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "Reinforcement learning framework to optimize constrained code design, modeling codeword assignments as bipartite graph matching and using logic synthesis tools to minimize hardware complexity",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=kBybSUskz7",
          "pdf_link": "https://openreview.net/pdf?id=kBybSUskz7"
        },
        "paper_internal_id": "kBybSUskz7",
        "category": "reject",
        "embedding_score": 0.6564007997512817,
        "final_score": 0.8112394213676453
      },
      "spotlight": {
        "paper": {
          "id": "Dzh0hQPpuf",
          "title": "Student-Informed Teacher Training",
          "abstract": "Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks.",
          "keywords": [
            "Reinforcement Learning",
            "Imitation Learning",
            "Robotics"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "To address the teacher-student asymmetry in imitation learning, we propose a joint learning framework for both teacher and student, adapting the teacher to the capabilities of the student during training to enhance alignment.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=Dzh0hQPpuf",
          "pdf_link": "https://openreview.net/pdf?id=Dzh0hQPpuf"
        },
        "paper_internal_id": "Dzh0hQPpuf",
        "category": "spotlight",
        "embedding_score": 0.6440072059631348,
        "final_score": 0.360621839761734
      },
      "oral": {
        "paper": {
          "id": "stUKwWBuBm",
          "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
          "abstract": "A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \\emph{all} $n$-player matrix and finite-horizon Markov games.  In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality.  To validate the expressivity of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model. We validate our findings on a simple multi-agent reinforcement learning benchmark. Our results open the doors for to the principled development of new decentralized multi-agent reinforcement learning algorithms.",
          "keywords": [
            "behavioral economics",
            "risk-aversion",
            "multi-agent reinforcement learning",
            "quantal response",
            "bounded rationality"
          ],
          "primary_area": "learning theory",
          "TLDR": "By incorporating risk aversion and bounded rationality into agents' decision-making processes, we introduced a computationally tractable equilibria class for matrix and Markov games which aligns with observed human behaviors.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-30",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=stUKwWBuBm",
          "pdf_link": "https://openreview.net/pdf?id=stUKwWBuBm"
        },
        "paper_internal_id": "stUKwWBuBm",
        "category": "oral",
        "embedding_score": 0.7354108095169067,
        "final_score": 0.5572941899299622
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
      "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in four diverse tasks covering text, images, and actions (mathematics, programming, visual question answering, and tool-use) and test multiple student and teacher models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. Project page: https://DataEnvGym.github.io.",
      "keywords": [
        "iterative data generation",
        "llm agent",
        "lifelong learning"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Spotlight",
      "forum_link": "https://openreview.net/forum?id=00SnKBGTsz",
      "pdf_link": "https://openreview.net/pdf?id=00SnKBGTsz",
      "label": "spotlight",
      "conference": "ICLR",
      "paper_id": "00SnKBGTsz"
    },
    "query_internal_id": "00SnKBGTsz",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "5EuAMDMPRK",
          "title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
          "abstract": "Balancing safety and usefulness in large language models has become a critical challenge in recent years. \nModels often exhibit unsafe behavior or adopt an overly cautious approach, leading to frequent overrefusal of benign prompts, which reduces their usefulness. \nAddressing these issues requires methods that maintain safety while avoiding overrefusal. \nIn this work, we examine how the overgeneration of training data using advanced teacher models (e.g., GPT-4o), including responses to both general-purpose and toxic prompts, influences the safety and usefulness in instruction-following language models.\nAdditionally, we present POROver, a strategy to use preference optimization methods in order to reduce overrefusal, via employing a superior teacher model's completions.\nOur results show that overgenerating completions for general-purpose prompts significantly enhances the model's safety and usefulness balance.\nSpecifically, the F1 score calculated between safety and usefulness increases from 74.4\\% to 91.8\\% due to a substantial increase in safety. \nMoreover, overgeneration for toxic prompts substantially increases the usefulness from 11.1\\% to 57.6\\% while maintaining safety.\nFurthermore, preference optimization algorithms, when applied with carefully curated preference data, can effectively increase a model's usefulness from 57.6\\% to 82.1\\% while maintaining comparable safety levels.",
          "keywords": [
            "LLM safety",
            "LLM usefulness",
            "Overrefusal in LLMs",
            "responsible AI"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "This paper examines the impact of using superior language models as teachers on the safety-usefulness trade-off in student models, and explores the use of preference optimization methods to reduce overrefusal.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=5EuAMDMPRK",
          "pdf_link": "https://openreview.net/pdf?id=5EuAMDMPRK"
        },
        "paper_internal_id": "5EuAMDMPRK",
        "category": "reject",
        "embedding_score": 0.6942760944366455,
        "final_score": 0.7867921590805054
      },
      "poster": {
        "paper": {
          "id": "9RCT0ngvZP",
          "title": "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning",
          "abstract": "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.",
          "keywords": [
            "synthetic data",
            "data influence",
            "instruction tuning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=9RCT0ngvZP",
          "pdf_link": "https://openreview.net/pdf?id=9RCT0ngvZP"
        },
        "paper_internal_id": "9RCT0ngvZP",
        "category": "poster",
        "embedding_score": 0.7645255327224731,
        "final_score": 0.8230626583099365
      },
      "oral": {
        "paper": {
          "id": "YUYJsHOf3c",
          "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
          "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.",
          "keywords": [
            "LLM",
            "reasoning",
            "generalization",
            "self-improvement"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose ReGenesis, a method to self-synthesize reasoning paths as post-training data of LLMs by progressing from general reasoning structures to task-specific reasoning paths, to improve LLMs' generalization capability in reasoning.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=YUYJsHOf3c",
          "pdf_link": "https://openreview.net/pdf?id=YUYJsHOf3c"
        },
        "paper_internal_id": "YUYJsHOf3c",
        "category": "oral",
        "embedding_score": 0.7124015092849731,
        "final_score": 0.5482689738273621
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "FairDen: Fair Density-Based Clustering",
      "abstract": "Fairness in data mining tasks like clustering has recently become an increasingly important aspect. \nHowever, few clustering algorithms exist that focus on fair groupings of data with sensitive attributes. \nIncluding fairness in the clustering objective is especially hard for density-based clustering, as it does not directly optimize a closed form objective like centroid-based or spectral methods.  \n\nThis paper introduces FairDen, the first fair, density-based clustering algorithm.\nWe capture the dataset's density-connectivity structure in a similarity matrix that we manipulate to encourage a balanced clustering. \nIn contrast to state-of-the-art, FairDen inherently handles categorical attributes, noise, and data with several sensitive attributes or groups.\nWe show that FairDen finds meaningful and fair clusters in extensive experiments.",
      "keywords": [
        "Fair Clustering",
        "Density-based Clustering",
        "Density-Connectivity",
        "Fairness",
        "Unsupervised Learning"
      ],
      "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
      "TLDR": "We developed the first density-based fair clustering method - it finds density-based clusters that have balanced ratios of sensitive groups, accounting for multiple non-binary sensitive attributes.",
      "creation_date": "2024-09-25",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-26",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=aPHHhnZktB",
      "pdf_link": "https://openreview.net/pdf?id=aPHHhnZktB",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "aPHHhnZktB"
    },
    "query_internal_id": "aPHHhnZktB",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "nphsoKxlFs",
          "title": "Dynamic Contrastive Learning for Time Series Representation",
          "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an unsupervised way, which allows for good performance in classification or detection tasks after later minimal human labeling. In this paper, we propose dynamic contrastive learning (DynaCL), an unsupervised representation learning framework for time series that uses temporal adjacent steps to define positive pairs. DynaCL adopts N-pair loss to dynamically treat all samples in a batch as positive or negative pairs, enabling efficient training and addressing the challenges of complicated sampling of positives. We demonstrate that DynaCL embeds instances from time series into well-defined, semantically meaningful clusters, which allows superior performance on downstream tasks on a variety of public time series datasets. Our findings also reveal that high scores on unsupervised clustering metrics do not guarantee that the representations are useful in downstream tasks.",
          "keywords": [
            "contrastive learning",
            "self-supervised learning",
            "time series analysis",
            "representation learning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We develop an unsupervised representation learning framework for time series, employing contrastive learning with multiple positive pairs",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=nphsoKxlFs",
          "pdf_link": "https://openreview.net/pdf?id=nphsoKxlFs"
        },
        "paper_internal_id": "nphsoKxlFs",
        "category": "reject",
        "embedding_score": 0.6341588497161865,
        "final_score": 0.8342937231063843
      },
      "spotlight": {
        "paper": {
          "id": "5ZEbpBYGwH",
          "title": "COPER: Correlation-based Permutations for Multi-View Clustering",
          "abstract": "Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We propose an end-to-end deep learning-based multi-view clustering framework for general data types (such as images and tables). Our approach involves generating meaningful fused representations using a novel permutation-based canonical correlation objective. We provide a theoretical analysis showing how the learned embeddings approximate those obtained by supervised linear discriminant analysis (LDA). Cluster assignments are learned by identifying consistent pseudo-labels across multiple views. Additionally, we establish a theoretical bound on the error caused by incorrect pseudo-labels in the unsupervised representations compared to LDA. Extensive experiments on ten multi-view clustering benchmark datasets provide empirical evidence for the effectiveness of the proposed model.",
          "keywords": [
            "clustering",
            "canonical correlation analysis",
            "self supervision",
            "multiview"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-19",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=5ZEbpBYGwH",
          "pdf_link": "https://openreview.net/pdf?id=5ZEbpBYGwH"
        },
        "paper_internal_id": "5ZEbpBYGwH",
        "category": "spotlight",
        "embedding_score": 0.690786600112915,
        "final_score": 0.9203947186470032
      },
      "oral": {
        "paper": {
          "id": "f4gF6AIHRy",
          "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
          "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Source code\nis available at: https://github.com/MediaBrain-SJTU/DiSF.git.",
          "keywords": [
            "file selection",
            "large language model",
            "pre-training",
            "submodular optimization"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=f4gF6AIHRy",
          "pdf_link": "https://openreview.net/pdf?id=f4gF6AIHRy"
        },
        "paper_internal_id": "f4gF6AIHRy",
        "category": "oral",
        "embedding_score": 0.6485691070556641,
        "final_score": 0.8966250419616699
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "NgLFQTBPRR",
      "title": "An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination",
      "abstract": "Unsupervised anomaly detection (AD) methods typically assume clean training data, yet real-world datasets often contain undetected or mislabeled anomalies, leading to significant performance degradation. Existing solutions require access to the training pipelines, data or prior knowledge of the proportions of anomalies in the data, limiting their real-world applicability. To address this challenge, we propose EPHAD, a simple yet effective test-time adaptation framework that updates the outputs of AD models trained on contaminated datasets using evidence gathered at test time. Our approach integrates the prior knowledge captured by the AD model trained on contaminated datasets with evidence derived from multimodal foundation models like Contrastive Language-Image Pre-training (CLIP), classical AD methods like the Latent Outlier Factor or domain-specific knowledge. We illustrate the intuition behind EPHAD using a synthetic toy example and validate its effectiveness through comprehensive experiments across eight visual AD datasets, twenty-six tabular AD datasets, and a real-world industrial AD dataset. Additionally, we conduct an ablation study to analyse hyperparameter influence and robustness to varying contamination levels, demonstrating the versatility and robustness of EPHAD across diverse AD models and evidence pairs. To ensure reproducibility, our code is publicly available at https://github.com/sukanyapatra1997/EPHAD.",
      "keywords": "['anomaly detection', 'anomaly', 'contamination', 'test-time adaptation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "NgLFQTBPRR",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "dCWSVAbWXM",
          "title": "Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection",
          "abstract": "Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any labels even when the training set may contain anomalous samples. To achieve FUAD,  we pioneer the introduction of Knowledge Distillation (KD) paradigm based on teacher–student framework into the FUAD setting. However, due to the presence of anomalies in the training data, traditional KD methods risk enabling the student to learn the teacher’s representation of anomalies under FUAD setting, thereby resulting in poor anomaly detection performance. To address this issue, we propose a novel Cross-Domain Distillation (CDD) framework based on the widely studied reverse distillation (RD) paradigm. Specifically, we design a Domain-Specific Training, which divides the training set into multiple domains with lower anomaly ratios and train a domain-specific student for each. Cross-Domain Knowledge Aggregation is then performed, where pseudo-normal features generated by domain-specific students collaboratively guide a global student to learn generalized normal representations across all samples. Experimental results on noisy versions of the MVTec AD and VisA datasets demonstrate that our method achieves significant performance improvements over the baseline, validating its effectiveness under FUAD setting.",
          "keywords": [
            "Anomaly Detection",
            "Unsupervised Learning",
            "Knowledge Distillation"
          ],
          "primary_area": "applications",
          "TLDR": "The first attempt to apply Knowledge Distillation paradigm for Fully Unsupervised Anomaly Detection.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=dCWSVAbWXM",
          "pdf_link": "https://openreview.net/pdf?id=dCWSVAbWXM"
        },
        "paper_internal_id": "dCWSVAbWXM",
        "category": "reject",
        "embedding_score": 0.8341978192329407,
        "final_score": 0.9571120142936707
      },
      "poster": {
        "paper": {
          "id": "pKg5zKIEuV",
          "title": "Quantifying Statistical Significance of Deep Nearest Neighbor Anomaly Detection via Selective Inference",
          "abstract": "In real-world applications, anomaly detection (AD) often operates without access to anomalous data, necessitating semi-supervised methods that rely solely on normal data.\nAmong these methods, deep $k$-nearest neighbor (deep $k$NN) AD stands out for its interpretability and flexibility, leveraging distance-based scoring in deep latent spaces.\nDespite its strong performance, deep $k$NN lacks a mechanism to quantify uncertainty—an essential feature for critical applications such as industrial inspection.\nTo address this limitation, we propose a statistical framework that quantifies the  significance of detected anomalies in the form of $p$-values, thereby enabling control over false positive rates at a user-specified significance level (e.g.,0.05).\nA central challenge lies in managing selection bias, which we tackle using Selective Inference—a principled method for conducting inference conditioned on data-driven selections.\nWe evaluate our method on diverse datasets and demonstrate that it provides reliable AD well-suited for industrial use cases.",
          "keywords": [
            "Anomaly Detection",
            "k-Nearest Neighbors",
            "Statistical Test",
            "Selective Inference",
            "Deep Learning"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "We propose a method to quantify the statistical significance of anomalies detected by deep k-nearest neighbor approach.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=pKg5zKIEuV",
          "pdf_link": "https://openreview.net/pdf?id=pKg5zKIEuV"
        },
        "paper_internal_id": "pKg5zKIEuV",
        "category": "poster",
        "embedding_score": 0.7838863134384155,
        "final_score": 0.9897704124450684
      },
      "oral": {
        "paper": {
          "id": "koEALFNBj1",
          "title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
          "abstract": "REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called \\textit{\\textbf{R}epresentation \\textbf{E}ntanglement for \\textbf{G}eneration} (\\textbf{REG}), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. \nREG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency.\nThis is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\\% increase in FLOPs and latency).\nThe inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. \nMore impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at: https://github.com/Martinser/REG.",
          "keywords": [
            "Diffusion Model Acceleration; Representation Entanglement; Diffusion Transformers"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-06",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=koEALFNBj1",
          "pdf_link": "https://openreview.net/pdf?id=koEALFNBj1"
        },
        "paper_internal_id": "koEALFNBj1",
        "category": "oral",
        "embedding_score": 0.702348530292511,
        "final_score": 0.44255104660987854
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "hq2CkcEY7h",
      "title": "Mitigating Instability in High Residual Adaptive Sampling for PINNs via Langevin Dynamics",
      "abstract": "Recently, physics-informed neural networks (PINNs) have gained attention in the scientific community for their potential to solve partial differential equations (PDEs). However, they face challenges related to resource efficiency and slow convergence. Adaptive sampling methods, which prioritize collocation points with high residuals, improve both efficiency and accuracy. However, these methods often neglect points with medium or low residuals, which can affect stability as the complexity of the model increases. In this paper, we investigate this limitation and show that high residual-based approaches require stricter learning rate bounds to ensure stability. To address this, we propose a Langevin dynamics-based Adaptive Sampling (LAS) framework that is robust to various learning rates and model complexities. Our experiments demonstrate that the proposed method outperforms existing approaches in terms of relative $L^{2}$ error, and stability across a range of environments, including high-dimensional PDEs where Monte Carlo integration-based methods typically suffer from instability.",
      "keywords": "['Langevin dynamics', 'Adaptive sampling', 'Physics-informed neural network', 'Learning stability']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "hq2CkcEY7h",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "xEzQCFSfPG",
          "title": "Grokking and Generalization Collapse: Insights from HTSR theory",
          "abstract": "Grokking is a surprising phenomenon in neural network training where test accuracy remains low for an extended period despite near-perfect training accuracy, only to suddenly leap to strong generalization. In this work, we study grokking using a depth-3, width-200 ReLU MLP trained on a subset of MNIST. We investigate it's long-term dynamics under both weight-decay and, critically, no-decay regimes—the latter often characterized by increasing $l^2$ weight norms. Our primary tool is the theory of Heavy-Tailed Self-Regularization **HTSR**, where we track the heavy-tailed exponent $\\alpha$. We find that $\\alpha$ reliably predicts both the initial grokking transition and subsequent anti-grokking. We benchmark these insights against four prior approaches: progress measures---Activation Sparsity, Absolute Weight Entropy, and Approximate Local Circuit Complexity ---and weight norm ($l^2$) analysis.\nOur experiments show that while comparative approaches register significant changes, **in this regime of increasing $l^2$ norm, the heavy-tailed exponent $\\alpha$ demonstrates a unique correlation with the ensuing large, long-term dip in test accuracy, a signal not reliably captured by most other measures.**\n\n\n\nExtending our zero weight decay experiment significantly beyond typical timescales ($10^{5}$ to approximately $10^{7}$ optimization steps), **we reveal a late-stage catastrophic generalization collapse (``anti-grokking''), characterized by a dramatic drop in test accuracy (over 25 percentage points) while training accuracy remains perfect**; notably, the heavy-tail metric $\\alpha$ uniquely provides an early warning of this impending collapse. Our results underscore the utility of Heavy-Tailed Self-Regularization theory for tracking generalization dynamics, even in the challenging regimes without explicit weight decay regularization.",
          "keywords": [
            "Grokking",
            "Heavy-Tailed Self-Regularization",
            "Random Matrix Theory",
            "Heavy-Tail Exponent",
            "Spectral Analysis",
            "Generalization Dynamics",
            "Catastrophic Generalization Collapse",
            "Implicit Regularization"
          ],
          "primary_area": "deep_learning",
          "TLDR": "Through studying grokking we have discovered how to detect when a model is overfit versus poorly trained without having access to either test or train data.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=xEzQCFSfPG",
          "pdf_link": "https://openreview.net/pdf?id=xEzQCFSfPG"
        },
        "paper_internal_id": "xEzQCFSfPG",
        "category": "reject",
        "embedding_score": 0.7141375541687012,
        "final_score": 0.07877114415168762
      },
      "poster": {
        "paper": {
          "id": "rOuRpOA6pm",
          "title": "Physics-informed machine learning with domain decomposition and global dynamics for three-dimensional intersecting flows",
          "abstract": "Physics-informed neural networks (PINNs) have emerged as a promising framework to develop complex scientific surrogate models, yet their scalability and accuracy often degrade in non-canonical geometries, such as non-rectangular domains or three-dimensional (3D) domains with high aspect ratios. These limitations hinder the broader adoption of vanilla PINNs in real-world, practical systems. In this work, we introduce a multi-domain PINN (MDPINN) framework designed to address the scalability and generalization challenges inherent in 3D non-rectangular domains governed by nonlinear fluid dynamics. The target domain consists of intersecting 3D fluid channels with a high aspect ratio, inducing complex flow features such as deflections, mixing, and recirculations. Our approach is grounded in two key innovations: 1) domain decomposition, which partitions the channel volumes into multiple cubic-like subdomains, each modeled by an individual PINN, 2) enforcement of global dynamics (MDPINN-GD), which ensures that the total mass flow rate entering the domain equals that exiting. These innovations reduce the complexity of the problem imposed on individual PINNs and guide effective network optimization toward physically consistent solutions throughout the domain. We demonstrate that our method achieves: 1) 74.8\\% accuracy improvement over a single-network PINN, and 2) 52.9\\% accuracy improvement over MDPINN that do not enforce global mass conservation. Furthermore, the MDPINN-GD framework exhibits accurate prediction even in highly complex regions-such as the channel intersecting zone and the outlet zone characterized by intense flow mixing and large velocity gradients-achieving maximum normalized mean absolute errors below 14.9\\% for velocity predictions compared to simulation results. This work establishes a path towards scalable, physically grounded surrogate modeling approach that is extensible to multiphysics and high-dimensional scientific problems.",
          "keywords": [
            "Physics-informed neural networks",
            "domain decomposition",
            "global dynamics",
            "three-dimensional fluid dynamics"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=rOuRpOA6pm",
          "pdf_link": "https://openreview.net/pdf?id=rOuRpOA6pm"
        },
        "paper_internal_id": "rOuRpOA6pm",
        "category": "poster",
        "embedding_score": 0.7772591710090637,
        "final_score": 0.8667581081390381
      },
      "oral": {
        "paper": {
          "id": "ImpizBSKcu",
          "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
          "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm.  We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$,\nand large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies:\n$(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network;\n$(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity;\n$(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated  `feature unlearning' regime at large times.",
          "keywords": [
            "Overfitting; feature learning; dynamical mean field theory; generalization;"
          ],
          "primary_area": "theory",
          "TLDR": "Large neural networks first learn low dimensional feature representation then overfit the data and revert to a kernel regime.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=ImpizBSKcu",
          "pdf_link": "https://openreview.net/pdf?id=ImpizBSKcu"
        },
        "paper_internal_id": "ImpizBSKcu",
        "category": "oral",
        "embedding_score": 0.720828652381897,
        "final_score": 0.020930370315909386
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "sPafJfwI2I",
      "title": "Reshape-then-Factorize: Communication-Efficient FL via Model-Agnostic Projection Optimization",
      "abstract": "Federated learning (FL) enables collaborative model training across distributed clients without sharing sensitive data. However, communication overhead remains a significant bottleneck, particularly for large-scale models. Low-rank decomposition techniques address this by approximating each layer’s weights or gradients with a product of low-rank matrices, thereby reducing the communication cost in FL. While effective, these methods are constrained by the layer's architecture and shapes, limiting their flexibility and performance.\nWe propose *Model-Agnostic Projection Optimization* (MAPO), a novel method that reshapes and factorizes the full model gradient into a *fixed reconstruction matrix* and a *trainable projection vector*, avoiding layer-wise decomposition and architecture constraints. MAPO directly optimizes the projection in a randomly sampled subspace, with all clients generating the reconstruction matrix via a shared random seed, incurring no additional communication overhead for synchronization.\nBy decoupling the gradient from architectural constraints through reshaping and enabling communication-free exploration of dynamic subspaces via seed sharing, MAPO provides a more flexible and efficient low-rank representation.\nEmpirical results demonstrate the effectiveness of MAPO in various FL settings.",
      "keywords": "['Federated Learning', 'Low-Rank Adaptation', 'Communication Efficiency', 'Subspace Optimization']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "sPafJfwI2I",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "GkB1ZUNz83",
          "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
          "abstract": "Federated Learning (FL) enables collaborative model training across distributed clients without data sharing, but its high computational and communication demands strain resource-constrained devices. While existing methods use dynamic pruning to improve efficiency by periodically adjusting sparse model topologies while maintaining sparsity, these approaches suffer from issues such as **greedy adjustments**, **unstable topologies**, and **communication inefficiency**, resulting in less robust models and suboptimal performance under data heterogeneity and partial client availability. To address these challenges, we propose **Fed**erated **R**obust pruning via combinatorial **T**hompson **S**ampling (FedRTS),a novel framework designed to develop robust sparse models. FedRTS enhances robustness and performance through its Thompson Sampling-based Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by stable and farsighted information,  instead of deterministic decisions reliant on unstable and myopic information in previous methods. Extensive experiments demonstrate that FedRTS achieves state-of-the-art performance in computer vision and natural language processing tasks while reducing communication costs, particularly excelling in scenarios with heterogeneous data distributions and partial client participation. Our codes are available at: https://github.com/Little0o0/FedRTS.",
          "keywords": [
            "Federared Learning",
            "Neural Network Pruning",
            "Combinatorial Thompson Sampling"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-01",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=GkB1ZUNz83",
          "pdf_link": "https://openreview.net/pdf?id=GkB1ZUNz83"
        },
        "paper_internal_id": "GkB1ZUNz83",
        "category": "poster",
        "embedding_score": 0.783612847328186,
        "final_score": 0.9999297857284546
      },
      "spotlight": {
        "paper": {
          "id": "4Ud0pRqFto",
          "title": "MARS-VFL: A Unified Benchmark for Vertical Federated Learning with Realistic Evaluation",
          "abstract": "Vertical Federated Learning (VFL) has emerged as a critical privacy-preserving learning paradigm, enabling collaborative model training by leveraging distributed features across clients. However, due to privacy concerns, there are few publicly available real-world datasets for evaluating VFL methods, which poses significant challenges to related research. To bridge this gap, we propose MARS-VFL, a unified benchmark for realistic VFL evaluation. It integrates data from practical applications involving collaboration across different features, maintaining compatibility with the VFL setting. Based on this, we standardize the evaluation of VFL methods from the mainstream aspects of efficiency, robustness, and security. We conduct comprehensive experiments to assess different VFL approaches, providing references for unified evaluation. Furthermore, we are the first to unify the evaluation of robustness challenges in VFL and introduce a new method for addressing robustness challenges, establishing standard baselines for future research.",
          "keywords": [
            "Vertical Federated Learning",
            "Distrubuted System",
            "Benchmark"
          ],
          "primary_area": "AL/ML_data_processing_and_benchmarking_infrastructure",
          "TLDR": "We introduce a unified benchmark for VFL that evaluates efficiency, robustness, and security based on realistic data distributions.",
          "creation_date": "2025-05-04",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=4Ud0pRqFto",
          "pdf_link": "https://openreview.net/pdf?id=4Ud0pRqFto"
        },
        "paper_internal_id": "4Ud0pRqFto",
        "category": "spotlight",
        "embedding_score": 0.7107442617416382,
        "final_score": 0.9900464415550232
      },
      "oral": {
        "paper": {
          "id": "aUAG1WS7J2",
          "title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
          "abstract": "Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model’s overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.",
          "keywords": [
            "Federated Learning;Federated Class-Incremental Learning; Data Replay"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=aUAG1WS7J2",
          "pdf_link": "https://openreview.net/pdf?id=aUAG1WS7J2"
        },
        "paper_internal_id": "aUAG1WS7J2",
        "category": "oral",
        "embedding_score": 0.7057745456695557,
        "final_score": 0.1966349482536316
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "1KXST1ksJ2",
      "title": "Learning to Plan Like the Human Brain via Visuospatial Perception and Semantic-Episodic Synergistic Decision-Making",
      "abstract": "Motion planning in high-dimensional continuous spaces remains challenging due to complex environments and computational constraints. Although learning-based planners, especially graph neural network (GNN)-based, have significantly improved planning performance, they still struggle with inaccurate graph construction and limited structural reasoning, constraining search efficiency and path quality. The human brain exhibits efficient planning through a two-stage Perception-Decision model. First, egocentric spatial representations from visual and proprioceptive input are constructed, and then semantic–episodic synergy is leveraged to support decision-making in uncertainty scenarios. Inspired by this process, we propose NeuroMP, a brain-inspired planning framework that learns to plan like the human brain. NeuroMP integrates a Perceptive Segment Selector inspired by visuospatial perception to construct safer graphs, and a Global Alignment Heuristic guide search in weakly connected graphs by modeling semantic-episodic synergistic decision-making. Experimental results demonstrate that NeuroMP significantly outperforms existing planning methods in efficiency and quality while maintaining a high success rate.",
      "keywords": "['Brain-inspired learning; Motion planning; Graph neural networks;']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "1KXST1ksJ2",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "nMUpDatZBh",
          "title": "VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics",
          "abstract": "In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose \\textit{Vision In-Context Operator Networks} (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON's adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9\\% compared to DPOT and 44.7\\% compared to MPP, while requiring only 72.5\\% and 34.8\\% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in \\textit{imperfect measurement systems} where sampling frequencies may differ or frames might be dropped—common challenges in real-world settings—without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41\\% relative performance degradation compared to 71.37\\%-74.49\\% degradation in baseline methods, demonstrating its versatility for depolying in realistic applications.",
          "keywords": [
            "AI4Science",
            "Learning PDE",
            "Fluid Dynamics",
            "In-Context Learning"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "VICON is a SOTA vision transformer-based model that efficiently learns multi-PDE operators from 2D data while handling irregular sampling and imperfect measurements, commonly seen in real industry.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=nMUpDatZBh",
          "pdf_link": "https://openreview.net/pdf?id=nMUpDatZBh"
        },
        "paper_internal_id": "nMUpDatZBh",
        "category": "reject",
        "embedding_score": 0.6673405766487122,
        "final_score": 0.19919107854366302
      },
      "spotlight": {
        "paper": {
          "id": "31CaYYw1Xz",
          "title": "A machine learning approach that beats Rubik's cubes",
          "abstract": "The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Our solution in its current implementation is approximately 25.6 times faster in solving 3x3x3 Rubik's cubes while requiring up to 8.5 times less model training time than the most efficient state-of-the-art competitor. Finally, it is demonstrated that even a single agent trained using a relatively small number of examples can robustly solve a broad range of puzzles represented by Cayley graphs of size up to $10^{145}$, confirming the generality of the proposed method.",
          "keywords": [
            "pathfinding",
            "multi-agent",
            "ML",
            "machine learning",
            "Cayley graph",
            "Rubik",
            "cube",
            "MLP"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "The paper proposes a novel machine learning–based approach that can solve different puzzles and demonstrate unprecedentedly short results when solving Rubik's cubes up to 5x5x5.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=31CaYYw1Xz",
          "pdf_link": "https://openreview.net/pdf?id=31CaYYw1Xz"
        },
        "paper_internal_id": "31CaYYw1Xz",
        "category": "spotlight",
        "embedding_score": 0.7452002763748169,
        "final_score": 0.6481255888938904
      },
      "oral": {
        "paper": {
          "id": "s0JVsx3bx1",
          "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
          "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance.\nOur experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals.\nEvaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\\times$ -- $50\\times$, outperforming other goal-conditioned baselines.\nIncreasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.",
          "keywords": [
            "Reinforcement Learning",
            "Self-Supervised Learning",
            "Contrastive RL",
            "Goal-conditioned RL",
            "Scaling"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "While most RL methods use shallow MLPs (~2–5 layers), we show that scaling up to 1000-layers for contrastive RL (CRL) can significantly boost performance, ranging from doubling performance to 50x on a diverse suite of robotic tasks.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-13",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=s0JVsx3bx1",
          "pdf_link": "https://openreview.net/pdf?id=s0JVsx3bx1"
        },
        "paper_internal_id": "s0JVsx3bx1",
        "category": "oral",
        "embedding_score": 0.7241430282592773,
        "final_score": 0.20990867912769318
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "H8fscnm6Xx",
      "title": "Unextractable Protocol Models: Collaborative Training and Inference without Weight Materialization",
      "abstract": "We consider a decentralized setup in which the participants collaboratively train and serve a large neural network, and where each participant only processes a subset of the model. \nIn this setup, we explore the possibility of unmaterializable weights, where a full weight set is never available to any one participant.\nWe introduce Unextractable Protocol Models (UPMs): a training and inference framework that leverages the sharded model setup to ensure model shards (i.e.,, subsets) held by participants are incompatible at different time steps. UPMs periodically inject time-varying, random, invertible transforms at participant boundaries; preserving the overall network function yet rendering cross-time assemblies incoherent.\nOn Qwen-2.5-0.5B and Llama-3.2-1B, 10 000 transforms leave FP32 perplexity unchanged ($\\Delta$PPL$< 0.01$; Jensen–Shannon drift $<4 \\times 10^{-5}$), and we show how to control growth for lower precision datatypes. Applying a transform every 30s adds 3% latency, 0.1% bandwidth, and 10% GPU-memory overhead at inference, while training overhead falls to 1.6% time and < 1% memory. We consider several attacks, showing that the requirements of direct attacks are impractical and easy to defend against, and that gradient-based fine-tuning of stitched partitions consumes $\\geq 60\\%$ of the tokens required to train from scratch. By enabling models to be collaboratively trained yet not extracted, UPMs make it practical to embed programmatic incentive mechanisms in community-driven decentralized training.",
      "keywords": "['Decentralized training', 'LLMs', 'Distributed training', 'Open source', 'Weight secrecy']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "H8fscnm6Xx",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "6JlzhISecd",
          "title": "A Stochastic Approximation Approach for Efficient Decentralized Optimization on Random Networks",
          "abstract": "A challenging problem in decentralized optimization is to develop algorithms with fast convergence on random and time varying topologies under unreliable and bandwidth-constrained communication network. This paper studies a stochastic approximation approach with a Fully Stochastic Primal Dual Algorithm (FSPDA) framework. Our framework relies on a novel observation that the randomness in time varying topology can be incorporated in a stochastic augmented Lagrangian formulation, whose expected value admits saddle points that coincide with stationary solutions of the decentralized optimization problem. With the FSPDA framework, we develop two new algorithms supporting efficient sparsified communication on random time varying topologies --- FSPDA-SA allows agents to execute multiple local gradient steps depending on the time varying topology to accelerate convergence, and FSPDA-STORM further incorporates a variance reduction step to improve sample complexity. For problems with smooth (possibly non-convex) objective function, within $T$ iterations, we show that FSPDA-SA (resp. FSPDA-STORM) finds an $\\mathcal{O}( 1/\\sqrt{T} )$-stationary (resp. $\\mathcal{O}( 1/T^{2/3} )$) solution. Numerical experiments show the benefits of the FSPDA algorithms.",
          "keywords": [
            "Decentralized Optimization",
            "Time Varying Graph",
            "Random Network",
            "Primal-dual Lagrangian",
            "Stochastic Approximation"
          ],
          "primary_area": "optimization",
          "TLDR": "Primal-dual decentralized algorithms incorporating random graph communication through a stochastic Lagrangian formulation.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=6JlzhISecd",
          "pdf_link": "https://openreview.net/pdf?id=6JlzhISecd"
        },
        "paper_internal_id": "6JlzhISecd",
        "category": "reject",
        "embedding_score": 0.6695345640182495,
        "final_score": 0.11849164217710495
      },
      "spotlight": {
        "paper": {
          "id": "V8dGVO5Xpg",
          "title": "Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training",
          "abstract": "Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training.\nMulti-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. \nHowever, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions.\nIn this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions.\n1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions.\n2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ($\\mathcal{O}(T^{\\frac{c\\beta}{c\\beta +1}}/{n m})$ in centralized and  $\\mathcal{O}(T^{\\frac{2c\\beta}{2c\\beta +2}}/{n m^{\\frac{1}{2c\\beta +2}}})$ in decentralized).\nFurthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings.",
          "keywords": [
            "stability and generalization; DSGD; Multiple Gossip Steps; generalization error; excess error"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-13",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=V8dGVO5Xpg",
          "pdf_link": "https://openreview.net/pdf?id=V8dGVO5Xpg"
        },
        "paper_internal_id": "V8dGVO5Xpg",
        "category": "spotlight",
        "embedding_score": 0.7314466834068298,
        "final_score": 0.22974491119384766
      },
      "oral": {
        "paper": {
          "id": "7AwFJzgIUW",
          "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
          "abstract": "Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines.",
          "keywords": [
            "Low Rank",
            "Adversarial Robustenss",
            "Adversarial Attacks",
            "Rank Adaptive",
            "Computer Vision",
            "Compression"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=7AwFJzgIUW",
          "pdf_link": "https://openreview.net/pdf?id=7AwFJzgIUW"
        },
        "paper_internal_id": "7AwFJzgIUW",
        "category": "oral",
        "embedding_score": 0.686253011226654,
        "final_score": 0.029501628130674362
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "heJ7NRInjs",
      "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive  LLM safeguards",
      "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models—designed to monitor LLM inputs and outputs and block potentially harmful content—has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates\nin two stages: (1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and (2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation\nscenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements. Experiments demonstrate that RSafe matches state-of-the-art guard models using limited amount of public data in both prompt- and response-level harmfulness detection, while achieving superior out-of-distribution generalization on both emerging harmful category and jailbreak attacks. Furthermore, RSafe provides human-readable explanations for its safety judgments for better interpretability. RSafe offers a robust, adaptive, and interpretable solution for LLM safety moderation, advancing the development of reliable safeguards in dynamic real-world environments. Our code is available at https://anonymous.4open.science/r/RSafe-996D.",
      "keywords": "['large language model', 'safety', 'moderation']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "heJ7NRInjs",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "0Y4gjqdvC6",
          "title": "Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency and Preference Matching",
          "abstract": "Nash Learning from Human Feedback (NLHF) is a game-theoretic framework for aligning large language models (LLMs) with human preferences by modeling learning as a two-player zero-sum game. However, using raw preference as the payoff in the game highly limits the potential of the game-theoretic LLM alignment framework.In this paper, we systematically study using what choices of payoff based on the pairwise human preferences can yield desirable alignment properties. We establish necessary and sufficient conditions for Condorcet consistency, diversity through mixed strategies, and Smith consistency. These results provide a theoretical foundation for the robustness of game-theoretic LLM alignment. Further, we show the impossibility of preference matching—i.e., no smooth and learnable mappings of pairwise preferences can guarantee a unique Nash equilibrium that matches a target policy, even under standard assumptions like the Bradley-Terry-Luce (BTL) model. This result highlight the fundamental limitation of game-theoretic LLM alignment.",
          "keywords": [
            "Large Language Models",
            "Preference Alignment",
            "Nash Equilibrium",
            "Nash Learning from Human Feedback"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=0Y4gjqdvC6",
          "pdf_link": "https://openreview.net/pdf?id=0Y4gjqdvC6"
        },
        "paper_internal_id": "0Y4gjqdvC6",
        "category": "reject",
        "embedding_score": 0.6440311670303345,
        "final_score": 0.5178653001785278
      },
      "spotlight": {
        "paper": {
          "id": "TkEdQv0bXB",
          "title": "Hyperbolic Fine-Tuning for Large Language Models",
          "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs.\n   In this study, we investigate the non-Euclidean characteristics of LLMs. \n   Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. \n   Motivated by these observations, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. \n   However, we find that this hyperbolic fine-tuning cannot be achieved through the naive application of exponential and logarithmic maps when the embedding and weight matrices both reside in Euclidean space.\n   To address this technical issue, we introduce hyperbolic low-rank efficient fine-tuning, HypLoRA, which performs low-rank adaptation directly on the hyperbolic manifold, preventing the cancellation effect produced by consecutive exponential and logarithmic maps and thereby preserving hyperbolic modeling capabilities.\n   Extensive experiments across various base models and two different reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance.",
          "keywords": [
            "Large Language Models",
            "Hyperbolic Space",
            "Low-Rank Adaptation",
            "Embedding Space"
          ],
          "primary_area": "deep_learning",
          "TLDR": "A novel method to efficiently fine-tune Large Language Models in hyperbolic space, unlocking their latent tree-like structures and significantly boosting complex reasoning performance by adapting directly on the hyperbolic manifold.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=TkEdQv0bXB",
          "pdf_link": "https://openreview.net/pdf?id=TkEdQv0bXB"
        },
        "paper_internal_id": "TkEdQv0bXB",
        "category": "spotlight",
        "embedding_score": 0.6361258029937744,
        "final_score": 0.49405550956726074
      },
      "oral": {
        "paper": {
          "id": "NM8Apk61NA",
          "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
          "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Code is available at \\url{https://github.com/godlin-sjtu/HyperET}.",
          "keywords": [
            "Efficient Training",
            "Multi-modal Large Language Models",
            "Granularity Levels",
            "Hyperbolic Space"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-18",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=NM8Apk61NA",
          "pdf_link": "https://openreview.net/pdf?id=NM8Apk61NA"
        },
        "paper_internal_id": "NM8Apk61NA",
        "category": "oral",
        "embedding_score": 0.6405396461486816,
        "final_score": 0.41055822372436523
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "M4Laq0Y5WG",
      "title": "Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation",
      "abstract": "In this paper, we propose \\textbf{Jasmine}, the first Stable Diffusion (SD)-based self-supervised framework for monocular depth estimation, which effectively harnesses SD’s visual priors to enhance the sharpness and generalization of unsupervised prediction. Previous SD-based methods are all supervised since adapting diffusion models for dense prediction requires high-precision supervision. In contrast, self-supervised reprojection suffers from inherent challenges (\\textit{e.g.}, occlusions, texture-less regions, illumination variance), and the predictions exhibit blurs and artifacts that severely compromise SD's latent priors. To resolve this, we construct a novel surrogate task of mix-batch image reconstruction. Without any additional supervision, it preserves the detail priors of SD models by reconstructing the images themselves while preventing depth estimation from degradation. Furthermore, to address the inherent misalignment between SD's scale and shift invariant estimation and self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU. It not only bridges this distribution gap but also isolates the fine-grained texture of SD output against the interference of reprojection loss. Extensive experiments demonstrate that Jasmine achieves SoTA performance on the KITTI benchmark and exhibits superior zero-shot generalization across multiple datasets.",
      "keywords": "['depth estimation', 'self-supervision', 'diffusion models', 'generative model']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "M4Laq0Y5WG",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "csjryswwao",
          "title": "SyntheOcc: Synthesize Occupancy-Controlled Street View Images through 3D Semantic MPIs",
          "abstract": "The advancement of autonomous driving is increasingly reliant on high-quality annotated datasets, especially in the task of 3D occupancy prediction, where the occupancy labels require dense 3D annotation with significant human effort. In this paper, we propose SyntheOcc, which denotes a diffusion model that Synthesize photorealistic and geometric controllable images by conditioning Occupancy labels in driving scenarios. This yields an unlimited amount of diverse, annotated, and controllable datasets for applications like training perception models and simulation. SyntheOcc addresses the critical challenge of how to efficiently encode 3D geometric information as conditional input to a 2D diffusion model. Our approach innovatively incorporates 3D semantic multi-plane images (MPIs) to provide comprehensive and spatially aligned 3D scene descriptions for conditioning. By doing so, SyntheOcc can generate photorealistic multi-view images and videos that faithfully align with the given geometric labels (semantics in 3D voxel space). Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes dataset prove its effectiveness in generating controllable occupancy datasets that serve as an effective data augmentation to perception models.",
          "keywords": [
            "Autonomous Driving",
            "Generative Model",
            "Image and Video Generation",
            "Data-centric AI",
            "3D Vision"
          ],
          "primary_area": "applications",
          "TLDR": "Geometric Controlled Generation Framework for Autonomous Driving. High-quality 3D controllable videogeneration benefit various downstream tasks.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=csjryswwao",
          "pdf_link": "https://openreview.net/pdf?id=csjryswwao"
        },
        "paper_internal_id": "csjryswwao",
        "category": "reject",
        "embedding_score": 0.7049331665039062,
        "final_score": 0.3974737823009491
      },
      "spotlight": {
        "paper": {
          "id": "Lf0W2gmNBg",
          "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
          "abstract": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose \\textbf{EAG3R}, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
          "keywords": [
            "Event Camera",
            "3D Vision",
            "Neuromorphic Computing"
          ],
          "primary_area": "applications",
          "TLDR": "EAG3R synergistically integrates standard RGB video frames with asynchronous event streams to achieve robust 3D geometry estimation in dynamic and extreme-lighting scenes where conventional RGB-only approaches struggle.",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=Lf0W2gmNBg",
          "pdf_link": "https://openreview.net/pdf?id=Lf0W2gmNBg"
        },
        "paper_internal_id": "Lf0W2gmNBg",
        "category": "spotlight",
        "embedding_score": 0.6606639623641968,
        "final_score": 0.9077264666557312
      },
      "oral": {
        "paper": {
          "id": "NM8Apk61NA",
          "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
          "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Code is available at \\url{https://github.com/godlin-sjtu/HyperET}.",
          "keywords": [
            "Efficient Training",
            "Multi-modal Large Language Models",
            "Granularity Levels",
            "Hyperbolic Space"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-02",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-18",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=NM8Apk61NA",
          "pdf_link": "https://openreview.net/pdf?id=NM8Apk61NA"
        },
        "paper_internal_id": "NM8Apk61NA",
        "category": "oral",
        "embedding_score": 0.6397845149040222,
        "final_score": 0.05580514669418335
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "gkcU26BOml",
      "title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect",
      "abstract": "Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like ‘bouba’ with round shapes and ‘kiki’ with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as a measure of model preference, and we use Grad-CAM as a novel approach to interpret visual attention in shape-word matching tasks. Our findings show that these model variants do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both model variants lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models’ responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions.",
      "keywords": "['Cross-modal associations', 'Vision-and-Language Models', 'bouba-kiki effect', 'Cognitive science']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "gkcU26BOml",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "dZRCZUvKPj",
          "title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness",
          "abstract": "Recent work shows that increasing inference-time compute through generation of long reasoning traces improves not just capability scores, but robustness to various text jailbreaks designed to control models or lower their guardrails. However, multimodal reasoning offers comparatively little defense against vision jailbreaks, which typically succeed by creating noise-like perturbations. When attacking a robust model, vision attacks are also capable of and often must resort to producing human-interpretable perturbations. Rather than operating in a model's blind-spot or out of its training distribution, such interpretable attacks construct familiar concepts connected to the attacker's goal. Inspired by the ability of robust models to force attacks into this space that appears more in-distribution for reasoning tasks, we posit the Robustness from Inference Compute Hypothesis (RICH): defending against attacks with inference compute (like reasoning) profits as those attacks become more in-distribution. To test this, we adversarially attack models of varying robustness with black-box-transfer and white-box attacks. RICH predicts a rich-get-richer dynamic: models that start with higher initial robustness gain more robustness benefits from increases in inference-time compute. Consistent with RICH, we find that robust models benefit more from increased compute, whereas non-robust models show little to no improvement. Our work suggests that inference-time compute can be an effective defense against adversarial attacks, provided the base model has some degree of robustness. In particular, layering disparate train-time and test-time defenses aids robustness not additively, but synergistically.",
          "keywords": [
            "VLMs",
            "robustness",
            "adversarial attacks",
            "reasoning",
            "scaling",
            "efficiency"
          ],
          "primary_area": "deep_learning",
          "TLDR": "Scaling inference-time compute to improve robustness is more beneficial as you improve base model robustness.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=dZRCZUvKPj",
          "pdf_link": "https://openreview.net/pdf?id=dZRCZUvKPj"
        },
        "paper_internal_id": "dZRCZUvKPj",
        "category": "reject",
        "embedding_score": 0.6856155395507812,
        "final_score": 0.06584816426038742
      },
      "spotlight": {
        "paper": {
          "id": "RnXS7aK4rK",
          "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
          "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a 3D spatial encoder—initialized from the backbone of the visual geometry model—to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct a training dataset from multiple sources and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that Spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks.",
          "keywords": [
            "Spatial intelligence",
            "Multimodal large language model",
            "3D Understanding"
          ],
          "primary_area": "deep_learning",
          "TLDR": "In this paper, we present Spatial-MLLM, a novel framework for spatial understanding and reasoning with only 2D inputs.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=RnXS7aK4rK",
          "pdf_link": "https://openreview.net/pdf?id=RnXS7aK4rK"
        },
        "paper_internal_id": "RnXS7aK4rK",
        "category": "spotlight",
        "embedding_score": 0.72024005651474,
        "final_score": 0.7414892911911011
      },
      "oral": {
        "paper": {
          "id": "qYkhCah8OZ",
          "title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
          "abstract": "Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. \nHowever, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR.",
          "keywords": [
            "Multimodal Large Language Models",
            "Multimodal Retrieval Augmented Generation"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=qYkhCah8OZ",
          "pdf_link": "https://openreview.net/pdf?id=qYkhCah8OZ"
        },
        "paper_internal_id": "qYkhCah8OZ",
        "category": "oral",
        "embedding_score": 0.7248292565345764,
        "final_score": 0.5219770073890686
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "WOzffPgVjF",
      "title": "Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding",
      "abstract": "Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel $\\textbf{T}$arget-$\\textbf{A}$ware Transformer for $\\textbf{STVG}$ ($\\textbf{TA-STVG}$), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. Moreover, TTS and ASA are designed for general purpose. When applied to existing methods such as TubeDETR and STCAT, we show substantial performance gains, verifying its generality. Code is released at https://github.com/HengLan/TA-STVG.",
      "keywords": "['Spatio-Temporal Video Grounding']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "WOzffPgVjF",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "xSOl0s1u77",
          "title": "TC-Bench: Benchmarking Temporal Compositionality in Conditional Video Generation",
          "abstract": "Video generation has many unique challenges beyond those of image generation. The temporal dimension introduces extensive possible variations across frames, over which consistency and continuity may be violated. In this study, we move beyond evaluating simple actions and argue that generated videos should incorporate the emergence of new concepts and their relation transitions like in real-world videos as time progresses. To assess the \\textbf{T}emporal \\textbf{C}ompositionality of video generation models, we propose TC-Bench, a benchmark of meticulously crafted text prompts, corresponding ground truth videos, and robust evaluation metrics. The prompts articulate the initial and final states of scenes, effectively reducing ambiguities for frame development and simplifying the assessment of transition completion. In addition, by collecting aligned real-world videos corresponding to the prompts, we expand TC-Bench's applicability from text-conditional models to image-conditional ones that can perform generative frame interpolation. We also develop new metrics to measure the completeness of component transitions in generated videos, which demonstrate significantly higher correlations with human judgments than existing metrics. Our comprehensive experimental results reveal that most video generators achieve less than ～20% of the compositional changes, highlighting enormous space for future improvement. Our analysis indicates that current video generation models struggle to interpret descriptions of compositional changes and dynamically map varied semantics across different time steps.",
          "keywords": [
            "Video Generation Benchmark; Text-to-Video Generation; Compositional Video Generation"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "We propose a new benchmark suite to evaluate temporal compositionality for conditional video generation",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=xSOl0s1u77",
          "pdf_link": "https://openreview.net/pdf?id=xSOl0s1u77"
        },
        "paper_internal_id": "xSOl0s1u77",
        "category": "reject",
        "embedding_score": 0.7577121257781982,
        "final_score": 0.733794093132019
      },
      "poster": {
        "paper": {
          "id": "14fFV0chUS",
          "title": "TRACE: Temporal Grounding Video LLM  via Causal Event Modeling",
          "abstract": "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. \nTo effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents video LLM outputs as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. \nThe TRACE process visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation.\nExtensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are avaliable at \\url{https://github.com/gyxxyg/TRACE}.",
          "keywords": [
            "video large language model",
            "video temporal grounding"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "This paper aims to address the mismatch between video structure and video LLMs on video temporal grounding tasks,  and propose a causal event modeling framework and the TRACE model as a solution.",
          "creation_date": "2024-09-23",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=14fFV0chUS",
          "pdf_link": "https://openreview.net/pdf?id=14fFV0chUS"
        },
        "paper_internal_id": "14fFV0chUS",
        "category": "poster",
        "embedding_score": 0.7938706874847412,
        "final_score": 0.9299342036247253
      },
      "spotlight": {
        "paper": {
          "id": "auZZ2gN0ZN",
          "title": "Dense Video Object Captioning from Disjoint Supervision",
          "abstract": "We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic.",
          "keywords": [
            "object captioning",
            "video",
            "tracking"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=auZZ2gN0ZN",
          "pdf_link": "https://openreview.net/pdf?id=auZZ2gN0ZN"
        },
        "paper_internal_id": "auZZ2gN0ZN",
        "category": "spotlight",
        "embedding_score": 0.8086330890655518,
        "final_score": 0.8573971390724182
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "HD6bWcj87Y",
      "title": "Data Shapley in One Training Run",
      "abstract": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",
      "keywords": "['Shapley value', 'data valuation.']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "HD6bWcj87Y",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "BQgAToASdX",
          "title": "Generalized Group Data Attribution",
          "abstract": "Data Attribution (DA) methods quantify the influence of individual training data points on model outputs and have broad applications such as explainability, data selection, and noisy label identification. However, existing DA methods are often computationally intensive, limiting their applicability to large-scale machine learning models. To address this challenge, we introduce the Generalized Group Data Attribution (GGDA) framework, which computationally simplifies DA by attributing to groups of training points instead of individual ones. GGDA is a general framework that subsumes existing attribution methods and can be applied to new DA techniques as they emerge. It allows users to optimize the trade-off between efficiency and fidelity based on their needs. Our empirical results demonstrate that GGDA applied to popular DA methods such as Influence Functions, TracIn, and TRAK results in upto 10x-50x speedups over standard DA methods while gracefully trading off attribution fidelity. For downstream applications such as dataset pruning and noisy label identification, \nwe demonstrate that GGDA significantly improves computational efficiency and maintains effectiveness, enabling practical applications in large-scale machine learning scenarios that were previously infeasible.",
          "keywords": [
            "generalized",
            "group",
            "data attribution",
            "efficiency",
            "training data",
            "influence",
            "tracin",
            "trak"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We speed up existing data attribution methods by orders of magnitude while gracefully trading off attribution fidelity.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=BQgAToASdX",
          "pdf_link": "https://openreview.net/pdf?id=BQgAToASdX"
        },
        "paper_internal_id": "BQgAToASdX",
        "category": "reject",
        "embedding_score": 0.7041870355606079,
        "final_score": 0.88062983751297
      },
      "poster": {
        "paper": {
          "id": "Y5LjYI4N6P",
          "title": "Efficient stagewise pretraining via progressive subnetworks",
          "abstract": "Recent developments in large language models have sparked interest in efficient\npretraining methods. Stagewise training approaches to improve efficiency, like\ngradual stacking and layer dropping (Reddi et al., 2023; Zhang & He, 2020), have\nrecently garnered attention. The prevailing view suggests that stagewise dropping\nstrategies, such as layer dropping, are ineffective, especially when compared to\nstacking-based approaches. This paper challenges this notion by demonstrating\nthat, with proper design, dropping strategies can be competitive, if not better, than\nstacking methods. Specifically, we develop a principled stagewise training framework, progressive subnetwork training, which only trains subnetworks within the\nmodel and progressively increases the size of subnetworks during training, until it\ntrains the full network. We propose an instantiation of this framework — Random\nPart Training (RAPTR) — that selects and trains only a random subnetwork (e.g.\ndepth-wise, width-wise) of the network at each step, progressively increasing the\nsize in stages. We show that this approach not only generalizes prior works like\nlayer dropping but also fixes their key issues. Furthermore, we establish a theoretical basis for such approaches and provide justification for (a) increasing complexity of subnetworks in stages, conceptually diverging from prior works on layer\ndropping, and (b) stability in loss across stage transitions in presence of key modern architecture components like residual connections and layer norms. Through\ncomprehensive experiments, we demonstrate that RAPTR can significantly speed\nup training of standard benchmarks like BERT and UL2, up to 33% compared to\nstandard training and, surprisingly, also shows better downstream performance on\nUL2, improving QA tasks and SuperGLUE by 1.5%; thereby, providing evidence\nof better inductive bias.",
          "keywords": [
            "Efficient stagewise training",
            "modular training",
            "language model pretraining",
            "implicit bias",
            "simple-to-complex learning"
          ],
          "primary_area": "optimization",
          "TLDR": "We propose progressive subnetwork training for efficient pre-training of large language models, which improves downstream performance over existing stagewise pre-training methods.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=Y5LjYI4N6P",
          "pdf_link": "https://openreview.net/pdf?id=Y5LjYI4N6P"
        },
        "paper_internal_id": "Y5LjYI4N6P",
        "category": "poster",
        "embedding_score": 0.6989902257919312,
        "final_score": 0.8499223589897156
      },
      "spotlight": {
        "paper": {
          "id": "P42DbV2nuV",
          "title": "Instance-dependent Early Stopping",
          "abstract": "In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.",
          "keywords": [
            "Early Stopping",
            "Supervised Learning",
            "Deep Learning",
            "Efficiency",
            "Sample Selection",
            "Data Pruning"
          ],
          "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
          "TLDR": "We propose an instance-dependent early stopping method that stops training at the instance level by determining whether the model has fully learned an instance. It reduces computational costs while maintaining or even improving model performance.",
          "creation_date": "2024-09-13",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-11",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=P42DbV2nuV",
          "pdf_link": "https://openreview.net/pdf?id=P42DbV2nuV"
        },
        "paper_internal_id": "P42DbV2nuV",
        "category": "spotlight",
        "embedding_score": 0.7086412310600281,
        "final_score": 0.6989132761955261
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "FDnZFpHmU4",
      "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
      "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$ \\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \\textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.",
      "keywords": "['Model ensembling', 'LLM']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "FDnZFpHmU4",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "qLxkXgmWwx",
          "title": "Investigating Factuality in Long-Form Text Generation: The Roles of Self-Known and Self-Unknown",
          "abstract": "Large language models (LLMs) have demonstrated strong capabilities in text understanding and generation. However, they often lack factuality, producing a mixture of true and false information, especially in long-form generation. In this work, we investigates the factuality of long-form text generation across various large language models (LLMs), including GPT-4, Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality scores tend to decline in later sentences of the generated text, accompanied by a rise in the number of unsupported claims.\nFurthermore, we explore the effectiveness of different evaluation settings to assess whether LLMs can accurately judge the correctness of their own outputs: Self-Known (the percentage of supported atomic claims, decomposed from LLM outputs, that the corresponding LLMs judge as correct) and Self-Unknown (the percentage of unsupported atomic claims that the corresponding LLMs judge as incorrect). The results indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail to achieve perfect Self-Known scores, while their Self-Unknown scores remain notably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved factuality, while higher Self-Unknown scores are associated with lower factuality. Interestingly, even without significant changes in the models' self-judgment (Self-Known and Self-Unknown), the number of unsupported claims can increases, likely as an artifact of long-form generation. These findings show the limitations of current LLMs in long-form generation, and provide valuable insights for improving factuality in long-form text generation.",
          "keywords": [
            "long-form generation",
            "Factuality"
          ],
          "primary_area": "generative models",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=qLxkXgmWwx",
          "pdf_link": "https://openreview.net/pdf?id=qLxkXgmWwx"
        },
        "paper_internal_id": "qLxkXgmWwx",
        "category": "reject",
        "embedding_score": 0.7363688945770264,
        "final_score": 0.9365604519844055
      },
      "poster": {
        "paper": {
          "id": "TljGdvzFq2",
          "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
          "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term **cross capabilities**. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce *CrossEval*, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that current LLMs consistently exhibit the ``Law of the Weakest Link,'' where cross-capability performance is significantly constrained by the weakest component. Across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight LLMs' underperformance in cross-capability tasks, emphasizing the need to identify and improve their weakest capabilities as a key research priority. The code, benchmarks, and evaluations are available on our [project website](https://www.llm-cross-capabilities.org).",
          "keywords": [
            "Cross Capability",
            "Law of the Weakest Link",
            "Evaluation",
            "Large Langauge Models",
            "Benchmark"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "We define and benchmark cross capabilities in LLMs, revealing the \"Law of the Weakest Link\": collaborative performance is significantly constrained by the weakest individual capability.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=TljGdvzFq2",
          "pdf_link": "https://openreview.net/pdf?id=TljGdvzFq2"
        },
        "paper_internal_id": "TljGdvzFq2",
        "category": "poster",
        "embedding_score": 0.7817444801330566,
        "final_score": 0.9702799916267395
      },
      "oral": {
        "paper": {
          "id": "f4gF6AIHRy",
          "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
          "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Source code\nis available at: https://github.com/MediaBrain-SJTU/DiSF.git.",
          "keywords": [
            "file selection",
            "large language model",
            "pre-training",
            "submodular optimization"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=f4gF6AIHRy",
          "pdf_link": "https://openreview.net/pdf?id=f4gF6AIHRy"
        },
        "paper_internal_id": "f4gF6AIHRy",
        "category": "oral",
        "embedding_score": 0.7843008041381836,
        "final_score": 0.8882801532745361
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "hUb2At2DsQ",
      "title": "Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach",
      "abstract": "As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextual information, inducing severe hallucination of formal definitions and theorems. To address the first issue, we propose **BEq** (_**B**idirectional **E**xtended Definitional E**q**uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition. For the second, we propose **RAutoformalizer** (_**R**etrieval-augmented **Autoformalizer**_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries. We parse the dependencies of libraries and propose to _structurally informalise_ formal objects by the topological order of dependencies. To evaluate OOD generalization and research-level capabilities, we build a novel benchmark, _Con-NF_, consisting of 961 informal-formal statement pairs from frontier mathematical researches. Experiments validate the effectiveness of our approaches: BEq is evaluated on 200 diverse formal statement pairs with expert-annotated equivalence label, exhibiting significantly improved accuracy ($82.50\\\\% \\mapsto 90.50\\\\%$) and precision ($70.59\\\\% \\mapsto 100.0\\\\%$). For dependency retrieval, a strong baseline is devised. Our RAutoformalizer substantially outperforms SOTA baselines in both in-distribution ProofNet benchmark ($12.83\\\\% \\mapsto 18.18\\\\%$, BEq@8) and OOD Con-NF scenario ($4.58\\\\%\\mapsto 16.86\\\\%$, BEq@8).",
      "keywords": "['Large Language Model', 'Formal Verification', 'Autoformalization']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "hUb2At2DsQ",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "NPDnRLFhc0",
          "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers",
          "abstract": "We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important stage when humans write systematic reviews about certain scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. Our pipeline's value and accuracy is validated by teams of human experts. We evaluate a diverse set of language models and retrieval systems on the benchmark and find the performance of the best models still falls significantly short of expert-level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to faciliate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench",
          "keywords": [
            "Biomedical Benchmark",
            "Scientific Information Retrieval",
            "Scientific Information Extraction",
            "Large Language Models",
            "BioNLP"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "A large-scale BioNLP benchmark for evaluating and training models on the task of Evidence Retrieval for hypotheses",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=NPDnRLFhc0",
          "pdf_link": "https://openreview.net/pdf?id=NPDnRLFhc0"
        },
        "paper_internal_id": "NPDnRLFhc0",
        "category": "reject",
        "embedding_score": 0.703662633895874,
        "final_score": 0.339905709028244
      },
      "poster": {
        "paper": {
          "id": "B5RrIFMqbe",
          "title": "FormalAlign: Automated Alignment Evaluation for Autoformalization",
          "abstract": "Autoformalization aims to convert informal mathematical proofs into machine-verifiable formats, bridging the gap between natural and formal languages. However, ensuring semantic alignment between the informal and formalized statements remains challenging. Existing approaches heavily rely on manual verification, hindering scalability. To address this, we introduce FormalAlign, a framework for automatically evaluating the alignment between natural and formal languages in autoformalization. FormalAlign trains on both the autoformalization sequence generation task and the representational alignment between input and output, employing a dual loss that combines a pair of mutually enhancing autoformalization and alignment tasks. Evaluated across four benchmarks augmented by our proposed misalignment strategies, FormalAlign demonstrates superior performance. In our experiments, FormalAlign outperforms GPT-4, achieving an Alignment-Selection Score 11.58\\% higher on \\forml-Basic (99.21\\% vs. 88.91\\%) and 3.19\\% higher on MiniF2F-Valid (66.39\\% vs. 64.34\\%). This effective alignment evaluation significantly reduces the need for manual verification.",
          "keywords": [
            "Large Language models",
            "Autoformalization",
            "Lean 4",
            "Formal Math",
            "AI for Math"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "FormalAlign is a framework that automatically evaluates the alignment between informal and formal mathematical proofs, significantly improving performance and reducing reliance on manual verification.",
          "creation_date": "2024-09-20",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=B5RrIFMqbe",
          "pdf_link": "https://openreview.net/pdf?id=B5RrIFMqbe"
        },
        "paper_internal_id": "B5RrIFMqbe",
        "category": "poster",
        "embedding_score": 0.8733752369880676,
        "final_score": 0.4499545991420746
      },
      "oral": {
        "paper": {
          "id": "WJaUkwci9o",
          "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
          "abstract": "Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.",
          "keywords": [
            "Learning theory",
            "Sample complexity",
            "Self-Improvement",
            "Language Models"
          ],
          "primary_area": "learning theory",
          "TLDR": "We offer a new theoretical perspective on the possibility of self-improvement in language models.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-15",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=WJaUkwci9o",
          "pdf_link": "https://openreview.net/pdf?id=WJaUkwci9o"
        },
        "paper_internal_id": "WJaUkwci9o",
        "category": "oral",
        "embedding_score": 0.7166182994842529,
        "final_score": 0.25949719548225403
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "UWdPsY7agk",
      "title": "Efficient Causal Decision Making with One-sided Feedback",
      "abstract": "We study a class of decision-making problems with one-sided feedback, where outcomes are only observable for specific actions. A typical example is bank loans, where the repayment status is known only if a loan is approved and remains undefined if rejected. In such scenarios, conventional approaches to causal decision evaluation and learning from observational data are not directly applicable. In this paper, we introduce a novel value function to evaluate decision rules that addresses the issue of undefined counterfactual outcomes. Without assuming no unmeasured confounders, we establish the identification of the value function using shadow variables. Furthermore, leveraging semiparametric theory, we derive the efficiency bound for the proposed value function and develop efficient methods for decision evaluation and learning. Numerical experiments and a real-world data application demonstrate the empirical performance of our proposed methods.",
      "keywords": "['semiparametric efficiency', 'one-sided feedback', 'causal decision making']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "UWdPsY7agk",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "en3NwykrHW",
          "title": "Minimax Optimal Regret Bound for Reinforcement Learning with Trajectory Feedback",
          "abstract": "We study the reinforcement learning (RL) problem with trajectory feedback. The trajectory feedback based reinforcement learning problem, where the learner can only observe the accumulative noised reward along the trajectory, is particularly suitable for the practical scenarios where the agent suffers extensively from querying the reward in each single step. For a finite-horizon Markov Decision Process (MDP) with $S$ states, $A$ actions and a horizon length of $H$, we develop an algorithm that enjoys an optimal regret of $\\tilde{O}\\left(\\sqrt{SAH^3K}\\right)$ in $K$ episodes for sufficiently large $K$. To achieve this, our technical contributions are two-fold: (1) we incorporate reinforcement learning with linear bandits problem to construct a tighter confidence region for the reward function; (2) we construct a reference transition model to better guide the exploration process.",
          "keywords": [
            "Reinforcement learning theory",
            "regret analysis",
            "trajectory feedback"
          ],
          "primary_area": "learning theory",
          "TLDR": "We prove a nearly optimal regret bound for reinforcement learning with trajectory feedback.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=en3NwykrHW",
          "pdf_link": "https://openreview.net/pdf?id=en3NwykrHW"
        },
        "paper_internal_id": "en3NwykrHW",
        "category": "reject",
        "embedding_score": 0.6773113012313843,
        "final_score": 0.3231036067008972
      },
      "spotlight": {
        "paper": {
          "id": "4FVGowGzQb",
          "title": "Learning from negative feedback, or positive feedback or both",
          "abstract": "Existing preference optimization methods often assume scenarios where paired preference feedback (preferred/positive vs. dis-preferred/negative examples) is available. This requirement limits their applicability in scenarios where only unpaired feedback—for example, either positive or negative— is available. To address this, we introduce a novel approach that decouples learning from positive and negative feedback. This decoupling enables control over the influence of each feedback type and, importantly, allows learning even when only one feedback type is present. A key contribution is demonstrating stable learning from negative feedback alone, a capability not well-addressed by current methods. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which uses expectation-maximization (EM) to directly optimize the probability of positive outcomes (as opposed to classic expected reward maximization). We address a key limitation in current EM-based methods: they solely maximize the likelihood of positive examples, while neglecting negative ones. We show how to extend EM algorithms to explicitly incorporate negative examples, leading to a theoretically grounded algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned value functions are available.",
          "keywords": [
            "Preference Optimization",
            "Policy Optimization",
            "Negative Feedback",
            "Positive feedback",
            "Reinforcement Learning",
            "Probabilistic Inference"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "A new policy optimization algorithm that learns from different type and number of feedback (positive, negative, or both) to optimize policies.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=4FVGowGzQb",
          "pdf_link": "https://openreview.net/pdf?id=4FVGowGzQb"
        },
        "paper_internal_id": "4FVGowGzQb",
        "category": "spotlight",
        "embedding_score": 0.7343587875366211,
        "final_score": 0.4505947530269623
      },
      "oral": {
        "paper": {
          "id": "stUKwWBuBm",
          "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
          "abstract": "A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \\emph{all} $n$-player matrix and finite-horizon Markov games.  In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality.  To validate the expressivity of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model. We validate our findings on a simple multi-agent reinforcement learning benchmark. Our results open the doors for to the principled development of new decentralized multi-agent reinforcement learning algorithms.",
          "keywords": [
            "behavioral economics",
            "risk-aversion",
            "multi-agent reinforcement learning",
            "quantal response",
            "bounded rationality"
          ],
          "primary_area": "learning theory",
          "TLDR": "By incorporating risk aversion and bounded rationality into agents' decision-making processes, we introduced a computationally tractable equilibria class for matrix and Markov games which aligns with observed human behaviors.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-04-30",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=stUKwWBuBm",
          "pdf_link": "https://openreview.net/pdf?id=stUKwWBuBm"
        },
        "paper_internal_id": "stUKwWBuBm",
        "category": "oral",
        "embedding_score": 0.688524603843689,
        "final_score": 0.03346691280603409
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "vyflgpwfJW",
      "title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
      "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations on data-driven workflows that are not covered in the manually collected split. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.",
      "keywords": "['scientific discovery', 'data-driven discovery', 'data analysis', 'large language models', 'hypothesis generation', 'hypothesis verification']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "vyflgpwfJW",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "lXFGpwtkRl",
          "title": "Improving Model Alignment Through Collective Intelligence of Open-Source Models",
          "abstract": "Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback; this necessitates high-quality human-labeled data. Constructing such datasets is often expensive and not scalable, and may face potential bottleneck on diversity. To address these challenges, we introduce Mixture-of-Agent Alignment (MoAA), an effective approach that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning (SFT) and preference optimization, leading to improved performance compared to using a single model alone, including the state-of-ther-art commercial model. This approach leads to an intriguing direction of model alignment through an scalable and diverse instruction data recipe based on open-sourced models.",
          "keywords": [
            "Model Alignment",
            "Multi-Agent Inference",
            "Large Language Model"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose MoAA that leverages multiple language models to generate diverse, high-quality data for scalable model alignment.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=lXFGpwtkRl",
          "pdf_link": "https://openreview.net/pdf?id=lXFGpwtkRl"
        },
        "paper_internal_id": "lXFGpwtkRl",
        "category": "reject",
        "embedding_score": 0.729692280292511,
        "final_score": 0.8536289930343628
      },
      "spotlight": {
        "paper": {
          "id": "Acvo2RGSCy",
          "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
          "abstract": "The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling *inference-time reasoning*, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.",
          "keywords": [
            "large language models",
            "decision theory",
            "decision making under uncertainty"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We introduce an inference-time reasoning procedure for reliable decision making under uncertainty with LLMs, drawing upon principles from classical decision theory.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=Acvo2RGSCy",
          "pdf_link": "https://openreview.net/pdf?id=Acvo2RGSCy"
        },
        "paper_internal_id": "Acvo2RGSCy",
        "category": "spotlight",
        "embedding_score": 0.786471962928772,
        "final_score": 0.6694164276123047
      },
      "oral": {
        "paper": {
          "id": "mtSSFiqW6y",
          "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
          "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
          "keywords": [
            "LLM inference",
            "speculative decoding"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-27",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mtSSFiqW6y",
          "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y"
        },
        "paper_internal_id": "mtSSFiqW6y",
        "category": "oral",
        "embedding_score": 0.7567310333251953,
        "final_score": 0.6287959218025208
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "PokeChamp: an Expert-level Minimax Language Agent for Competitive Pokemon",
      "abstract": "We introduce \\texttt{Pok\\'eChamp}, a Large Language Model (LLM) powered game-theoretic aware agent for two-player competitive Pok\\'emon battles, that uses an LLM prior and collected high-Elo human data to model minimax search without any additional training. \\texttt{Pok\\'eChamp} uses a depth-limited minimax search online where the LLM replaces three key components: 1) action sampling from the LLM guided by prompts (including from a damage calculation tool), 2) opponent-modeling via the historical likelihood of actions from our dataset to model the effect of LLM-predicted opponent actions, and 3) state value calculation for the LLM to reflect on each intrinsic state. \\texttt{Pok\\'eChamp} outperforms all existing AIs (76\\%) and heuristic bots (84\\%) by an enormous margin, including winning consistently (>50\\%) against prior human-parity work run with a frontier model, GPT 4-o, while using an open-source 8 billion parameter Llama 3.1 model. \\texttt{Pok\\'eChamp} achieves expert performance in the top 10\\% of players on the online ladder against competitive human players at an Elo of 1500. Finally, we collect the largest Pok\\'emon battling dataset, including 1 million+ games with 150k+ high Elo games, prepare a series of battling benchmarks based on real player data and puzzles to analyze specific battling abilities, and provide crucial updates to the local game engine. Our code is available \\href{https://sites.google.com/view/pokechamp-llm}{online}.",
      "keywords": [
        "multiagent",
        "LLM agents",
        "competitive games",
        "game theory",
        "reinforcement learning"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-05",
      "venue": "Submitted to ICLR 2025",
      "forum_link": "https://openreview.net/forum?id=zi8YBcmXqA",
      "pdf_link": "https://openreview.net/pdf?id=zi8YBcmXqA",
      "label": "reject",
      "conference": "ICLR",
      "paper_id": "zi8YBcmXqA"
    },
    "query_internal_id": "zi8YBcmXqA",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "BCP5nAHXqs",
          "title": "Human Simulacra: Benchmarking the Personification of Large Language Models",
          "abstract": "Large Language Models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted the attention of the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters.  We hope this work will serve as a benchmark in the field of human simulation, paving the way for future research.",
          "keywords": [
            "Large Language Models",
            "Human simulation"
          ],
          "primary_area": "datasets and benchmarks",
          "TLDR": "We are the first to introduce a benchmark for the personification of large language models, including high-quality data, rigorous and innovative evaluation methods, and comprehensive benchmark tests.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-16",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=BCP5nAHXqs",
          "pdf_link": "https://openreview.net/pdf?id=BCP5nAHXqs"
        },
        "paper_internal_id": "BCP5nAHXqs",
        "category": "poster",
        "embedding_score": 0.7264003157615662,
        "final_score": 0.9978863596916199
      },
      "spotlight": {
        "paper": {
          "id": "csbf1p8xUq",
          "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale",
          "abstract": "Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and \nwhile some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality responses for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages. We introduce **X-ALMA**, a model designed to ensure top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. After the final stage of training regimen, our proposed **A**daptive **R**ejection **P**reference **O**ptimization (**ARPO**) surpasses existing preference optimization methods in translation tasks.",
          "keywords": [
            "Large Language Model",
            "Machine Translation",
            "Multilingual"
          ],
          "primary_area": "applications to computer vision, audio, language, and other modalities",
          "TLDR": "We present X-ALMA, a multilingual machine translation model that prioritizes quality over quantity by delivering top-tier performance across 50 diverse languages, regardless of their resource levels",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=csbf1p8xUq",
          "pdf_link": "https://openreview.net/pdf?id=csbf1p8xUq"
        },
        "paper_internal_id": "csbf1p8xUq",
        "category": "spotlight",
        "embedding_score": 0.6940181851387024,
        "final_score": 0.9776239395141602
      },
      "oral": {
        "paper": {
          "id": "UHPnqSTBPO",
          "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
          "abstract": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this *selective evaluation* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce *Simulated Annotators*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose *Cascaded Selective Evaluation*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, *guarantees* over 80% human agreement with almost 80% test coverage.",
          "keywords": [
            "Large Language Model",
            "LLM",
            "LLM Judge",
            "Evaluation",
            "Alignment"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "We propose Cascaded Selective Evaluation, an LLM-as-Judge framework that dynamically selects when to trust different judge models to reduce evaluation overhead, while providing a provable guarantee of human-judge agreement.",
          "creation_date": "2024-09-22",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-25",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=UHPnqSTBPO",
          "pdf_link": "https://openreview.net/pdf?id=UHPnqSTBPO"
        },
        "paper_internal_id": "UHPnqSTBPO",
        "category": "oral",
        "embedding_score": 0.6962891817092896,
        "final_score": 0.9787019491195679
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "SWEb: A Large Web Dataset for the Scandinavian Languages",
      "abstract": "This paper presents the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens. The paper details the collection and processing pipeline, and introduces a novel model-based text extractor that significantly reduces complexity in comparison with rule-based approaches. We also introduce a new cloze-style benchmark for evaluating language models in Swedish, and use this test to compare models trained on the SWEb data to models trained on FineWeb, with competitive results. All data, models and code are shared openly.",
      "keywords": [
        "dataset",
        "pre-training",
        "swedish",
        "danish",
        "norwegian",
        "icelandic"
      ],
      "primary_area": "datasets and benchmarks",
      "TLDR": "We present the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens.",
      "creation_date": "2024-09-25",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-03",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=vhPE3PtTgC",
      "pdf_link": "https://openreview.net/pdf?id=vhPE3PtTgC",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "vhPE3PtTgC"
    },
    "query_internal_id": "vhPE3PtTgC",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "asA7vvsgcI",
          "title": "Detecting Training Data of Large Language Models via Expectation Maximization",
          "abstract": "The widespread deployment of large language models (LLMs) has led to impressive advancements, yet information about their training data, a critical factor in their performance, remains undisclosed. Membership inference attacks (MIAs) aim to determine whether a specific instance was part of a target model's training data. MIAs can offer insights into LLM outputs and help detect and address concerns such as data contamination and compliance with privacy and copyright standards. However, applying MIAs to LLMs presents unique challenges due to the massive scale of pre-training data and the ambiguous nature of membership. Additionally, creating appropriate benchmarks to evaluate MIA methods is not straightforward, as training and test data distributions are often unknown. In this paper, we introduce EM-MIA, a novel MIA method for LLMs that iteratively refines membership scores and prefix scores via an expectation-maximization algorithm, leveraging the duality that the estimates of these scores can be improved by each other. Membership scores and prefix scores assess how each instance is likely to be a member and discriminative as a prefix, respectively. Our method achieves state-of-the-art results on the WikiMIA dataset. To further evaluate EM-MIA, we present OLMoMIA, a benchmark built from OLMo resources, which allows us to control the difficulty of MIA tasks with varying degrees of overlap between training and test data distributions. We believe that EM-MIA serves as a robust MIA method for LLMs and that OLMoMIA provides a valuable resource for comprehensively evaluating MIA approaches, thereby driving future research in this critical area.",
          "keywords": [
            "large language models",
            "membership inference attack",
            "data contamination",
            "memorization"
          ],
          "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=asA7vvsgcI",
          "pdf_link": "https://openreview.net/pdf?id=asA7vvsgcI"
        },
        "paper_internal_id": "asA7vvsgcI",
        "category": "reject",
        "embedding_score": 0.6881989240646362,
        "final_score": 0.39997169375419617
      },
      "spotlight": {
        "paper": {
          "id": "1Iuw1jcIrf",
          "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
          "abstract": "Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining  often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline.",
          "keywords": [
            "large language model",
            "mathematical reasoning",
            "continued pretraining"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-16",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-17",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=1Iuw1jcIrf",
          "pdf_link": "https://openreview.net/pdf?id=1Iuw1jcIrf"
        },
        "paper_internal_id": "1Iuw1jcIrf",
        "category": "spotlight",
        "embedding_score": 0.6802423596382141,
        "final_score": 0.4350196421146393
      },
      "oral": {
        "paper": {
          "id": "vf5aUZT0Fz",
          "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
          "abstract": "Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",
          "keywords": [
            "Decentralized Training",
            "Federated Learning",
            "Multi-domain Training",
            "Multilingual Training"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We propose DEPT, a pre-training framework that decouples embedding layers from the transformer body, enabling robust training on heterogeneous data, improving generalization, and reducing memory footprint.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=vf5aUZT0Fz",
          "pdf_link": "https://openreview.net/pdf?id=vf5aUZT0Fz"
        },
        "paper_internal_id": "vf5aUZT0Fz",
        "category": "oral",
        "embedding_score": 0.7137613296508789,
        "final_score": 0.1496478021144867
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression",
      "abstract": "We analyze two variants of Local Gradient Descent applied to distributed logistic regression with heterogeneous, separable data and show convergence at the rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication rounds. In contrast, all existing convergence guarantees for Local GD applied to any problem are at least $\\Omega(1/R)$, meaning they fail to show the benefit of local updates. The key to our improved guarantee is showing progress on the logistic regression objective when using a large stepsize $\\eta \\gg 1/K$, whereas prior analysis depends on $\\eta \\leq 1/K$.",
      "keywords": [
        "optimization",
        "convex optimization",
        "distributed optimization",
        "federated learning",
        "logistic regression"
      ],
      "primary_area": "optimization",
      "TLDR": "This paper provides an improved analysis for local gd which proves the benefit of local steps for heterogeneous logistic regression.",
      "creation_date": "2024-09-27",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=lydPkW4lfz",
      "pdf_link": "https://openreview.net/pdf?id=lydPkW4lfz",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "lydPkW4lfz"
    },
    "query_internal_id": "lydPkW4lfz",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "mscnV6JZkT",
          "title": "Distributed Gradient Descent with Many Local Steps in Overparameterized Models",
          "abstract": "In distributed training of machine learning models, gradient descent with local iterative steps is a very popular method, variants of which are commonly known as Local-SGD or the Federated Averaging (FedAvg). In this method, gradient steps based on local datasets are taken independently in distributed compute nodes to update  the local models, which are then aggregated intermittently. Although the existing convergence analysis suggests that with heterogeneous data, FedAvg encounters quick performance degradation as the number of local steps increases, it is shown to work quite well in practice, especially in the distributed training of large language models. In this work we try to explain this good performance from a viewpoint of implicit bias in Local Gradient Descent (Local-GD) with a large number of local steps. In overparameterized regime, the gradient descent at each compute node would lead the model to a specific direction locally. We characterize the dynamics of the aggregated global model and compare it to the centralized model trained with all of the data in one place. In particular, we analyze the implicit bias of gradient descent on linear models, for both regression and classification tasks. Our analysis shows that the aggregated global model  converges exactly to the centralized model for regression tasks, and converges (in direction) to the same feasible set as centralized model  for classification tasks. We further propose a Modified Local-GD with a refined aggregation and theoretically show it converges to the centralized model in direction for linear classification. We empirically verified our theoretical findings in linear models and also conducted experiments on distributed fine-tuning of pretrained neural networks to further apply our theory.",
          "keywords": [
            "Distributed Learning",
            "Overparameterization",
            "Optimization",
            "Federated Learning"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=mscnV6JZkT",
          "pdf_link": "https://openreview.net/pdf?id=mscnV6JZkT"
        },
        "paper_internal_id": "mscnV6JZkT",
        "category": "reject",
        "embedding_score": 0.8182839751243591,
        "final_score": 0.8615926504135132
      },
      "spotlight": {
        "paper": {
          "id": "SOd07Qxkw4",
          "title": "Improved Convergence Rate for Diffusion Probabilistic Models",
          "abstract": "Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of $d^{1/3}\\varepsilon^{-2/3}$, which is better than $d^{5/12}\\varepsilon^{-1}$, the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling (Shen & Lee, 2019), and then extended to diffusion models by Gupta et al. (2024). Our theory accommodates $\\varepsilon$-accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only $O(\\log^2(d/\\varepsilon))$ parallel rounds in a similar way to prior works.",
          "keywords": [
            "score-based generative model",
            "diffusion model",
            "probability flow ODE",
            "randomized learning rate"
          ],
          "primary_area": "learning theory",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-16",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=SOd07Qxkw4",
          "pdf_link": "https://openreview.net/pdf?id=SOd07Qxkw4"
        },
        "paper_internal_id": "SOd07Qxkw4",
        "category": "spotlight",
        "embedding_score": 0.7595070600509644,
        "final_score": 0.10513724386692047
      },
      "oral": {
        "paper": {
          "id": "sbG8qhMjkZ",
          "title": "Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent",
          "abstract": "We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\\KSD$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant 'negative part' proportional to $N$ times the expected $\\KSD^2$ and a smaller 'positive part'. This observation leads to $\\KSD$ rates of order $1/\\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Mat\\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.",
          "keywords": [
            "Stein Variational Gradient Descent",
            "Non-asymptotic Rates",
            "Variational Inference"
          ],
          "primary_area": "learning theory",
          "TLDR": "Near-optimal finite-particle, discrete-time rates for SVGD",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-29",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=sbG8qhMjkZ",
          "pdf_link": "https://openreview.net/pdf?id=sbG8qhMjkZ"
        },
        "paper_internal_id": "sbG8qhMjkZ",
        "category": "oral",
        "embedding_score": 0.725293755531311,
        "final_score": 0.53611820936203
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Rational Decision-Making Agent with Learning Internal Utility Judgment",
      "abstract": "With remarkable advancements, large language models (LLMs) have attracted significant efforts to develop LLM-based agents capable of executing intricate multi-step decision-making tasks. Existing approaches predominantly build upon the external performance measure to guide the decision-making process but the reliance on the external performance measure as prior is problematic in real-world scenarios, where such prior may be unavailable, flawed, or even erroneous. For genuine autonomous decision-making for LLM-based agents, it is imperative to develop rationality from their posterior experiences to judge the utility of each decision independently. In this work, we propose RaDAgent (Rational Decision-Making Agent), which fosters the development of its rationality through an iterative framework involving Experience Exploration and Utility Learning. Within this framework, Elo-based Utility Learning is devised to assign Elo scores to individual decision steps to judge their utilities via pairwise comparisons. Consequently, these Elo scores guide the decision-making process to derive optimal outcomes. Experimental results on the Game of 24, WebShop, ToolBench and RestBench datasets demonstrate RaDAgent’s superiority over baselines, achieving about 7.8% improvement on average. Besides, RaDAgent also can reduce costs (ChatGPT API calls), highlighting its effectiveness and efficiency.",
      "keywords": [
        "Decision Making",
        "Autonomous Agent",
        "Large Lanugage Model",
        "Elo Rating"
      ],
      "primary_area": "generative models",
      "TLDR": "",
      "creation_date": "2024-09-26",
      "original_date": "2024-10-04",
      "modification_date": "2025-02-26",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=GEBkyKZOc4",
      "pdf_link": "https://openreview.net/pdf?id=GEBkyKZOc4",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "GEBkyKZOc4"
    },
    "query_internal_id": "GEBkyKZOc4",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "jwGPmIqE99",
          "title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making",
          "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.",
          "keywords": [
            "LLM Agent",
            "Strategic Decision Making",
            "Markov Decision Making Process"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=jwGPmIqE99",
          "pdf_link": "https://openreview.net/pdf?id=jwGPmIqE99"
        },
        "paper_internal_id": "jwGPmIqE99",
        "category": "reject",
        "embedding_score": 0.8045254349708557,
        "final_score": 0.9527501463890076
      },
      "spotlight": {
        "paper": {
          "id": "K2jOacHUlO",
          "title": "To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts",
          "abstract": "Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model’s internal knowledge. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context to resolve knowledge conflicts. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. \nOur results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs.",
          "keywords": [
            "Large Language Model",
            "Knowledge Conflict",
            "Retrieval Augmented Generation",
            "Confidence Estimation",
            "Reasoning"
          ],
          "primary_area": "interpretability and explainable AI",
          "TLDR": "We benchmark the challenge of ensuring large language models remain situationally faithful to potentially incorrect external information and propose Self-Guided Confidence Reasoning to enhance LLM's reliability.",
          "creation_date": "2024-09-27",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=K2jOacHUlO",
          "pdf_link": "https://openreview.net/pdf?id=K2jOacHUlO"
        },
        "paper_internal_id": "K2jOacHUlO",
        "category": "spotlight",
        "embedding_score": 0.7113261222839355,
        "final_score": 0.9326833486557007
      },
      "oral": {
        "paper": {
          "id": "mMPMHWOdOy",
          "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
          "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.",
          "keywords": [
            "Mathematical Reasoning",
            "Evol-Instruct",
            "Reinforcement Learning"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-01",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=mMPMHWOdOy",
          "pdf_link": "https://openreview.net/pdf?id=mMPMHWOdOy"
        },
        "paper_internal_id": "mMPMHWOdOy",
        "category": "oral",
        "embedding_score": 0.7415398955345154,
        "final_score": 0.9080603718757629
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "title": "Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models",
      "abstract": "With the rapid advancement of test-time compute search strategies to improve the mathematical problem-solving capabilities of large language models (LLMs), the need for building robust verifiers has become increasingly important. However, all these inference strategies rely on existing verifiers originally designed for Best-of-N search, which makes them sub-optimal for tree search techniques at test time. During tree search, existing verifiers can only offer indirect and implicit assessments of partial solutions or under-value prospective intermediate steps, thus resulting in the premature pruning of promising intermediate steps. To overcome these limitations, we propose token-supervised value models (TVMs) -- a new class of verifiers that assign each token a probability that reflects the likelihood of reaching the correct final answer. This new token-level supervision enables TVMs to directly and explicitly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search at test time. Experimental results demonstrate that combining tree-search-based inference strategies with TVMs significantly improves the accuracy of LLMs in mathematical problem-solving tasks, surpassing the performance of existing verifiers.",
      "keywords": [
        "Large Language Models",
        "Mathematical Problem-Solving",
        "Verifiers"
      ],
      "primary_area": "foundation or frontier models, including LLMs",
      "TLDR": "",
      "creation_date": "2024-09-28",
      "original_date": "2024-10-04",
      "modification_date": "2025-03-02",
      "venue": "ICLR 2025 Poster",
      "forum_link": "https://openreview.net/forum?id=6HcnC3pPkp",
      "pdf_link": "https://openreview.net/pdf?id=6HcnC3pPkp",
      "label": "poster",
      "conference": "ICLR",
      "paper_id": "6HcnC3pPkp"
    },
    "query_internal_id": "6HcnC3pPkp",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "7tOc6h8bea",
          "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation",
          "abstract": "Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval increases from 21\\% to 34\\% with 16 samples and math performance on GSM8K improves from 84\\% to 91\\%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74\\% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50–75\\% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.",
          "keywords": [
            "LLMs",
            "inference-time",
            "inference-time efficiency",
            "Best-of-N",
            "self-evaluation"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "Enabling adaptive inference-time compute through capability-aware and mid-generation self-evaluations",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=7tOc6h8bea",
          "pdf_link": "https://openreview.net/pdf?id=7tOc6h8bea"
        },
        "paper_internal_id": "7tOc6h8bea",
        "category": "reject",
        "embedding_score": 0.780516505241394,
        "final_score": 0.5827236175537109
      },
      "spotlight": {
        "paper": {
          "id": "A6Y7AqlzLW",
          "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
          "abstract": "A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure \n *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot.  Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training  **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs.",
          "keywords": [
            "LLM",
            "Math Reasoning",
            "Process Supervision",
            "Reward Models",
            "RL",
            "Search"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-02",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=A6Y7AqlzLW",
          "pdf_link": "https://openreview.net/pdf?id=A6Y7AqlzLW"
        },
        "paper_internal_id": "A6Y7AqlzLW",
        "category": "spotlight",
        "embedding_score": 0.7784304618835449,
        "final_score": 0.41436484456062317
      },
      "oral": {
        "paper": {
          "id": "4FWAwZtd2n",
          "title": "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning",
          "abstract": "Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on performance, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models (PRMs); and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to, as effectively as possible, allocate test-time compute per prompt in an adaptive manner. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling for math reasoning problems by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute  can be used to outperform a 14x larger model.",
          "keywords": [
            "test-time compute",
            "LLMs",
            "scaling",
            "language models"
          ],
          "primary_area": "foundation or frontier models, including LLMs",
          "TLDR": "We find that by optimally scaling test-time compute we can outperform much larger models in a FLOPs matched evaluation.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-03",
          "venue": "ICLR 2025 Oral",
          "forum_link": "https://openreview.net/forum?id=4FWAwZtd2n",
          "pdf_link": "https://openreview.net/pdf?id=4FWAwZtd2n"
        },
        "paper_internal_id": "4FWAwZtd2n",
        "category": "oral",
        "embedding_score": 0.8000040650367737,
        "final_score": 0.7336715459823608
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "s0JVsx3bx1",
      "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance.\nOur experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals.\nEvaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\\times$ -- $50\\times$, outperforming other goal-conditioned baselines.\nIncreasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.",
      "keywords": "['Reinforcement Learning', 'Self-Supervised Learning', 'Contrastive RL', 'Goal-conditioned RL', 'Scaling']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "s0JVsx3bx1",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "tESKKiKhVp",
          "title": "Evolutionary Distributed Training",
          "abstract": "We introduce Evolutionary Distributed Training (EDT), a nature-inspired approach to distributed model training. EDT replaces centralized gradient synchronization with evaluation, pairwise model crossover, and mutation, enabling communication-efficient training across loosely connected devices. While early investigations show limited effectiveness in language model pretraining, EDT demonstrates strong potential in reinforcement learning (RL). In complex multi-agent environments, EDT facilitates diverse reward exploration and emergent strategies by evolving both policy and reward functions, outperforming traditional training in adaptability and strategic diversity. We also hypothesize EDT as a promising framework for post-training and alignment, offering optimization towards multi-objective, non-differentiable goals. This work positions EDT as a scalable, evolutionary recipe for distributed learning, offering early insights into where it may best fit within the deep learning landscape.",
          "keywords": [
            "Large Language Models",
            "Distributed Training",
            "Evolutionary Algorithms",
            "Reinforcement Learning"
          ],
          "primary_area": "deep_learning",
          "TLDR": "An early investigation of evolutionary approach to distributed model training.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=tESKKiKhVp",
          "pdf_link": "https://openreview.net/pdf?id=tESKKiKhVp"
        },
        "paper_internal_id": "tESKKiKhVp",
        "category": "reject",
        "embedding_score": 0.7379088401794434,
        "final_score": 0.4242902100086212
      },
      "poster": {
        "paper": {
          "id": "oEgybA04dY",
          "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
          "abstract": "The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, \nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps—surpassing all previous open-source efforts in scale.\nThis pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.",
          "keywords": [
            "Multimodal LLM",
            "Visual Reasoning",
            "Cognitive Behavior Transfer"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-04-05",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=oEgybA04dY",
          "pdf_link": "https://openreview.net/pdf?id=oEgybA04dY"
        },
        "paper_internal_id": "oEgybA04dY",
        "category": "poster",
        "embedding_score": 0.7340995073318481,
        "final_score": 0.4341130554676056
      },
      "spotlight": {
        "paper": {
          "id": "neZSGqhxDa",
          "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
          "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
          "keywords": [
            "reasoning",
            "language model",
            "reinforcement learning",
            "self-play",
            "LLM"
          ],
          "primary_area": "applications",
          "TLDR": "self-play reasoning RL with no data can achieve SOTA against RL models trained with human data",
          "creation_date": "2025-05-03",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=neZSGqhxDa",
          "pdf_link": "https://openreview.net/pdf?id=neZSGqhxDa"
        },
        "paper_internal_id": "neZSGqhxDa",
        "category": "spotlight",
        "embedding_score": 0.7488428354263306,
        "final_score": 0.4791123867034912
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "eafIjoZAHm",
      "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "abstract": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse $k$-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
      "keywords": "['graph neural network', 'graph machine learning', 'explainability', 'xai', 'global explanation', 'text-based explanation', 'exemplar', 'exemplar theory']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "eafIjoZAHm",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "7pEVq8yN3U",
          "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs",
          "abstract": "Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on *propagating* that knowledge: models cannot answer questions that require them to reason with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach, PropMEND, extends the meta-objective of MEND so that gradient updates on a piece of knowledge are transformed to allow answering of multi-hop questions involving that knowledge.\nOn the RippleEdit dataset, our method significantly improves performance on propagation questions whose answers are not explicitly stated in the injected fact, in contrast to existing methods that only improve on propagation questions where the answer can be copied verbatim.\nTo study the extent of generalization that our propagation achieves, we construct StoryPropagation, a controlled dataset focusing on entities and relations that the model already understands well. We find that PropMEND generalizes effectively to partially unseen entity-relation pairs, indicating the effectiveness of our meta-trained hypernetwork for knowledge propagation.",
          "keywords": [
            "Knowledge Editing",
            "Knowledge Propagation",
            "Entity",
            "Large Language Model"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=7pEVq8yN3U",
          "pdf_link": "https://openreview.net/pdf?id=7pEVq8yN3U"
        },
        "paper_internal_id": "7pEVq8yN3U",
        "category": "reject",
        "embedding_score": 0.7265269756317139,
        "final_score": 0.48634234070777893
      },
      "poster": {
        "paper": {
          "id": "iyFH9KRGBo",
          "title": "Correlation Dimension of Autoregressive Large Language Models",
          "abstract": "Large language models (LLMs) have achieved remarkable progress in natural\nlanguage generation, yet they continue to display puzzling behaviors—such as\nrepetition and incoherence—even when exhibiting low perplexity. This\nhighlights a key limitation of conventional evaluation metrics, which\nemphasize local prediction accuracy while overlooking long-range structural\ncomplexity.  We introduce correlation dimension, a fractal-geometric measure\nof self-similarity, to quantify the epistemological complexity of text as\nperceived by a language model. This measure captures the hierarchical\nrecurrence structure of language, bridging local and global properties in a\nunified framework.  Through extensive experiments, we show that correlation\ndimension (1) reveals three distinct phases during pretraining, (2) reflects\ncontext-dependent complexity, (3) indicates a model's tendency toward\nhallucination, and (4) reliably detects multiple forms of degeneration in\ngenerated text.  The method is computationally efficient, robust to model\nquantization (down to 4-bit precision), broadly applicable across\nautoregressive architectures (e.g., Transformer and Mamba), and provides\nfresh insight into the generative dynamics of LLMs.",
          "keywords": [
            "correlation dimension",
            "fractal dimension",
            "large language models",
            "self-similarity",
            "complexity",
            "degeneration",
            "hallucination",
            "LLM evaluation"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose correlation dimension as a practical, model-agnostic metric that captures structural complexity and detects degeneration in large language model outputs beyond what perplexity reveals.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=iyFH9KRGBo",
          "pdf_link": "https://openreview.net/pdf?id=iyFH9KRGBo"
        },
        "paper_internal_id": "iyFH9KRGBo",
        "category": "poster",
        "embedding_score": 0.7242641448974609,
        "final_score": 0.9365003108978271
      },
      "spotlight": {
        "paper": {
          "id": "WYnvP3DePZ",
          "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks",
          "abstract": "Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$ framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection.",
          "keywords": [
            "Graph Neural Networks",
            "Link Representation",
            "Expressiveness"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=WYnvP3DePZ",
          "pdf_link": "https://openreview.net/pdf?id=WYnvP3DePZ"
        },
        "paper_internal_id": "WYnvP3DePZ",
        "category": "spotlight",
        "embedding_score": 0.7419449090957642,
        "final_score": 0.9976369142532349
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "4xvE6Iy77Y",
      "title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/.",
      "keywords": "['Preference-based Reinforcement Learning', 'Foundation Models for Robotics', 'Neuro-Symbolic Fusion', 'Multimodal Feedback', 'Causal Inference', 'Trajectory Synthesis', 'Robot Manipulation']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "4xvE6Iy77Y",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "vMfJM9oBYL",
          "title": "Learning from Preferences and Mixed Demonstrations in General Settings",
          "abstract": "Reinforcement learning is a general method for learning in sequential settings, but it can often be difficult to specify a good reward function when the task is complex.\nIn these cases, preference feedback or expert demonstrations can be used instead.\nHowever, existing approaches utilising both together are either ad-hoc or rely on domain-specific properties.\nBuilding upon previous work, we develop a mathematical framework for learning from human data and based on this we introduce LEOPARD: Learning Estimated Objectives from Preferences And Ranked Demonstrations.\nLEOPARD can simultaneously learn from a broad range of data, including negative/failed demonstrations, to effectively learn reward functions in general domains.\nIt does this by modelling the human feedback as reward-rational partial orderings over available trajectories.\nWe find that when a limited amount of preference and demonstration feedback is available, LEOPARD outperforms baselines by a significant margin.\nFurthermore, we use LEOPARD to investigate learning from many types of feedback compared to just a single one, and find that a combination of feedback types is often beneficial.",
          "keywords": [
            "reinforcement learning",
            "rl",
            "human feedback",
            "rlhf",
            "modelling",
            "preferences",
            "demonstrations",
            "rankings",
            "machine learning",
            "reward learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "We introduce a principled method for learning reward functions in RL from preferences, and ranked positive and negative demonstrations.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=vMfJM9oBYL",
          "pdf_link": "https://openreview.net/pdf?id=vMfJM9oBYL"
        },
        "paper_internal_id": "vMfJM9oBYL",
        "category": "reject",
        "embedding_score": 0.7493319511413574,
        "final_score": 0.8899009823799133
      },
      "poster": {
        "paper": {
          "id": "mPuOMcN9E7",
          "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
          "abstract": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged—motivated by PbRL’s recent empirical success, particularly in aligning large language models (LLMs)—most existing studies focus only on pairwise comparisons. A few recent works  (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024)  have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve—and can even deteriorate—as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett–Luce (PL) model for ranking feedback over action subsets and propose **M-AUPO**, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that **M-AUPO** achieves a suboptimality gap of $\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter’s norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
          "keywords": [
            "Preference-based Reinforcement Learning",
            "Ranking Feedback",
            "Plackett–Luce Model",
            "Reinforcement Learning from Human Feedback",
            "Dueling Bandit"
          ],
          "primary_area": "theory",
          "TLDR": "We present the first theoretical analysis of PbRL with ranking feedback, showing that longer ranking feedback can provably improve sample efficiency.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-17",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=mPuOMcN9E7",
          "pdf_link": "https://openreview.net/pdf?id=mPuOMcN9E7"
        },
        "paper_internal_id": "mPuOMcN9E7",
        "category": "poster",
        "embedding_score": 0.7351844310760498,
        "final_score": 0.9107862114906311
      },
      "spotlight": {
        "paper": {
          "id": "QtnCPZMxYg",
          "title": "Trajectory Graph Learning: Aligning with Long Trajectories in Reinforcement Learning Without Reward Design",
          "abstract": "Reinforcement learning (RL) often relies on manually designed reward functions, which are difficult to specify and can lead to issues such as reward hacking and suboptimal behavior. Alternatives like inverse RL and preference-based RL attempt to infer surrogate rewards from demonstrations or preferences but suffer from ambiguity and distribution mismatch. A more direct approach, inspired by imitation learning, avoids reward modeling by leveraging expert demonstrations. However, most existing methods align actions only at individual states, failing to capture the coherence of long-horizon trajectories.\n\nIn this work, we study the problem of directly aligning policies with expert-labeled trajectories to preserve long-horizon behavior without relying on reward signals. Specifically, we aim to learn a policy that maximizes the probability of generating the expert trajectories. Nevertheless, we prove that, in its general form, this trajectory alignment problem is NP-complete. \nTo address this, we propose Trajectory Graph Learning (TGL), a framework that leverages structural assumptions commonly satisfied in practice—such as bounded realizability of expert trajectories or a tree-structured MDP. These enable a graph-based policy planning algorithm that computes optimal policies in polynomial time under known dynamics. For settings with unknown dynamics, we develop a sample-efficient algorithm based on UCB-style exploration and establish sub-linear regret. Experiments on grid-world tasks demonstrate that TGL substantially outperforms standard imitation learning methods for long-trajectory planning.",
          "keywords": [
            "Reinforcement Learning",
            "Trajectory Alignment",
            "Trajectory Graph Learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=QtnCPZMxYg",
          "pdf_link": "https://openreview.net/pdf?id=QtnCPZMxYg"
        },
        "paper_internal_id": "QtnCPZMxYg",
        "category": "spotlight",
        "embedding_score": 0.7214760184288025,
        "final_score": 0.6947309970855713
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "F0JzotXYgC",
      "title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "abstract": "A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top-$p$ structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n \\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and norm conditions, our bounds yield sharp estimates for $\\| (A + E)_p - A_p \\|$, where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up to a factor of $\\sqrt{n}$. As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes.",
      "keywords": "['Spectral norm', 'low-rank approximation', 'differentially private PCA', 'contour integration', 'matrix analysis']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "F0JzotXYgC",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "kXdW2KySK5",
          "title": "Variance-Dependent Regret Lower Bounds for Contextual Bandits",
          "abstract": "Variance-dependent regret bounds for linear contextual bandits, which improve upon the classical $\\tilde{O}(d\\sqrt{K})$ regret bound to $\\tilde{O}(d\\sqrt{\\sum_{k=1}^K\\sigma_k^2})$, where $d$ is the context dimension, $K$ is the number of rounds, and $\\sigma^2_k$ is the noise variance in round $k$, has been widely studied in recent years. However, most existing works focus on the regret upper bounds instead of lower bounds. To our knowledge, the only lower bound is from Jia et al. (2024), which proved that for any eluder dimension $d_{\\textbf{elu}}$ and total variance budget $\\Lambda$, there exists an instance with $\\sum_{k=1}^K\\sigma_k^2\\leq \\Lambda$ for which  any algorithm incurs a variance-dependent lower bound of $\\Omega(\\sqrt{d_{\\textbf{elu}}\\Lambda})$. However, this lower bound has a $\\sqrt{d}$ gap with existing upper bounds. Moreover, it only considers a fixed total variance budget $\\Lambda$ and does not apply to a general variance sequence $\\{\\sigma_1^2,\\ldots,\\sigma_K^2\\}$.\nIn this paper, to overcome the limitations of Jia et al. (2024), we consider the general variance sequence under two settings. For a prefixed sequence, where the entire variance sequence is revealed to the learner at the beginning of the learning process, we establish a variance-dependent lower bound of $\\Omega(d \\sqrt{\\sum_{k=1}^K\\sigma_k^2 }/\\log K)$ for linear contextual bandits. For an adaptive sequence, where an adversary can generate the variance $\\sigma_k^2$ in each round $k$ based on historical observations, we show that when the adversary must generate $\\sigma_k^2$ before observing the decision set, a similar lower bound of $\\Omega(d\\sqrt{ \\sum_{k=1}^K\\sigma_k^2} /\\log^6(dK))$ holds. In both settings, our results match the upper bounds of the SAVE algorithm (Zhao et al. 2023) up to logarithmic factors. Furthermore, if the adversary can generate the variance $\\sigma_k$ after observing the decision set $\\mathcal{D}_k$, we construct a counter-example showing that it is impossible to construct a variance-dependent lower bound if the adversary properly selects variances in collaboration with the learner.\nOur lower bound proofs use a novel peeling technique that groups rounds by variance magnitude. For each group, we construct separate instances and assign the learner distinct decision sets. We believe this proof technique may be of independent interest.",
          "keywords": [
            "Bandit",
            "Reinforcement Learning"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=kXdW2KySK5",
          "pdf_link": "https://openreview.net/pdf?id=kXdW2KySK5"
        },
        "paper_internal_id": "kXdW2KySK5",
        "category": "reject",
        "embedding_score": 0.6899369955062866,
        "final_score": 0.15558956563472748
      },
      "poster": {
        "paper": {
          "id": "uEFC25uUwU",
          "title": "The $\\varphi$ Curve: The Shape of Generalization through the Lens of Norm-based Capacity Control",
          "abstract": "Understanding how the test risk scales with model complexity is a central question in machine learning. Classical theory is challenged by the learning curves observed for large over-parametrized deep networks. Capacity measures based on parameter count typically fail to account for these empirical observations. To tackle this challenge, we consider norm-based capacity measures and develop our study for random features based estimators, widely used as simplified theoretical models for more complex networks. In this context, we provide a precise characterization of how the estimator’s norm concentrates and how it governs the associated test error. Our results show that the predicted learning curve admits a phase transition from under- to over-parameterization, but no double descent behavior. This confirms that more classical U-shaped behavior is recovered considering appropriate capacity measures based on models norms rather than size. From a technical point of view, we leverage deterministic equivalence as the key tool and further develop new deterministic quantities which are of independent interest.",
          "keywords": [
            "generalization",
            "norm-based capacity",
            "deterministic equivalence"
          ],
          "primary_area": "theory",
          "TLDR": "We provide a precise description on how the test risk scales with a suitable norm-based capacity when compared to a classical metric of model size.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=uEFC25uUwU",
          "pdf_link": "https://openreview.net/pdf?id=uEFC25uUwU"
        },
        "paper_internal_id": "uEFC25uUwU",
        "category": "poster",
        "embedding_score": 0.6954854726791382,
        "final_score": 0.9823333621025085
      },
      "spotlight": {
        "paper": {
          "id": "004uTlSufe",
          "title": "How Well Can Differential Privacy Be Audited in One Run?",
          "abstract": "Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.",
          "keywords": [
            "differential privacy",
            "privacy auditing"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We characterize the capabilities of one-run privacy auditing and its inherent limitations and present new approaches to mitigate them.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=004uTlSufe",
          "pdf_link": "https://openreview.net/pdf?id=004uTlSufe"
        },
        "paper_internal_id": "004uTlSufe",
        "category": "spotlight",
        "embedding_score": 0.686631441116333,
        "final_score": 0.638371467590332
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "QN0E0KX2LM",
      "title": "Learning Linear Attention in Polynomial Time",
      "abstract": "Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question.  Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention.  We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS.  Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization.  We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models.",
      "keywords": "['Transformers', 'Learning Theory', 'PAC learning']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "QN0E0KX2LM",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "wh3p37VYm2",
          "title": "Mechanistic Insights into Grokking from the Embedding Layer",
          "abstract": "Grokking, a delayed generalization in neural networks after perfect training performance, has been observed in Transformers and MLPs, but the components driving it remain underexplored. We show that embeddings are central to grokking: introducing them into MLPs induces delayed generalization in modular arithmetic tasks, whereas MLPs without embeddings can generalize immediately. Our analysis identifies two key mechanisms: (1) Embedding update dynamics, where rare tokens stagnate due to sparse gradient updates and weight decay, and (2) Bilinear coupling, where the interaction between embeddings and downstream weights introduces saddle points and increases sensitivity to initialization.  \nTo confirm these mechanisms, we investigate frequency-aware sampling, which balances token updates by minimizing gradient variance, and embedding-specific learning rates, derived from the asymmetric curvature of the bilinear loss landscape. We prove that an adaptive learning rate ratio, \\(\\frac{\\eta_E}{\\eta_W} \\propto \\frac{\\sigma_{\\max}(E)}{\\sigma_{\\max}(W)} \\cdot \\frac{f_W}{f_E}\\), mitigates bilinear coupling effects, accelerating convergence. Our methods not only improve grokking dynamics but also extend to broader challenges in Transformer optimization, where bilinear interactions hinder efficient training.",
          "keywords": [
            "Embedding learning",
            "Token frequencey",
            "Coupled system"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "Explain the embedding role in optimization of MLP",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=wh3p37VYm2",
          "pdf_link": "https://openreview.net/pdf?id=wh3p37VYm2"
        },
        "paper_internal_id": "wh3p37VYm2",
        "category": "reject",
        "embedding_score": 0.771939754486084,
        "final_score": 0.27422651648521423
      },
      "poster": {
        "paper": {
          "id": "RBWnyDEBKf",
          "title": "Constant Bit-size Transformers Are Turing Complete",
          "abstract": "We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers.",
          "keywords": [
            "Transformer",
            "Turing complete",
            "Post machine",
            "context window length",
            "space complexity"
          ],
          "primary_area": "theory",
          "TLDR": "Simulating Turing machines with transformers requires scaling only the context window length, proportional to space complexity.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=RBWnyDEBKf",
          "pdf_link": "https://openreview.net/pdf?id=RBWnyDEBKf"
        },
        "paper_internal_id": "RBWnyDEBKf",
        "category": "poster",
        "embedding_score": 0.7762489318847656,
        "final_score": 0.7416432499885559
      },
      "spotlight": {
        "paper": {
          "id": "0EILv1HcmG",
          "title": "Quantum Doubly Stochastic Transformers",
          "abstract": "At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn’s algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn’s algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.",
          "keywords": [
            "Quantum Machine Learning",
            "Transformers",
            "Self-Attention",
            "Computer Vision"
          ],
          "primary_area": "other",
          "TLDR": "A hybrid quantum-classical Transformer model with quantum-induced doubly-stochastic attention that stabilizes and improves small-scale vision transformers",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=0EILv1HcmG",
          "pdf_link": "https://openreview.net/pdf?id=0EILv1HcmG"
        },
        "paper_internal_id": "0EILv1HcmG",
        "category": "spotlight",
        "embedding_score": 0.7475940585136414,
        "final_score": 0.9757503867149353
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "JbJVWljk7r",
      "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
      "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new $\\texttt{FP4}$ Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves $\\textbf{1038}$ $\\texttt{TOPS}$ on $\\texttt{RTX5090}$, which is a $\\textbf{5}\\times$ speedup over the fastest FlashAttention on $\\texttt{RTX5090}$. Experiments show that our $\\texttt{FP4}$ attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient $\\texttt{8-bit}$ attention for both forward and backward propagation. Experiments indicate that $\\texttt{8-bit}$ attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.",
      "keywords": "['Attention', 'Quantization', 'Efficient Attention', 'GPU Kernel']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "JbJVWljk7r",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "YtsX7irxbq",
          "title": "When recalling in-context, Transformers are not SSMs",
          "abstract": "Despite the advantageous subquadratic complexity of modern recurrent deep learning models -- such as state-space models (SSMs) -- recent studies have highlighted their potential shortcomings compared to transformers on reasoning and memorization tasks. In this paper, we dive deeper into one of such benchmarks: associative recall (AR), which has been shown to correlate well with language modeling performance, and inspect in detail the effects of scaling and optimization issues in recently proposed token mixing strategies. We first demonstrate that, unlike standard transformers, the choice of learning rate plays a critical role in the performance of modern recurrent models: an issue that can severely affect reported performance in previous works and suggests further research is needed to stabilize training. Next, we show that recurrent and attention-based models exhibit contrasting benefits when scaling in width as opposed to depth, with attention being notably unable to solve AR when limited to a single layer. We then further inspect 1-layer transformers, revealing that despite their poor performance, their training dynamics surprisingly resemble the formation of induction heads, a phenomenon previously observed only in their 2-layer counterparts. Finally, through architectural ablations, we study how components affects Transformer and Mamba’s performance and optimization stability.",
          "keywords": [
            "SSMs",
            "Attention",
            "In-Context Learning",
            "Language Modeling",
            "Mamba"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=YtsX7irxbq",
          "pdf_link": "https://openreview.net/pdf?id=YtsX7irxbq"
        },
        "paper_internal_id": "YtsX7irxbq",
        "category": "reject",
        "embedding_score": 0.72218918800354,
        "final_score": 0.20698854327201843
      },
      "poster": {
        "paper": {
          "id": "o9iReV4FGm",
          "title": "Fast attention mechanisms: a tale of parallelism",
          "abstract": "Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the expressive power previously established for standard attention in terms of matching the capabilities of MPC algorithms, and (2) can solve key reasoning tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC framework, we further prove that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers, thereby providing a unified way to reason about a broad class of efficient attention approximations.",
          "keywords": [
            "Transformer theory",
            "representational strength",
            "nearest neighbor search",
            "massively parallel computation"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=o9iReV4FGm",
          "pdf_link": "https://openreview.net/pdf?id=o9iReV4FGm"
        },
        "paper_internal_id": "o9iReV4FGm",
        "category": "poster",
        "embedding_score": 0.7556389570236206,
        "final_score": 0.9648333191871643
      },
      "oral": {
        "paper": {
          "id": "1b7whO4SfY",
          "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
          "abstract": "Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention.\nYet, existing literature rarely examines the specific effects of gating.\nIn this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset.\nOur central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance.\nThis modification also enhances training stability, tolerates larger learning rates, and improves scaling properties.\nBy comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. \nWe also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research.\nFurthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).",
          "keywords": [
            "Attention",
            "Large Language Model",
            "Gating"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We find applying a query-dependent head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA) consistently improves performance, improves scaling properties and mitigates the `massive activation' and `attention sink'.",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=1b7whO4SfY",
          "pdf_link": "https://openreview.net/pdf?id=1b7whO4SfY"
        },
        "paper_internal_id": "1b7whO4SfY",
        "category": "oral",
        "embedding_score": 0.7439923882484436,
        "final_score": 0.06809418648481369
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "sAFottNlra",
      "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
      "abstract": "Developing large language models is expensive and often involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable and useful for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark’s ability to separate better models from worse models, and noise, a benchmark’s sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error.  These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce four interventions designed to directly affect signal or noise.  For example, we propose that switching to a  metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and scaling law error. We also find that filtering noisy benchmarks such that they have better signal-to-noise ratio leads to more reliable evaluations. We also find that averaging the output of a model's checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise.  We use 30 benchmarks for these experiments, and 465 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 50K evaluation benchmark results, totaling 200M instances.",
      "keywords": "['language models', 'benchmarks', 'evaluation']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "sAFottNlra",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "GmKpnuMUFC",
          "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling",
          "abstract": "Large language models (LLMs) can often accurately describe probability distributions using natural language, yet they still struggle to generate faithful samples from them. This mismatch limits their use in tasks requiring reliable stochasticity, such as Monte Carlo methods, agent-based simulations, and randomized decision-making. We investigate this gap between knowledge and sampling in the context of Bernoulli distributions. We introduce Verbalized Rejection Sampling (VRS), a natural-language adaptation of classical rejection sampling that prompts the LLM to reason about and accept or reject proposed samples. Despite relying on the same Bernoulli mechanism internally, VRS substantially reduces sampling bias across models. We provide theoretical analysis showing that, under mild assumptions, VRS improves over direct sampling, with gains attributable to both the algorithm and prompt design. More broadly, our results show how classical probabilistic tools can be verbalized and embedded into LLM workflows to improve reliability, without requiring access to model internals or heavy prompt engineering.",
          "keywords": [
            "rejection sampling",
            "large language models"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "We show that LLMs struggle to faithfully sample from distributions they can describe, and introduce Verbalized Rejection Sampling—a natural-language adaptation of classical rejection sampling—that significantly reduces this bias.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=GmKpnuMUFC",
          "pdf_link": "https://openreview.net/pdf?id=GmKpnuMUFC"
        },
        "paper_internal_id": "GmKpnuMUFC",
        "category": "reject",
        "embedding_score": 0.7319393157958984,
        "final_score": 0.21379199624061584
      },
      "poster": {
        "paper": {
          "id": "CCBPSxWOhi",
          "title": "Linguini: A benchmark for language-agnostic linguistic reasoning",
          "abstract": "We propose a new benchmark to measure a language model's linguistic reasoning skills without relying on pre-existing language-specific knowledge. The test covers 894 questions grouped in 160 problems across 75 (mostly) extremely low-resource languages, extracted from the International Linguistic Olympiad corpus. To attain high accuracy on this benchmark, models don't need previous knowledge of the tested language, as all the information needed to solve the linguistic puzzle is presented in the context. We find that, while all analyzed models rank below 25% accuracy, there is a significant gap between open and closed models, with the best-performing proprietary model scoring 24.05% and the best-performing open model 8.84%.",
          "keywords": [
            "linguistic reasoning"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track poster",
          "forum_link": "https://openreview.net/forum?id=CCBPSxWOhi",
          "pdf_link": "https://openreview.net/pdf?id=CCBPSxWOhi"
        },
        "paper_internal_id": "CCBPSxWOhi",
        "category": "poster",
        "embedding_score": 0.7358333468437195,
        "final_score": 0.6640208959579468
      },
      "oral": {
        "paper": {
          "id": "Q3qAsZAEZw",
          "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
          "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
          "keywords": [
            "Large Language Models (LLMs)",
            "Reproducibility",
            "Numerical precision",
            "Deterministic inference"
          ],
          "primary_area": "deep_learning",
          "TLDR": "This paper demonstrates that low precision causes non-reproducible LLM inference across different setups, proposing a hybrid-precision method, LayerCast, that computes in FP32 to achieve determinism while saving memory.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=Q3qAsZAEZw",
          "pdf_link": "https://openreview.net/pdf?id=Q3qAsZAEZw"
        },
        "paper_internal_id": "Q3qAsZAEZw",
        "category": "oral",
        "embedding_score": 0.8133158683776855,
        "final_score": 0.26304417848587036
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "qXAABCxYQ2",
      "title": "Which Algorithms Have Tight Generalization Bounds?",
      "abstract": "We study which machine learning algorithms have tight generalization bounds with respect to a given collection of population distributions. Our results build on and extend the recent work of Gastpar et al. (2023). First, we present conditions that preclude the existence of tight generalization bounds. Specifically, we show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds. Next, we show that algorithms that are sufficiently loss-stable do have tight generalization bounds.  We conclude with a simple characterization that relates the existence of tight generalization bounds to the conditional variance of the algorithm's loss.",
      "keywords": "['learning theory']",
      "decision": "Accept (Spotlight)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "qXAABCxYQ2",
    "query_category": "spotlight",
    "matches": {
      "reject": {
        "paper": {
          "id": "vWaMUMrBpF",
          "title": "Inconsistency-Aware Minimization: Improving Generalization with Unlabeled Data",
          "abstract": "Accurately estimating the generalization gap and devising optimization methods that generalize better are crucial for deep learning models, particularly in both theoretical understanding and practical applications. The ability to leverage unlabeled data for these purposes offers significant advantages in real-world scenarios. This paper introduces a novel generalization measure, termed $\\textit{local inconsistency}$, developed from an information-geometric perspective of the neural network's parameter space; a key feature is its computability from unlabeled data. We establish its theoretical underpinnings by connecting local inconsistency to the Fisher Information Matrix (FIM) and the loss Hessian. Empirically, we demonstrate that local inconsistency not only correlates with the generalization gap but also exhibits characteristics comparable to $\\textit{sharpness}$. Based on these findings, we propose Inconsistency-Aware Minimization (IAM), a regularization strategy that incorporates local inconsistency. We demonstrate that in standard supervised learning settings, IAM enhances generalization, achieving performance comparable to existing methods such as Sharpness-Aware Minimization (SAM). Furthermore, IAM exhibits notable efficacy in semi-supervised learning scenarios, where the local inconsistency regularizer is computed from the unlabeled data portion to further improve model performance.",
          "keywords": [
            "Generalization",
            "Regularization",
            "Training Method",
            "Deep Learning",
            "Inconsistency"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=vWaMUMrBpF",
          "pdf_link": "https://openreview.net/pdf?id=vWaMUMrBpF"
        },
        "paper_internal_id": "vWaMUMrBpF",
        "category": "reject",
        "embedding_score": 0.7197990417480469,
        "final_score": 0.9468098282814026
      },
      "poster": {
        "paper": {
          "id": "5aeD5UbiLv",
          "title": "GIST: Greedy Independent Set Thresholding for Max-Min Diversification with Submodular Utility",
          "abstract": "This work studies a novel subset selection problem called *max-min diversification with monotone submodular utility* (MDMS), which has a wide range of applications in machine learning, e.g., data sampling and feature selection.\nGiven a set of points in a metric space,\nthe goal of MDMS is to maximize $f(S) = g(S) + \\lambda \\cdot \\text{div}(S)$\nsubject to a cardinality constraint $|S| \\le k$,\nwhere\n$g(S)$ is a monotone submodular function\nand\n$\\text{div}(S) = \\min_{u,v \\in S : u \\ne v} \\text{dist}(u,v)$ is the *max-min diversity* objective.\nWe propose the `GIST` algorithm, which gives a $\\frac{1}{2}$-approximation guarantee for MDMS\nby approximating a series of maximum independent set problems with a bicriteria greedy algorithm.\nWe also prove that it is NP-hard to approximate within a factor of $0.5584$.\nFinally, we show in our empirical study that `GIST` outperforms state-of-the-art benchmarks\nfor a single-shot data sampling task on ImageNet.",
          "keywords": [
            "approximation algorithm",
            "submodular maximization",
            "max-min diversification",
            "data sampling",
            "subset selection"
          ],
          "primary_area": "optimization",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=5aeD5UbiLv",
          "pdf_link": "https://openreview.net/pdf?id=5aeD5UbiLv"
        },
        "paper_internal_id": "5aeD5UbiLv",
        "category": "poster",
        "embedding_score": 0.7226772308349609,
        "final_score": 0.9926169514656067
      },
      "oral": {
        "paper": {
          "id": "ImpizBSKcu",
          "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
          "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm.  We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$,\nand large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies:\n$(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network;\n$(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity;\n$(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated  `feature unlearning' regime at large times.",
          "keywords": [
            "Overfitting; feature learning; dynamical mean field theory; generalization;"
          ],
          "primary_area": "theory",
          "TLDR": "Large neural networks first learn low dimensional feature representation then overfit the data and revert to a kernel regime.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=ImpizBSKcu",
          "pdf_link": "https://openreview.net/pdf?id=ImpizBSKcu"
        },
        "paper_internal_id": "ImpizBSKcu",
        "category": "oral",
        "embedding_score": 0.7144260406494141,
        "final_score": 0.9792327284812927
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "2rgYVFiWPL",
      "title": "Sample complexity of Schrödinger potential estimation",
      "abstract": "We address the problem of Schrödinger potential estimation, which plays a crucial role in modern generative modelling approaches based on Schrödinger bridges and stochastic optimal control for SDEs. Given a simple prior diffusion process, these methods search for a path between two given distributions $\\rho_0$  and $\\rho_T$ requiring minimal efforts. The optimal drift in this case can be expressed through a Schrödinger potential. In the present paper, we study generalization ability of an empirical Kullback-Leibler (KL) risk minimizer over a class of admissible log-potentials aimed at fitting the marginal distribution at time $T$. Under reasonable assumptions on the target distribution $\\rho_T$ and the prior process, we derive a non-asymptotic high-probability upper bound on the KL-divergence between $\\rho_T$ and the terminal density corresponding to the estimated log-potential. In particular, we show that the excess KL-risk may decrease as fast as $\\mathcal O(\\log n / n)$ when the sample size $n$ tends to infinity even if both $\\rho_0$  and $\\rho_T$ have unbounded supports.",
      "keywords": "['Schrödinger bridge', 'stochastic optimal control', 'Schrödinger potential', 'high-probability bounds', 'excess risk']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "2rgYVFiWPL",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "C7BIQRM57T",
          "title": "Momentum Multi-Marginal Schrödinger Bridge Matching",
          "abstract": "Understanding complex systems by inferring trajectories from sparse sample snapshots is a fundamental challenge in a wide range of domains, e.g., single-cell biology, meteorology, and economics. Despite advancements in Bridge and Flow matching frameworks, current methodologies rely on pairwise interpolation between adjacent snapshots. This hinders their ability to capture long-range temporal dependencies and potentially affects the coherence of the inferred trajectories. To address these issues, we introduce Momentum Multi-Marginal Schrödinger Bridge Matching (3MSBM), a novel matching framework that learns smooth measure-valued splines for stochastic systems that satisfy multiple positional constraints. This is achieved by lifting the dynamics to phase space and generalizing stochastic bridges to be conditioned on several points, forming a multi-marginal conditional stochastic optimal control problem. The underlying dynamics are then learned by minimizing a variational objective, having fixed the path induced by the multi-marginal conditional bridge. As a matching approach, 3MSBM learns transport maps that preserve intermediate marginals throughout training, significantly improving convergence and scalability. Extensive experimentation in a series of real-world applications validates the superior performance of 3MSBM compared to existing methods in capturing complex dynamics with temporal dependencies, opening new avenues for training matching frameworks in multi-marginal settings.",
          "keywords": [
            "Diffusion models",
            "Schrödinger bridge",
            "Distribution matching",
            "Trajectory Inference"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We introduce Momentum Multi-Marginal Schrödinger Bridge Matching (3MSBM), a novel matching framework that learns smooth trajectories for stochastic systems that satisfy multiple positional constraints.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=C7BIQRM57T",
          "pdf_link": "https://openreview.net/pdf?id=C7BIQRM57T"
        },
        "paper_internal_id": "C7BIQRM57T",
        "category": "poster",
        "embedding_score": 0.7394192218780518,
        "final_score": 0.5927038192749023
      },
      "spotlight": {
        "paper": {
          "id": "K0FbK2GOGj",
          "title": "Instance-Optimality for Private KL Distribution Estimation",
          "abstract": "We study the fundamental problem of estimating an unknown discrete distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances $p$ and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on $p$ is compared to the minimum achievable estimation error over a small local neighborhood of $p$. Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works.",
          "keywords": [
            "Differential Privacy",
            "Distribution Estimation",
            "Instance-Optimality"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=K0FbK2GOGj",
          "pdf_link": "https://openreview.net/pdf?id=K0FbK2GOGj"
        },
        "paper_internal_id": "K0FbK2GOGj",
        "category": "spotlight",
        "embedding_score": 0.7094197273254395,
        "final_score": 0.47022420167922974
      },
      "oral": {
        "paper": {
          "id": "rMhQBlhh4c",
          "title": "Adjoint Schrödinger Bridge Sampler",
          "abstract": "Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers",
          "keywords": [
            "Boltzmann distribution",
            "diffusion sampler",
            "Schrödinger bridge"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=rMhQBlhh4c",
          "pdf_link": "https://openreview.net/pdf?id=rMhQBlhh4c"
        },
        "paper_internal_id": "rMhQBlhh4c",
        "category": "oral",
        "embedding_score": 0.7874988913536072,
        "final_score": 0.25148725509643555
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "xZnjIkIzST",
      "title": "Restore3D: Breathing Life into Broken Objects with Shape and Texture Restoration",
      "abstract": "Restoring incomplete or damaged 3D objects is crucial for cultural heritage preservation, occluded object reconstruction, and artistic design.\nExisting methods primarily focus on geometric completion, often neglecting texture restoration and struggling with relatively complex and diverse objects.\nWe introduce Restore3D, a novel framework that simultaneously restores both the shape and texture of broken objects using multi-view images. To address limited training data, we develop an automated data generation pipeline that synthesizes paired incomplete-complete samples from large-scale 3D datasets. \nCentral to Restore3D is a multi-view model, enhanced by a carefully designed Mask Self-Perceiver module with a Depth-Aware Mask Rectifier.\nThe rectified masks, learned through the self-perceiver, facilitate an image integration and enhancement phase that preserves shape and texture patterns of incomplete objects and mitigates the low-resolution limitations of the base model, yielding high-resolution, semantically coherent, and view-consistent multi-view images. \nA coarse-to-fine reconstruction strategy is then employed to recover detailed textured 3D meshes from refined multi-view images. Comprehensive experiments show that Restore3D produces visually and geometrically faithful 3D textured meshes, outperforming existing methods and paving the way for more robust 3D object restoration. Project page: https://nip-ss.github.io/NIPS-anonymous/ .",
      "keywords": "['Diffusion Models', '3D object completion', 'Multi-view Image Generation', 'Multi-view Image Inpainting']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "xZnjIkIzST",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "qbVbZWxUib",
          "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing",
          "abstract": "Recent progress in 3D object generation has greatly improved both the quality and efficiency.\nHowever, most existing methods generate a single mesh with all parts fused together, which limits the ability to edit or manipulate individual parts.\nA key challenge is that different objects may have a varying number of parts.\nTo address this, we propose a new end-to-end framework for part-level 3D object generation.\nGiven a single input image, our method generates high-quality 3D objects with an arbitrary number of complete and semantically meaningful parts.\nWe introduce a dual volume packing strategy that organizes all parts into two complementary volumes, allowing for the creation of complete and interleaved parts that assemble into the final object.\nExperiments show that our model achieves better quality, diversity, and generalization than previous image-based part-level generation methods.\nOur project page is at \\url{https://research.nvidia.com/labs/dir/partpacker/}.",
          "keywords": [
            "3D Generation",
            "Part Generation",
            "Image-to-3D"
          ],
          "primary_area": "applications",
          "TLDR": "We present a method to generate high-quality 3D shape composed of individual and complete parts from a single-view image.",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=qbVbZWxUib",
          "pdf_link": "https://openreview.net/pdf?id=qbVbZWxUib"
        },
        "paper_internal_id": "qbVbZWxUib",
        "category": "poster",
        "embedding_score": 0.7471088767051697,
        "final_score": 0.957064688205719
      },
      "spotlight": {
        "paper": {
          "id": "8a9bAZFeIu",
          "title": "Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation",
          "abstract": "Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.",
          "keywords": [
            "Image-to-3D",
            "Generative Models",
            "Analysis"
          ],
          "primary_area": "evaluation",
          "TLDR": "We present Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation.",
          "creation_date": "2025-04-13",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=8a9bAZFeIu",
          "pdf_link": "https://openreview.net/pdf?id=8a9bAZFeIu"
        },
        "paper_internal_id": "8a9bAZFeIu",
        "category": "spotlight",
        "embedding_score": 0.7503449320793152,
        "final_score": 0.9031029343605042
      },
      "oral": {
        "paper": {
          "id": "aLhA7AYLLR",
          "title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
          "abstract": "Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations.",
          "keywords": [
            "Image fusion",
            "multimodal images",
            "degradation"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=aLhA7AYLLR",
          "pdf_link": "https://openreview.net/pdf?id=aLhA7AYLLR"
        },
        "paper_internal_id": "aLhA7AYLLR",
        "category": "oral",
        "embedding_score": 0.7045776844024658,
        "final_score": 0.7534354329109192
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "AYcKh0oT3h",
      "title": "Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications",
      "abstract": "In Online Convex Optimization (OCO), when the stochastic gradient has a finite variance, many algorithms provably work and guarantee a sublinear regret. However, limited results are known if the gradient estimate has a heavy tail, i.e., the stochastic gradient only admits a finite $\\mathsf{p}$-th central moment for some $\\mathsf{p}\\in\\left(1,2\\right]$. Motivated by it, this work examines different old algorithms for OCO (e.g., Online Gradient Descent) in the more challenging heavy-tailed setting. Under the standard bounded domain assumption, we establish new regrets for these classical methods without any algorithmic modification. Remarkably, these regret bounds are fully optimal in all parameters (can be achieved even without knowing $\\mathsf{p}$), suggesting that OCO with heavy tails can be solved effectively without any extra operation (e.g., gradient clipping). Our new results have several applications. A particularly interesting one is the first provable convergence result for nonsmooth nonconvex optimization under heavy-tailed noise without gradient clipping.",
      "keywords": "['Online Learning', 'Online Convex Optimization', 'Heavy Tails']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "AYcKh0oT3h",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "YmbQ0qnQ76",
          "title": "$O(\\sqrt{T})$ Static Regret and Instance Dependent Constraint Violation for Constrained Online Convex Optimization",
          "abstract": "The constrained version of the standard online convex optimization (OCO) framework, called COCO is considered, where on every round, a convex cost function and a convex constraint function are revealed to the learner after it chooses the action for that round.\nThe objective is to simultaneously minimize the static regret and cumulative constraint violation (CCV). \nAn algorithm is proposed that guarantees a static regret of $O(\\sqrt{T})$ and a CCV of $\\min\\{{\\cal V}, O(\\sqrt{T}\\log T) \\}$, where ${\\cal V}$ depends on the distance between the consecutively revealed constraint sets, the shape of constraint sets, dimension of action space and the diameter of the action space. \nWhen constraint sets have additional structure, ${\\cal V}=O(1)$. Compared to the state of the art results, static regret of $O(\\sqrt{T})$ and CCV of $O(\\sqrt{T}\\log T)$, that were universal, the new result on CCV is instance dependent, which is derived by exploiting the geometric properties of the constraint sets.",
          "keywords": [
            "online convex optimization",
            "regret"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "An algorithm with a static regret of $O(\\sqrt{T})$ and a CCV of $\\min\\{{\\cal V}, O(\\sqrt{T}\\log T) \\}$, for constrained online convex optimization where ${\\cal V}$ depends on the geometric properties of the instance .",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=YmbQ0qnQ76",
          "pdf_link": "https://openreview.net/pdf?id=YmbQ0qnQ76"
        },
        "paper_internal_id": "YmbQ0qnQ76",
        "category": "poster",
        "embedding_score": 0.7688925266265869,
        "final_score": 0.9904658794403076
      },
      "spotlight": {
        "paper": {
          "id": "ONc9vWkwCp",
          "title": "On the necessity of adaptive regularisation: Optimal anytime online learning on $\\boldsymbol{\\ell_p}$-balls",
          "abstract": "We study online convex optimization on $\\ell_p$-balls in $\\mathbb{R}^d$ for $p > 2$. While always sub-linear, the optimal regret exhibits a shift between the high-dimensional setting ($d > T$), when the dimension $d$ is greater than the time horizon $T$ and the low-dimensional setting ($d \\leq T$). We show that Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is adaptive to the dimension regime is anytime optimal for all dimension regimes. Motivated by this, we ask whether it is possible to obtain anytime optimality of FTRL with fixed non-adaptive regularisation. Our main result establishes that for separable regularisers, adaptivity in the regulariser is necessary, and that any fixed regulariser will be sub-optimal in one of the two dimension regimes. Finally, we provide lower bounds which rule out sub-linear regret bounds for the linear bandit problem in sufficiently high-dimension for all $\\ell_p$-balls with $p \\geq 1$.",
          "keywords": [
            "OCO",
            "lp-balls",
            "high-dimension"
          ],
          "primary_area": "theory",
          "TLDR": "Our work shows that in online convex optimization over lp-balls (p>2), anytime optimality can be achieved with Follow-the-Regularized-Leader using adaptive regularization, and that for separable regularizers this adaptivity is necessary.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=ONc9vWkwCp",
          "pdf_link": "https://openreview.net/pdf?id=ONc9vWkwCp"
        },
        "paper_internal_id": "ONc9vWkwCp",
        "category": "spotlight",
        "embedding_score": 0.7972390651702881,
        "final_score": 0.9869148135185242
      },
      "oral": {
        "paper": {
          "id": "UVDihUz0iT",
          "title": "High-Dimensional Calibration from Swap Regret",
          "abstract": "We study the online calibration of multi-dimensional forecasts over an arbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an arbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds when actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual $\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds. When $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is the $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$ algorithms for learning with experts implies that it is possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) = d^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng 2025.\n\nInterestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate $\\rho$ -- in fact, our algorithm is identical for every setting of $\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round.\n\nFinally, we prove that any online calibration algorithm that guarantees $\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex requires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq \\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding $d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng 2025, and shows that an exponential dependence on $1/\\epsilon$ is necessary.",
          "keywords": [
            "Calibration",
            "Swap Regret",
            "Online Learning"
          ],
          "primary_area": "theory",
          "TLDR": "An algorithm for calibrating forecasts of high-dimensional outcomes.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=UVDihUz0iT",
          "pdf_link": "https://openreview.net/pdf?id=UVDihUz0iT"
        },
        "paper_internal_id": "UVDihUz0iT",
        "category": "oral",
        "embedding_score": 0.7616047859191895,
        "final_score": 0.9111025929450989
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "YqzAsStE6n",
      "title": "Linear Bandits with Non-i.i.d. Noise",
      "abstract": "We study the linear stochastic bandit problem, relaxing the standard i.i.d. assumption on the observation noise. \nAs an alternative to this restrictive assumption, we allow the noise terms across rounds to be sub-Gaussian but \ninterdependent, with dependencies that decay over time. To address this setting, we develop new confidence sequences \nusing a recently introduced reduction scheme to sequential probability assignment, and use these to derive a bandit \nalgorithm based on the principle of optimism in the face of uncertainty. We provide regret bounds for the \nresulting algorithm, expressed in terms of the decay rate of the strength of dependence between observations. Among \nother results, we show that our bounds recover the standard rates up to a factor of the mixing time for geometrically \nmixing observation noise.",
      "keywords": "['linear bandits', 'non-i.i.d', 'online learning']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "YqzAsStE6n",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "uih8cWS3JF",
          "title": "Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards",
          "abstract": "We study stochastic linear bandits with heavy-tailed rewards, where the rewards have a finite $(1+\\epsilon)$-absolute central moment bounded by $\\upsilon$ for some $\\epsilon \\in (0,1]$. We improve both upper and lower bounds on the minimax regret compared to prior work. When $\\upsilon = \\mathcal{O}(1)$, the best prior known regret upper bound is $\\tilde{O}(d T^{\\frac{1}{1+\\epsilon}})$.  While a lower with the same scaling has been given, it relies on a construction using $\\upsilon = d$, and adapting the construction to the bounded-moment regime with $\\upsilon = \\mathcal{O}(1)$ yields only a $\\Omega(d^{\\frac{\\epsilon}{1+\\epsilon}} T^{\\frac{1}{1+\\epsilon}})$ lower bound. This matches the known rate for multi-armed bandits and is generally loose for linear bandits, in particular being $\\sqrt{d}$ below the optimal rate in the finite-variance case ($\\epsilon = 1$).\nWe propose a new elimination-based algorithm guided by experimental design, which achieves regret $\\tilde{\\mathcal{O}}(d^{\\frac{1+3\\epsilon}{2(1+\\epsilon)}} T^{\\frac{1}{1+\\epsilon}})$, thus improving the dependence on $d$ for all $\\epsilon \\in (0,1)$ and recovering a known optimal result for $\\epsilon = 1$.  We also establish a lower bound of $\\Omega(d^{\\frac{2\\epsilon}{1+\\epsilon}} T^{\\frac{1}{1+\\epsilon}})$, which strictly improves upon the multi-armed bandit rate and highlights the hardness of heavy-tailed linear bandit problems. For finite action sets of size $n$, we derive upper and lower bounds of\n$\\tilde{\\mathcal{O}}(\\sqrt d (\\log n)^{\\frac{\\epsilon}{1+\\epsilon}}T^{\\frac{1}{1+\\epsilon}})$ and\n$\\tilde\\Omega(d^{\\frac{\\epsilon}{1+\\epsilon}}(\\log n)^{\\frac{\\epsilon}{1+\\epsilon}} T^{\\frac{1}{1+\\epsilon}})$, respectively.\nFinally, we provide action-set-dependent regret upper bounds and show that for some geometries, such as $l_p$-norm balls for $p \\le 1 + \\epsilon$, we can further reduce the dependence on $d$.",
          "keywords": [
            "linear bandits",
            "heavy-tailed",
            "experimental design",
            "regret analysis",
            "online learning"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=uih8cWS3JF",
          "pdf_link": "https://openreview.net/pdf?id=uih8cWS3JF"
        },
        "paper_internal_id": "uih8cWS3JF",
        "category": "poster",
        "embedding_score": 0.8165490627288818,
        "final_score": 0.9107449054718018
      },
      "spotlight": {
        "paper": {
          "id": "K3xaVpSHkV",
          "title": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness",
          "abstract": "We study the problem of learning in the presence of an adversary that can corrupt an $\\eta$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as $\\Theta(d\\eta)$, where $d$ is the VC dimension of the hypothesis class [Hanneke, Karbasi, Mahmoody, Mehalel, and Moran (NeurIPS 2022)]. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is $\\widetilde\\Theta(\\sqrt{d\\eta})$, answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al.\\ showed that deterministic learners can be forced to suffer error close to $1$ even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner’s random bits are fully visible to the adversary. In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of $\\Omega(\\sqrt{d\\eta})$ infinitely often.",
          "keywords": [
            "Learning theorey",
            "Data Poisoning"
          ],
          "primary_area": "theory",
          "TLDR": "Giving tight bounds for the acess loss in the Agnostic Learning under Targeted Poisoning modle unsing randomized learnes.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=K3xaVpSHkV",
          "pdf_link": "https://openreview.net/pdf?id=K3xaVpSHkV"
        },
        "paper_internal_id": "K3xaVpSHkV",
        "category": "spotlight",
        "embedding_score": 0.6918585300445557,
        "final_score": 0.6141791939735413
      },
      "oral": {
        "paper": {
          "id": "gL4muAFwsh",
          "title": "Does Stochastic Gradient really succeed for bandits?",
          "abstract": "Recent works of Mei et al. (2023, 2024) have deepened the theoretical understanding of the *Stochastic Gradient Bandit* (SGB) policy, showing that using a constant learning rate guarantees asymptotic convergence to the optimal policy, and that sufficiently *small* learning rates can yield logarithmic regret. However, whether logarithmic regret holds beyond small learning rates remains unclear. In this work, we take a step towards characterizing the regret *regimes* of SGB as a function of its learning rate. For two--armed bandits, we identify a sharp threshold, scaling with the sub-optimality gap $\\Delta$, below which SGB achieves *logarithmic* regret on all instances, and above which it can incur *polynomial* regret on some instances. \nThis result highlights the necessity of knowing (or estimating) $\\Delta$ to ensure logarithmic regret with a constant learning rate.\nFor general $K$-armed bandits, we further show the learning rate must scale inversely with $K$ to avoid polynomial regret. We introduce novel techniques to derive regret upper bounds for SGB, laying the groundwork for future advances in the theory of gradient-based bandit algorithms.",
          "keywords": [
            "bandits",
            "policy gradient"
          ],
          "primary_area": "theory",
          "TLDR": "We propose a novel regret analysis of a simple policy gradient algorithm for bandits, characterizing regret regimes depending on its learning rate.",
          "creation_date": "2025-04-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=gL4muAFwsh",
          "pdf_link": "https://openreview.net/pdf?id=gL4muAFwsh"
        },
        "paper_internal_id": "gL4muAFwsh",
        "category": "oral",
        "embedding_score": 0.8028756976127625,
        "final_score": 0.3712596893310547
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "jTaxGFy34h",
      "title": "Robust Wasserstein  $k$-center Clustering: Algorithms and Acceleration",
      "abstract": "The classical metric $k$-center problem is widely used in data representation tasks. However, real-world datasets often contain noise and exhibit complex structures, making the traditional metric $k$-center problem insufficient for such scenarios. To address these challenges, we present the \\textbf{R}obust \\textbf{W}asserstein \\textbf{C}enter clustering (RWC-clustering)  problem.\nCompared to the classical setting, the main challenge in designing an algorithm for the RWC-clustering problem lies in effectively handling noise in the cluster centers. To this end, we introduce a dedicated purification step to eliminate noise, based on which we develop our clustering algorithm.\nFurthermore, when dealing with large-scale datasets, both storage and computation become highly resource-intensive. To alleviate this, we adopt the \\textit{coreset} technique to improve the computational and storage efficiency by compressing the dataset.  \nRoughly speaking, this coreset method enables us to calculate the objective value on a small-size coreset, while ensuring a close approximation to the value on the original dataset in theory; thus, it substantially saves the storage and computation resources.  \nFinally, experimental results show the effectiveness of our RWC-clustering  problem and the efficiency of the coreset method.",
      "keywords": "['clustering; coreset; Wasserstein distance']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "jTaxGFy34h",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "ODgWBaErst",
          "title": "Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks",
          "abstract": "Neural networks are widely used for image–related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory‑ and compute‑footprint can be reduced by compression. In this work, we focus on compression through tensorization and low‑rank representations. Whereas classical approaches search for a low‑rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight‑space, we use data‑informed norms that measure the error in function space. Concretely, we minimize the change in the layer’s output distribution, which can be expressed as $\\lVert (W - \\widetilde{W}) \\Sigma^{1/2}\\rVert_F$ where $\\Sigma^{1/2}$ is the square root of the covariance matrix of the layer’s input and $W$, $\\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker‑2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post‑compression fine‑tuning, our data‑informed approach often achieves competitive accuracy without any fine‑tuning. We further show that the same covariance‑based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable.\nExperiments on several CNN architectures (ResNet‑18/50, and GoogLeNet) and datasets (ImageNet, FGVC‑Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.",
          "keywords": [
            "Compression of neural networks",
            "tensor decomposition",
            "low-rank approximation",
            "distribution-aware norm"
          ],
          "primary_area": "optimization",
          "TLDR": "Distribution-aware ALS algorithms for Tucker-2 and CP decompositions compress CNN weights by directly minimizing output-distribution shift, delivering competitive, fine-tuning-free accuracy that even transfers across datasets.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=ODgWBaErst",
          "pdf_link": "https://openreview.net/pdf?id=ODgWBaErst"
        },
        "paper_internal_id": "ODgWBaErst",
        "category": "poster",
        "embedding_score": 0.6838710308074951,
        "final_score": 0.9597833156585693
      },
      "spotlight": {
        "paper": {
          "id": "o7Z8TClGjp",
          "title": "Unifying Proportional Fairness in Centroid and Non-Centroid Clustering",
          "abstract": "Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study _centroid clustering_, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study _non-centroid clustering_, in which each data point's loss is determined by its maximum distance to any other data point in its cluster. \n  \nWe generalize both paradigms to introduce _semi-centroid clustering_, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria---the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case.",
          "keywords": [
            "Proportional Fairness",
            "Clustering",
            "Algorithmic Fairness"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We design proportionally fair clustering methods when each agent's loss function is determined by both its distance from the other agents in its cluster and to a representative agent in its cluster.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=o7Z8TClGjp",
          "pdf_link": "https://openreview.net/pdf?id=o7Z8TClGjp"
        },
        "paper_internal_id": "o7Z8TClGjp",
        "category": "spotlight",
        "embedding_score": 0.7735410928726196,
        "final_score": 0.8032779693603516
      },
      "oral": {
        "paper": {
          "id": "VYLdKb5dzO",
          "title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
          "abstract": "In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of $\\mathcal{O}(1/\\sqrt{n})$, where $n$ is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data \"memorization\" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must \"memorize'' a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.",
          "keywords": [
            "generalization error",
            "information theory",
            "conditional mutual information",
            "CMI",
            "learning theory",
            "projection",
            "memorization"
          ],
          "primary_area": "theory",
          "TLDR": "",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=VYLdKb5dzO",
          "pdf_link": "https://openreview.net/pdf?id=VYLdKb5dzO"
        },
        "paper_internal_id": "VYLdKb5dzO",
        "category": "oral",
        "embedding_score": 0.6756307482719421,
        "final_score": 0.6801772713661194
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "F20AfNqMq9",
      "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments",
      "abstract": "With the recent success of world-model agents—which extend the core idea of model-based reinforcement learning by learning a differentiable model for sample-efficient control across diverse tasks—active inference (AIF) offers a complementary, neuroscience-grounded paradigm that unifies perception, learning, and action within a single probabilistic framework powered by a generative model. Despite this promise, practical AIF agents still rely on accurate immediate predictions and exhaustive planning, a limitation that is exacerbated in delayed environments requiring plans over long horizons—tens to hundreds of steps. Moreover, most existing agents are evaluated on robotic or vision benchmarks which, while natural for biological agents, fall short of real-world industrial complexity. We address these limitations with a generative–policy architecture featuring (i) a multi-step latent transition that lets the generative model predict an entire horizon in a single look-ahead, (ii) an integrated policy network that enables the transition and receives gradients of the expected free energy, (iii) an alternating optimization scheme that updates model and policy from a replay buffer, and (iv)  a single gradient step that plans over long horizons, eliminating exhaustive planning from the control loop. We evaluate our agent in an environment that mimics a realistic industrial scenario with delayed and long-horizon settings. The empirical results confirm the effectiveness of the proposed approach, demonstrating the coupled world-model with the AIF formalism yields an end-to-end probabilistic controller capable of effective decision making in delayed, long-horizon settings without handcrafted rewards or expensive planning.",
      "keywords": "['Active Inference - Deep Probabilistic Models - Model-based Reinforcement Learning - World Models']",
      "decision": "Reject",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "F20AfNqMq9",
    "query_category": "reject",
    "matches": {
      "poster": {
        "paper": {
          "id": "X9diEuva9R",
          "title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
          "abstract": "Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL  system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77x training speedup compared to synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
          "keywords": [
            "distributed system"
          ],
          "primary_area": "infrastructure",
          "TLDR": "",
          "creation_date": "2025-05-07",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-16",
          "venue": "NeurIPS 2025 poster",
          "forum_link": "https://openreview.net/forum?id=X9diEuva9R",
          "pdf_link": "https://openreview.net/pdf?id=X9diEuva9R"
        },
        "paper_internal_id": "X9diEuva9R",
        "category": "poster",
        "embedding_score": 0.7377714514732361,
        "final_score": 0.10030172765254974
      },
      "spotlight": {
        "paper": {
          "id": "ph1V6n7BSv",
          "title": "EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling",
          "abstract": "World models represent a promising approach for training reinforcement learning agents with significantly improved sample efficiency. While most world model methods primarily rely on sequences of discrete latent variables to model environment dynamics, this compression often neglects critical visual details essential for reinforcement learning. Recent diffusion-based world models condition generation on a fixed context length of frames to predict the next observation, using separate recurrent neural networks to model rewards and termination signals. Although this architecture effectively enhances visual fidelity, the fixed context length approach inherently limits memory capacity.\nIn this paper, we introduce EDELINE, a unified world model architecture that integrates state space models with diffusion models. Our approach outperforms existing baselines across visually challenging Atari 100k tasks, memory-demanding Crafter benchmark, and 3D first-person ViZDoom environments, demonstrating superior performance in all these diverse challenges. Code is available at https://github.com/LJH-coding/EDELINE.",
          "keywords": [
            "Model-based Reinforcement Learning",
            "Atari 100k",
            "Doom",
            "Crafter",
            "MAMBA",
            "Diffusion",
            "World Model"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "EDELINE combines diffusion models with state space models to create a world model for reinforcement learning that overcomes memory limitations in previous approaches.",
          "creation_date": "2025-05-01",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=ph1V6n7BSv",
          "pdf_link": "https://openreview.net/pdf?id=ph1V6n7BSv"
        },
        "paper_internal_id": "ph1V6n7BSv",
        "category": "spotlight",
        "embedding_score": 0.748928964138031,
        "final_score": 0.039774056524038315
      },
      "oral": {
        "paper": {
          "id": "s0JVsx3bx1",
          "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
          "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance.\nOur experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals.\nEvaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\\times$ -- $50\\times$, outperforming other goal-conditioned baselines.\nIncreasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.",
          "keywords": [
            "Reinforcement Learning",
            "Self-Supervised Learning",
            "Contrastive RL",
            "Goal-conditioned RL",
            "Scaling"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "While most RL methods use shallow MLPs (~2–5 layers), we show that scaling up to 1000-layers for contrastive RL (CRL) can significantly boost performance, ranging from doubling performance to 50x on a diverse suite of robotic tasks.",
          "creation_date": "2025-05-10",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-13",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=s0JVsx3bx1",
          "pdf_link": "https://openreview.net/pdf?id=s0JVsx3bx1"
        },
        "paper_internal_id": "s0JVsx3bx1",
        "category": "oral",
        "embedding_score": 0.7422376871109009,
        "final_score": 0.0049441722221672535
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "z5KTxW5sJd",
      "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review",
      "abstract": "The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process.\nIn this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality.\nOur experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.",
      "keywords": "['Large Language Models', 'Peer Review Redesign', 'Comparative Paper Evaluation']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "z5KTxW5sJd",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "obXGSmmG70",
          "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06% on APP, while maintaining high performance on complex tasks. This substantial token decrease directly translates to a significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering a practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications.",
          "keywords": [
            "Adaptive Reasoning",
            "Chain-of-Thought",
            "Large Language Models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "LLMs using Chain-of-Thought (CoT) for everything is wasteful. We built AdaCoT, a smart system that teaches LLMs when to use CoT based on clear principles, saving compute and improving user experience without sacrificing performance on hard tasks.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=obXGSmmG70",
          "pdf_link": "https://openreview.net/pdf?id=obXGSmmG70"
        },
        "paper_internal_id": "obXGSmmG70",
        "category": "reject",
        "embedding_score": 0.6891937255859375,
        "final_score": 0.5875777006149292
      },
      "spotlight": {
        "paper": {
          "id": "3k70Vt0YFS",
          "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code",
          "abstract": "Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement genuinely novel ideas from recent research papers—ideas unseen during pretraining—remains unclear. We introduce ResearchCodeBench, a benchmark that evaluates LLMs’ ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.",
          "keywords": [
            "Machine learning benchmarks",
            "Code generation",
            "Large language models",
            "Research automation"
          ],
          "primary_area": "datasets_&_benchmarks_for_language",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=3k70Vt0YFS",
          "pdf_link": "https://openreview.net/pdf?id=3k70Vt0YFS"
        },
        "paper_internal_id": "3k70Vt0YFS",
        "category": "spotlight",
        "embedding_score": 0.7616376280784607,
        "final_score": 0.8941307067871094
      },
      "oral": {
        "paper": {
          "id": "jMhRbV47pS",
          "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
          "abstract": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
          "keywords": [
            "emergence",
            "sparse attention",
            "in-context learning",
            "induction head"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We show that learning sparse attention is prone to emerging behaviors during training, and study (theoretically and empirically) how data and model design influence emergence speed.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=jMhRbV47pS",
          "pdf_link": "https://openreview.net/pdf?id=jMhRbV47pS"
        },
        "paper_internal_id": "jMhRbV47pS",
        "category": "oral",
        "embedding_score": 0.6583269238471985,
        "final_score": 0.7039464712142944
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "24wDPGiDzA",
      "title": "Unified Scaling Laws for Compressed Representations",
      "abstract": "Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression strategies, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include proposing and validating a general scaling law formulation applicable both individually but also composably across compression types. We demonstrate both theoretically and empirically that a simple metric based on Gaussian mean squared error fitting can robustly predict parameter efficiency across compressed models. Additionally, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats. Finally, we identify conditions under which these unified scaling laws fail.",
      "keywords": "['scaling laws', 'large language models', 'model compression', 'quantization', 'sparsity']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "24wDPGiDzA",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "LPUr2CexmX",
          "title": "DO-EM: Density Operator Expectation Maximization",
          "abstract": "Density operators, quantum generalizations of probability distributions, are gaining prominence in machine learning due to their foundational role in quantum computing. Generative modeling based on density operator models (**DOMs**) is an emerging field, but existing training algorithms - such as those for the Quantum Boltzmann Machine - do not scale to real-world data, such as the MNIST dataset. The Expectation-Maximization algorithm has played a fundamental role in enabling scalable training of probabilistic latent variable models on real-world datasets. *In this paper, we develop an Expectation-Maximization framework to learn latent variable models defined through **DOMs** on classical hardware, with resources comparable to those used for probabilistic models, while scaling to real-world data.* However, designing such an algorithm is nontrivial due to the absence of a well-defined quantum analogue to conditional probability, which complicates the Expectation step. To overcome this, we reformulate the Expectation step as a quantum information projection (QIP) problem and show that the Petz Recovery Map provides a solution under sufficient conditions. Using this formulation, we introduce the Density Operator Expectation Maximization (DO-EM) algorithm - an iterative Minorant-Maximization procedure that optimizes a quantum evidence lower bound. We show that the **DO-EM** algorithm ensures non-decreasing log-likelihood across iterations for a broad class of models. Finally, we present Quantum Interleaved Deep Boltzmann Machines (**QiDBMs**), a **DOM** that can be trained with the same resources as a DBM. When trained with **DO-EM** under Contrastive Divergence, a **QiDBM** outperforms larger classical DBMs in image generation on the MNIST dataset, achieving a 40–60% reduction in the Fréchet Inception Distance.",
          "keywords": [
            "Density Operators",
            "Expectation-Maximization",
            "Quantum Unsupervised Learning",
            "Latent Variable Models"
          ],
          "primary_area": "probabilistic_methods",
          "TLDR": "Training density operator latent variable models using DO-EM outperforms corresponding classical latent variable models.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=LPUr2CexmX",
          "pdf_link": "https://openreview.net/pdf?id=LPUr2CexmX"
        },
        "paper_internal_id": "LPUr2CexmX",
        "category": "reject",
        "embedding_score": 0.6874068975448608,
        "final_score": 0.4837077558040619
      },
      "spotlight": {
        "paper": {
          "id": "dpllevHMbc",
          "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules",
          "abstract": "Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs).  However, existing studies predominantly focus on the final-step loss, leaving open whether the entire $\\textit{loss dynamics}$ obey similar laws and, crucially, how the $\\textit{learning rate schedule}$ (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel $\\textbf{intrinsic-time}$ viewpoint, which captures the training progress more faithfully than iteration count. We then establish a $\\textbf{Functional Scaling Law (FSL)}$ that captures the full loss trajectory under arbitrary LRSs, with the schedule’s influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training.",
          "keywords": [
            "Scaling Laws",
            "Learning Rate Schedule",
            "Kernel Regression",
            "LLM Pre-Training",
            "Loss Dynamics"
          ],
          "primary_area": "theory",
          "TLDR": "We introduce a Functional Scaling Law that predicts full SGD loss dynamics under arbitrary learning rate schedules.",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=dpllevHMbc",
          "pdf_link": "https://openreview.net/pdf?id=dpllevHMbc"
        },
        "paper_internal_id": "dpllevHMbc",
        "category": "spotlight",
        "embedding_score": 0.7686235904693604,
        "final_score": 0.5256152153015137
      },
      "oral": {
        "paper": {
          "id": "WCRPgBpbcA",
          "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
          "abstract": "In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number $N$ of tokens is large and the inverse temperature parameter $\\beta$ of the model scales together with $N$. In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations.",
          "keywords": [
            "mean-field limits",
            "moderate interaction",
            "mean-field transformers",
            "self-attention models",
            "clustering",
            "multiscale"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-01",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=WCRPgBpbcA",
          "pdf_link": "https://openreview.net/pdf?id=WCRPgBpbcA"
        },
        "paper_internal_id": "WCRPgBpbcA",
        "category": "oral",
        "embedding_score": 0.6989258527755737,
        "final_score": 0.25024545192718506
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "6VoDizmIoY",
      "title": "H3D-DGS: Exploring Heterogeneous 3D Motion Representation for Deformable 3D Gaussian Splatting",
      "abstract": "Dynamic scene reconstruction poses a persistent challenge in 3D vision. Deformable 3D Gaussian Splatting has emerged as an effective method for this task, offering real-time rendering and high visual fidelity.\nThis approach decomposes a dynamic scene into a static representation in a canonical space and time-varying scene motion.\nScene motion is defined as the collective movement of all Gaussian points, and for compactness, existing approaches commonly adopt implicit neural fields or sparse control points. \nHowever, these methods predominantly rely on gradient-based optimization for all motion information. Due to the high degree of freedom, they struggle to converge on real-world datasets exhibiting complex motion.\nTo preserve the compactness of motion representation and address convergence challenges, this paper proposes heterogeneous 3D control points, termed \\textbf{H3D control points}, whose attributes are obtained using a hybrid strategy combining optical flow back-projection and gradient-based methods. \nThis design decouples directly observable motion components from those that are geometrically occluded.\nSpecifically, components of 3D motion that project onto the image plane are directly acquired via optical flow back projection, while unobservable portions are refined through gradient-based optimization.\nExperiments on the Neu3DV and CMU-Panoptic datasets demonstrate that our method achieves superior performance over state-of-the-art deformable 3D Gaussian splatting techniques. Remarkably, our method converges within just 100 iterations and achieves a per-frame processing speed of 2 seconds on a single NVIDIA RTX 4070 GPU.",
      "keywords": "['3DGS', '3D motion representation', 'dynamic scene reconstruction', 'streaming reconstruction']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "6VoDizmIoY",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "FQbkBcpcvA",
          "title": "Rethinking cross entropy for continual fine-tuning: policy gradient with entropy annealing",
          "abstract": "While large pretrained vision models have achieved widespread success, their post-training adaptation in continual learning remains vulnerable to catastrophic forgetting. We challenge the conventional use of cross-entropy (CE) loss, a surrogate for 0-1 loss, by reformulating classification through reinforcement learning. Our approach frames classification as a one-step Markov Decision Process (MDP), where input samples serve as states, class labels as actions, and a fully observable reward model is derived from ground-truth labels.  From this formulation, we derive Expected Policy Gradient (EPG), a gradient-based method that directly minimizes the 0-1 loss (i.e., misclassification error). Theoretical and empirical analyses reveal a critical distinction between EPG and CE: while CE encourages exploration via high-entropy outputs, EPG adopts an exploitation-centric approach, prioritizing high-confidence samples through implicit sample weighting. Building on this insight, we propose an adaptive entropy annealing strategy (aEPG) that transitions from exploratory to exploitative learning during continual adaptation of a pre-trained model. Our method outperforms CE-based optimization across diverse benchmarks (Split-ImageNet-R, Split-Food101, Split-CUB100, CLRS) and parameter-efficient modules (LoRA, Adapter, Prefix). More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models. These findings suggest that excessive exploration may disrupt pretrained knowledge and establish exploitative learning as a crucial principle for adapting foundation vision models to evolving classification tasks.",
          "keywords": [
            "Continual learning",
            "reinforcement learning",
            "cross-entropy",
            "class-incremental learning"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "We propose Expected Policy Gradient (EPG), a reinforcement learning method that directly optimizes classification accuracy (0-1 loss)  outperforming cross-entropy for continual fine-tuning pretrained vision models.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=FQbkBcpcvA",
          "pdf_link": "https://openreview.net/pdf?id=FQbkBcpcvA"
        },
        "paper_internal_id": "FQbkBcpcvA",
        "category": "reject",
        "embedding_score": 0.6445224285125732,
        "final_score": 0.05693397298455238
      },
      "spotlight": {
        "paper": {
          "id": "shFhW4zqd6",
          "title": "EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting",
          "abstract": "Scene reconstruction from casually captured videos has wide real-world applications. Despite recent progress, existing methods relying on traditional cameras tend to fail in high-speed scenarios due to insufficient observations and inaccurate pose estimation. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution and low latency, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event cameras to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, enabling continuous supervision between discrete frames. Second, we extract motion information through Contrast Maximization (CMax) of warped events, which calibrates camera poses and provides gradient-domain constraints for 3DGS. Third, to address the absence of color information in events, we combine photometric bundle adjustment (PBA) with a Fixed-GS training strategy that separates structure and color optimization, effectively ensuring color consistency across different views. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our method achieves up to 3dB higher PSNR and 40% lower Absolute Trajectory Error (ATE) compared to state-of-the-art methods under challenging high-speed scenarios.",
          "keywords": [
            "3D Gaussian Splatting (3DGS); Novel View Synthesis (NVS)"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=shFhW4zqd6",
          "pdf_link": "https://openreview.net/pdf?id=shFhW4zqd6"
        },
        "paper_internal_id": "shFhW4zqd6",
        "category": "spotlight",
        "embedding_score": 0.7609915733337402,
        "final_score": 0.615664005279541
      },
      "oral": {
        "paper": {
          "id": "zIzZxDsNNP",
          "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing",
          "abstract": "Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense.",
          "keywords": [
            "Physics sensing",
            "sensor placement",
            "flow models"
          ],
          "primary_area": "deep_learning",
          "TLDR": "We propose a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements with theoretical guarantees, both aiming for accurate physics sensing.",
          "creation_date": "2025-04-15",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-15",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=zIzZxDsNNP",
          "pdf_link": "https://openreview.net/pdf?id=zIzZxDsNNP"
        },
        "paper_internal_id": "zIzZxDsNNP",
        "category": "oral",
        "embedding_score": 0.67702317237854,
        "final_score": 0.2552531957626343
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "jSeWBdH0Xx",
      "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging",
      "abstract": "Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding $300$ slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40\\% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75\\% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent $512\\times512\\times241$ volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D",
      "keywords": "['3D Medical Imaging', 'Vision-Language Modeling', 'Radiology Report Generation', 'Text-Conditional 3D CT Synthesis', 'Volumetric Tokenization']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "jSeWBdH0Xx",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "eR8raBLZW7",
          "title": "BriLLM: Brain-inspired Large Language Model",
          "abstract": "This paper reports the brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of \"least resistance\" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM versions in Chinese and English, with 4000 tokens, 32-dimensional node size, 32-token sequence prediction ability, model sizes around 2B and 1B respectively, bringing language model prediction performance comparable to GPT-1.",
          "keywords": [
            "LLM"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=eR8raBLZW7",
          "pdf_link": "https://openreview.net/pdf?id=eR8raBLZW7"
        },
        "paper_internal_id": "eR8raBLZW7",
        "category": "reject",
        "embedding_score": 0.7279635667800903,
        "final_score": 0.19023869931697845
      },
      "spotlight": {
        "paper": {
          "id": "xcw6UWNtFJ",
          "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
          "abstract": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.\nThe benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.\nEven the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
          "keywords": [
            "Medical VQA Benchmark",
            "Medical Reasoning Benchmark",
            "Structured Clinical Information",
            "Chest X-ray Understanding",
            "Structured Diagnostic Reasoning",
            "Large Vision-Language Model"
          ],
          "primary_area": "AL/ML_data_and_benchmarks_for_health_sciences",
          "TLDR": "We present CheXStruct and CXReasonBench: CheXStruct, an automated pipeline for extracting intermediate reasoning steps directly from chest X-rays, and CXReasonBench, a benchmark for evaluating whether models follow structured diagnostic reasoning.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=xcw6UWNtFJ",
          "pdf_link": "https://openreview.net/pdf?id=xcw6UWNtFJ"
        },
        "paper_internal_id": "xcw6UWNtFJ",
        "category": "spotlight",
        "embedding_score": 0.7614060640335083,
        "final_score": 0.9465216994285583
      },
      "oral": {
        "paper": {
          "id": "DwFDfrPsm8",
          "title": "NOVA: A Benchmark for Rare Anomaly Localization and Clinical Reasoning in Brain MRI",
          "abstract": "In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Open-world recognition ensures that such systems remain robust as ever-emerging, previously _unknown_ categories appear and must be addressed without retraining.\nFoundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging.\nHowever, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.\n\nWe therefore present NOVA, a challenging, real-life _evaluation-only_ benchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. \nBecause NOVA is never used for training, it serves as an _extreme_ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space.  \nBaseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops, with approximately a 65\\% gap in localisation compared to natural-image benchmarks and 40\\% and 20\\% gaps in captioning and reasoning, respectively, compared to resident radiologists. Therefore, NOVA establishes a testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.",
          "keywords": [
            "Vision-Language Models",
            "Zero-shot Learning",
            "Anomaly Detection",
            "Dataset Benchmarking",
            "Medical Imaging",
            "Brain MRI",
            "Multi-modal Data",
            "Rare Diseases"
          ],
          "primary_area": "AL/ML_data_and_benchmarks_for_health_sciences",
          "TLDR": "NOVA is an extreme OOD stress-test dataset of ∼900 multi-modal brain MRI scans (with 281 rare pathologies) for benchmarking VLMs on three clinical tasks: anomaly localization, captioning, and diagnostic reasoning.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track oral",
          "forum_link": "https://openreview.net/forum?id=DwFDfrPsm8",
          "pdf_link": "https://openreview.net/pdf?id=DwFDfrPsm8"
        },
        "paper_internal_id": "DwFDfrPsm8",
        "category": "oral",
        "embedding_score": 0.7637665271759033,
        "final_score": 0.7076715230941772
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "IoSLbwZkal",
      "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective",
      "abstract": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how small perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.",
      "keywords": "['stability', 'graph convolutional neural networks', 'graph signal processing']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "IoSLbwZkal",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "AhyjbXSmUN",
          "title": "Probabilistic Temporal Sampling for Anomaly Detection in Ethereum Networks",
          "abstract": "The rapid growth of the Ethereum network necessitates advanced anomaly detection techniques to enhance security, transparency, and resilience against evolving malicious activities. While there have been significant strides in anomaly detection, they often fall short in capturing the intricate spatial-temporal patterns inherent in blockchain transactional data. This study presents a scalable framework that integrates Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) specifically designed to adapt to the complexities and temporal dynamics of the Ethereum transaction network. Unlike traditional methods that focus on detecting specific attack types, such as front-running or flash loan exploits, our approach targets time-sensitive anomalies more broadly—detecting irregularities such as rapid transaction bursts, anomalous token swaps, and sudden volume spikes. This broader focus reduces reliance on pre-defined attack categories, making the method more adaptable to emerging and evolving malicious strategies. To ground our contributions, we establish three theoretical results: (1) the effectiveness of TRW in enhancing GCN-based anomaly detection by capturing temporal dependencies, (2) the identification of weight cancellation conditions in the anomaly detection process, and (3) the scalability and efficiency improvements of GCNs achieved through probabilistic sampling. Empirical evaluations demonstrate that the TRW-GCN framework outperforms state-of-the-art Temporal Graph Attention Networks (TGAT) in detecting time-sensitive anomalies. Furthermore, as part of our ablation study, we evaluated various anomaly detection techniques on the TRW-GCN embeddings and found that our proposed scoring classifier consistently achieves higher accuracy and precision compared to baseline methods such as Isolation Forest, One-Class SVM, and DBSCAN, thereby validating the robustness and adaptability of our framework.",
          "keywords": [
            "Probabilistic sampling",
            "Temporal random walk",
            "Graph convolutional networks",
            "Transaction anomaly detection",
            "Ethereum networks"
          ],
          "primary_area": "machine_learning_for_sciences",
          "TLDR": "TRW-GCN for Anomaly Detection in Ethereum Networks",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=AhyjbXSmUN",
          "pdf_link": "https://openreview.net/pdf?id=AhyjbXSmUN"
        },
        "paper_internal_id": "AhyjbXSmUN",
        "category": "reject",
        "embedding_score": 0.7501593828201294,
        "final_score": 0.4831499457359314
      },
      "spotlight": {
        "paper": {
          "id": "DwXX8c7xst",
          "title": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs",
          "abstract": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property $T$ that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed.  \nExamples of target properties $T$ include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions.  \nThe LLM reveals an \\emph{observable signal} $Z$ that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits.\nYet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off.\nWe fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit.\nTreating the mutual information $I(Z;T)$ between the observation $Z$ and the target property $T$ as the leaked bits per query, we show that achieving error $\\varepsilon$ requires at least $\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy.\nThus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy.\nExperiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen.\nOur results provide the first principled yardstick for balancing transparency and security when deploying LLMs.",
          "keywords": [
            "Large Language Models",
            "Jailbreak attack",
            "Security"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-15",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=DwXX8c7xst",
          "pdf_link": "https://openreview.net/pdf?id=DwXX8c7xst"
        },
        "paper_internal_id": "DwXX8c7xst",
        "category": "spotlight",
        "embedding_score": 0.694212794303894,
        "final_score": 0.8232635855674744
      },
      "oral": {
        "paper": {
          "id": "eafIjoZAHm",
          "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
          "abstract": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse $k$-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
          "keywords": [
            "graph neural network",
            "graph machine learning",
            "explainability",
            "xai",
            "global explanation",
            "text-based explanation",
            "exemplar",
            "exemplar theory"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We generate global text-based explanations using representative nodes (exemplars) in the embedding space. The exemplars are selected via coverage maximization, and their signatures are explained using natural language rules from a self-refining LLM.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=eafIjoZAHm",
          "pdf_link": "https://openreview.net/pdf?id=eafIjoZAHm"
        },
        "paper_internal_id": "eafIjoZAHm",
        "category": "oral",
        "embedding_score": 0.7056791186332703,
        "final_score": 0.45268651843070984
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "yrrU5YChQr",
      "title": "Vulnerable Data-Aware Adversarial Training",
      "abstract": "Fast adversarial training (FAT) has been considered as one of the most effective alternatives to the computationally-intensive adversarial training. Generally, FAT methods pay equal attention to each sample of the target task. However, the distance between each sample and the decision boundary is different, learning samples which are far from the decision boundary (i.e., less important to adversarial robustness) brings additional training cost and leads to sub-optimal results. To tackle this issue, we present vulnerable data-aware adversarial training (VDAT) in this study. Specifically, we first propose a margin-based vulnerability calculation method to measure the vulnerability of data samples. Moreover, we propose a vulnerability-aware data filtering method to reduce the training data for adversarial training thus improve the training efficiency. The experiments are conducted in terms of adversarial training and robust neural architecture search on CIFAR-10, CIFAR-100, and ImageNet-1K. The results demonstrate that VDAT is up to 76% more efficient than state-of-the-art FAT methods, while achieving improvements regarding the natural accuracy and adversarial accuracy in both scenarios. Furthermore, the visualizations and ablation studies show the effectiveness of both core components designed in VDAT.",
      "keywords": "['Adversarial Training', 'Adversarial Robustness', 'Decision Boundary Analysis']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "yrrU5YChQr",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "QJtanJS4T9",
          "title": "Irreducible Loss Floors in Gradient Descent Convergence and Energy Footprint",
          "abstract": "Despite their central role, convergence analyses of the dynamics of loss functions\nduring training require strong assumptions (e.g convexity and smoothness) which\nare non-trivial to prove. In this work, we introduce a framework for deriving\nnecessary convergence conditions that hold without restrictive assumptions on\nthe dataset or the model architecture. By linking microscopic properties such as\nindividual sample losses and their gradient to macroscopic training dynamics, we\nderive tight lower bounds for loss functions, applicable to both full-batch and mini-\nbatch gradient systems. These bounds reveal the presence of irreducible floors\nthat optimizers cannot surpass and beyond theoretical guarantees, this framework offers a practical tool for anticipating convergence speed, and estimating\nminimum training time and energy requirements. Thus, this framework can be\nused to ensure the sustainability and feasibility of large-scale training regimes.",
          "keywords": [
            "gradient descent",
            "convergence",
            "loss bounds",
            "optimization",
            "training dynamics",
            "sustainability",
            "efficiency",
            "feasibility",
            "computational cost",
            "irreducible loss",
            "non-convex optimization",
            "lower bounds"
          ],
          "primary_area": "theory",
          "TLDR": "We derive theoretical lower bounds for loss functions, revealing convergence limits without relying on standard convexity assumptions.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=QJtanJS4T9",
          "pdf_link": "https://openreview.net/pdf?id=QJtanJS4T9"
        },
        "paper_internal_id": "QJtanJS4T9",
        "category": "reject",
        "embedding_score": 0.66568922996521,
        "final_score": 0.9717124700546265
      },
      "spotlight": {
        "paper": {
          "id": "a7hHwWnZey",
          "title": "Fast Training of Large Kernel Models with Delayed Projections",
          "abstract": "Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes—a key ingredient that has driven the success of neural networks. In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size. Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible. We validate our algorithm, \\EP4, across multiple datasets, demonstrating drastic training speedups without compromising the performance. Our implementation is publicly available at: https://github.com/EigenPro/EigenPro\n.",
          "keywords": [
            "Kernel machines",
            "large-scale kernel machines",
            "Preconditioned-SGD",
            "Nyström approximation"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "We introduce a new SGD-based algorithm with delayed projection for training kernel machines that achieves comparable or superior performance while reducing training time from days to under an hour.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=a7hHwWnZey",
          "pdf_link": "https://openreview.net/pdf?id=a7hHwWnZey"
        },
        "paper_internal_id": "a7hHwWnZey",
        "category": "spotlight",
        "embedding_score": 0.677053689956665,
        "final_score": 0.8910889625549316
      },
      "oral": {
        "paper": {
          "id": "7AwFJzgIUW",
          "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
          "abstract": "Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines.",
          "keywords": [
            "Low Rank",
            "Adversarial Robustenss",
            "Adversarial Attacks",
            "Rank Adaptive",
            "Computer Vision",
            "Compression"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=7AwFJzgIUW",
          "pdf_link": "https://openreview.net/pdf?id=7AwFJzgIUW"
        },
        "paper_internal_id": "7AwFJzgIUW",
        "category": "oral",
        "embedding_score": 0.7041610479354858,
        "final_score": 0.9270361065864563
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "I4fBSpDOha",
      "title": "Focus-Then-Reuse: Fast Adaptation in Visual Perturbation Environments",
      "abstract": "Visual reinforcement learning has shown promise in various real-world applications. However, deploying policies in complex real-world environments with visual perturbations remains a significant challenge. We notice that humans tend to filter information at the object level prior to decision-making, facilitating efficient skill transfer across different contexts. Inspired by this, we introduce Focus-Then-Reuse (FTR), a method utilizing a novel object selection mechanism to focus on task-relevant objects, and directly reuse the simulation-trained policy on them. The training of the object selection mechanism integrates prior knowledge from a vision-language model and feedback from the environment. Experimental results on challenging tasks based on DeepMind Control Suite and Franka Emika Robotics demonstrate that FTR enables rapid adaptation in visual perturbation environments and achieves state-of-the-art performance. The source code is available at https://github.com/LAMDA-RL/FTR.",
      "keywords": "['reinforcement learning; visual domain adaptation']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "I4fBSpDOha",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "cEd00CXWE5",
          "title": "Beyond Two-Stage Training: Integrating SFT and RL for Improved Reasoning in LLMs",
          "abstract": "Reinforcement learning (RL) has proven effective in incentiving the reasoning abilities of large language models (LLMs), but faces significant efficiency challenges due to its extensive trial-and-error nature. A common practice is to employ supervised fine-tuning (SFT) as a warm-up stage; however, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. Specifically, the SFT objective is explicitly conditioned on the optimal solution of the RL objective. During training, lower-level updates enable the model to receive SFT supervision concurrently with RL-based exploration, while upper-level updates are optimized to ensure that the joint training yields higher rewards than RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.",
          "keywords": [
            "LLM",
            "Reasoning",
            "RL",
            "SFT"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-11-26",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=cEd00CXWE5",
          "pdf_link": "https://openreview.net/pdf?id=cEd00CXWE5"
        },
        "paper_internal_id": "cEd00CXWE5",
        "category": "reject",
        "embedding_score": 0.6623552441596985,
        "final_score": 0.2654396593570709
      },
      "spotlight": {
        "paper": {
          "id": "neZSGqhxDa",
          "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
          "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
          "keywords": [
            "reasoning",
            "language model",
            "reinforcement learning",
            "self-play",
            "LLM"
          ],
          "primary_area": "applications",
          "TLDR": "self-play reasoning RL with no data can achieve SOTA against RL models trained with human data",
          "creation_date": "2025-05-03",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=neZSGqhxDa",
          "pdf_link": "https://openreview.net/pdf?id=neZSGqhxDa"
        },
        "paper_internal_id": "neZSGqhxDa",
        "category": "spotlight",
        "embedding_score": 0.6612827777862549,
        "final_score": 0.6187079548835754
      },
      "oral": {
        "paper": {
          "id": "RxkCwOKVKa",
          "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
          "abstract": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available.",
          "keywords": [
            "reinforcement learning",
            "inference strategies",
            "complex decision-making"
          ],
          "primary_area": "reinforcement_learning",
          "TLDR": "Using search strategies at inference-time can provide massive performance boost on numerous complex reinforcement learning tasks, within only a couple seconds of execution time.",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=RxkCwOKVKa",
          "pdf_link": "https://openreview.net/pdf?id=RxkCwOKVKa"
        },
        "paper_internal_id": "RxkCwOKVKa",
        "category": "oral",
        "embedding_score": 0.6863281726837158,
        "final_score": 0.6517844796180725
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "IL1wvzOgqD",
      "title": "Towards A Translative Model of Sperm Whale Vocalization",
      "abstract": "Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using Fréchet Audio Distance and through perceptual studies with expert marine biologists. On downstream tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham",
      "keywords": "['Sperm Whale Communication', 'Bioacoustics', 'Masked Acoustic Token Modeling', 'Generative Audio Models', 'Representation Learning']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "IL1wvzOgqD",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "b6s1jIHj6o",
          "title": "Hear you are: Teaching LLMs Spatial Reasoning with Vision and Spatial Sound",
          "abstract": "Many audio-visual learning methods have focused on aligning audio and visual information, either through semantic or temporal correspondence. However, most of these works have utilized monaural audio, which does not contain information about the spatial location of the sound source. In contrast, humans and other animals utilize binaural hearing to perceive this spatial information. Combining spatial sound and visual perception enables powerful high-level reasoning: for example, a person looking for their phone may hear the ringing sound coming from a backpack sitting on a table, and quickly infer that the missing phone is inside the backpack. In this paper, we investigate the problem of Audio-Visual Spatial Reasoning. We design a spatial audio-visual question answering dataset to cover scenarios where semantic correspondence between audio and visual signals is absent but spatial alignment exists, as well as cases with multiple audio-visual semantic correspondences that require spatial reasoning to disambiguate. We propose a model that learns spatial comprehension across the audio and vision modalities by connecting them with a large language model and experimentally demonstrate that spatial sound perception is an essential part of our task.",
          "keywords": [
            "Audio-visual Spatial Reasoning",
            "Spatial audio",
            "Multi-modal LLMs"
          ],
          "primary_area": "applications",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=b6s1jIHj6o",
          "pdf_link": "https://openreview.net/pdf?id=b6s1jIHj6o"
        },
        "paper_internal_id": "b6s1jIHj6o",
        "category": "reject",
        "embedding_score": 0.6454653739929199,
        "final_score": 0.35545533895492554
      },
      "spotlight": {
        "paper": {
          "id": "Xeb2EYBKkr",
          "title": "Two Heads are Better than One: Simulating Large Transformers with Small Ones",
          "abstract": "The quadratic complexity of self‑attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences?\n\nIn this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.",
          "keywords": [
            "Transformers",
            "theory",
            "representational strength"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-11",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=Xeb2EYBKkr",
          "pdf_link": "https://openreview.net/pdf?id=Xeb2EYBKkr"
        },
        "paper_internal_id": "Xeb2EYBKkr",
        "category": "spotlight",
        "embedding_score": 0.6225900650024414,
        "final_score": 0.505338728427887
      },
      "oral": {
        "paper": {
          "id": "jzPQRbGkAq",
          "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
          "abstract": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at\nhttps://github.com/asdryau/TransPhase.",
          "keywords": [
            "Motion Generation",
            "Phase Autoencoder",
            "Long Term Motion Sequence Generation",
            "Motion Inbetweening"
          ],
          "primary_area": "deep_learning",
          "TLDR": "The proposed Compositional Phase Diffusion framework consistently generates semantically aligned multi-clip motion with smooth transitions by using latent-phase diffusion modules (SPDM and TPDM) to preserve phase continuity and enable inbetweening.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=jzPQRbGkAq",
          "pdf_link": "https://openreview.net/pdf?id=jzPQRbGkAq"
        },
        "paper_internal_id": "jzPQRbGkAq",
        "category": "oral",
        "embedding_score": 0.6127973794937134,
        "final_score": 0.3720773756504059
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "uUWb5eawL9",
      "title": "Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models",
      "abstract": "Capability evaluations play a crucial role in assessing and regulating frontier AI systems. The effectiveness of these evaluations faces a significant challenge: strategic underperformance, or ``sandbagging'', where models deliberately underperform during evaluation. \nSandbagging can manifest either through explicit developer intervention or through unintended model behavior, presenting a fundamental obstacle to accurate capability assessment. We introduce a novel sandbagging detection method based on injecting noise of varying magnitudes into model weights. While non-sandbagging models show predictable performance degradation with increasing noise, we demonstrate that sandbagging models exhibit anomalous performance improvements, likely due to disruption of underperformance mechanisms while core capabilities remain partially intact. Through experiments across various model architectures, sizes, and sandbagging techniques, we establish this distinctive response pattern as a reliable, model-agnostic signal for detecting sandbagging behavior. Importantly, we find noise-injection is capable of eliciting the full performance of Mistral Large 120B in a setting where the model underperforms without being instructed to do so. Our findings provide a practical tool for AI evaluation and oversight, addressing a challenge in ensuring accurate capability assessment of frontier AI systems.",
      "keywords": "['Large Language Models', 'Sandbagging', 'Noise Injection', 'Deception Detection']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "uUWb5eawL9",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "QJtanJS4T9",
          "title": "Irreducible Loss Floors in Gradient Descent Convergence and Energy Footprint",
          "abstract": "Despite their central role, convergence analyses of the dynamics of loss functions\nduring training require strong assumptions (e.g convexity and smoothness) which\nare non-trivial to prove. In this work, we introduce a framework for deriving\nnecessary convergence conditions that hold without restrictive assumptions on\nthe dataset or the model architecture. By linking microscopic properties such as\nindividual sample losses and their gradient to macroscopic training dynamics, we\nderive tight lower bounds for loss functions, applicable to both full-batch and mini-\nbatch gradient systems. These bounds reveal the presence of irreducible floors\nthat optimizers cannot surpass and beyond theoretical guarantees, this framework offers a practical tool for anticipating convergence speed, and estimating\nminimum training time and energy requirements. Thus, this framework can be\nused to ensure the sustainability and feasibility of large-scale training regimes.",
          "keywords": [
            "gradient descent",
            "convergence",
            "loss bounds",
            "optimization",
            "training dynamics",
            "sustainability",
            "efficiency",
            "feasibility",
            "computational cost",
            "irreducible loss",
            "non-convex optimization",
            "lower bounds"
          ],
          "primary_area": "theory",
          "TLDR": "We derive theoretical lower bounds for loss functions, revealing convergence limits without relying on standard convexity assumptions.",
          "creation_date": "2025-05-11",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=QJtanJS4T9",
          "pdf_link": "https://openreview.net/pdf?id=QJtanJS4T9"
        },
        "paper_internal_id": "QJtanJS4T9",
        "category": "reject",
        "embedding_score": 0.6449716091156006,
        "final_score": 0.42471152544021606
      },
      "spotlight": {
        "paper": {
          "id": "XBMjXb6f4w",
          "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D",
          "abstract": "AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI’s Inspect framework and we make our code publicly available.",
          "keywords": [
            "Sabotage Evaluations",
            "Sandbagging",
            "AI Control",
            "AI Safety",
            "AI Alignment",
            "Dangerous Capability Evals"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "We evaluate frontier LM agents' capabilities to sabotage and sandbag ML engineering tasks without being detected by automated monitors.",
          "creation_date": "2025-05-12",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 spotlight",
          "forum_link": "https://openreview.net/forum?id=XBMjXb6f4w",
          "pdf_link": "https://openreview.net/pdf?id=XBMjXb6f4w"
        },
        "paper_internal_id": "XBMjXb6f4w",
        "category": "spotlight",
        "embedding_score": 0.8203323483467102,
        "final_score": 0.7449315786361694
      },
      "oral": {
        "paper": {
          "id": "7AwFJzgIUW",
          "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
          "abstract": "Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines.",
          "keywords": [
            "Low Rank",
            "Adversarial Robustenss",
            "Adversarial Attacks",
            "Rank Adaptive",
            "Computer Vision",
            "Compression"
          ],
          "primary_area": "deep_learning",
          "TLDR": "",
          "creation_date": "2025-05-08",
          "original_date": "2025-10-29",
          "modification_date": "2025-10-29",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=7AwFJzgIUW",
          "pdf_link": "https://openreview.net/pdf?id=7AwFJzgIUW"
        },
        "paper_internal_id": "7AwFJzgIUW",
        "category": "oral",
        "embedding_score": 0.666154146194458,
        "final_score": 0.41059672832489014
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "7ieS4EYKnB",
      "title": "Flick: Empowering Federated Learning with Commonsense Knowledge",
      "abstract": "Federated Learning (FL) has emerged as a privacy-preserving framework for training models on data generated at the edge. However, the heterogeneity of data silos (e.g., label skew and domain shift) often leads to inconsistent learning objectives and suboptimal model performance. Inspired by the data-driven approach, we propose Flick, a novel data generation framework for heterogeneous **F**ederated **L**earning w**i**th **C**ommonsense **K**nowledge from Large Language Models (LLMs). In Flick, the client performs the local data summary to capture client-specific knowledge in textual form. The central server then distills task-relevant, high-quality knowledge from the out-of-the-box LLM -- guided by cross-client-specific insights -- to generate informative text prompts. These prompts direct a generative model in producing synthetic data, enabling global model fine-tuning and local data compensation. This process gradually aligns the label and feature distributions across clients. Extensive results on three datasets demonstrate that Flick improves the global model accuracy by up to 11.43\\%, and accelerates convergence by up to 12.9$\\times$, validating its effectiveness in addressing data heterogeneity.",
      "keywords": "['Federated Learning', 'Data Heterogeneity']",
      "decision": "Accept (Poster)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "7ieS4EYKnB",
    "query_category": "poster",
    "matches": {
      "reject": {
        "paper": {
          "id": "InyYuWLWHD",
          "title": "LayerGuard: Poisoning-Resilient Federated Learning via Layer-Wise Similarity Analysis",
          "abstract": "In recent years, model poisoning attacks have gradually evolved from conventional global parameter manipulations to more stealthy and strategic Targeted Layer Poisoning (TLP) attacks.These attacks achieve high attack success rates by selectively poisoning only a subset of layers. However, most existing defenses rely on evaluation of the entire network and are thus ineffective against TLP attacks, posing new challenges to the security of Federated Learning (FL).In this paper, we propose \\textbf{LayerGuard}, a comprehensive defense framework featuring dynamic detection and adaptive aggregation to protect FL against advanced model poisoning attacks. Diverging from traditional methods that analyze the entire network collectively, \\textbf{LayerGuard} performs layer-wise similarity analysis to detect anomalous clients and adaptively identifies layers under attack based on the clustering behavior of malicious updates, facilitating more precise threat detection. Building on this, we introduce a joint weighting mechanism in the aggregation process, which evaluates each client's credibility at the layer level from two complementary informational dimensions: inter-layer and intra-layer, balancing attack mitigation and benign contribution retention. Extensive experiments across various datasets and model architectures demonstrate that \\textbf{LayerGuard} successfully reduces the average attack success rate of TLP attacks to around 5\\%. Moreover, when confronted with other advanced model poisoning attacks, \\textbf{LayerGuard} consistently maintains global model accuracy—even under high poisoning rates and severe non-IID conditions—comparable to that of FedAvg under no-attack settings, marking a significant improvement over existing defenses.",
          "keywords": [
            "Federated Learning; Security; Model Poisoning Attacks; Robust Aggregation"
          ],
          "primary_area": "social_and_economic_aspects_of_machine_learning",
          "TLDR": "A new FL defense achieves state-of-the-art robustness against advanced model poisoning attacks and effectively counters the emerging threat of Targeted Layer Poisoning (TLP) attacks.",
          "creation_date": "2025-05-12",
          "original_date": null,
          "modification_date": "2025-10-29",
          "venue": "Submitted to NeurIPS 2025",
          "forum_link": "https://openreview.net/forum?id=InyYuWLWHD",
          "pdf_link": "https://openreview.net/pdf?id=InyYuWLWHD"
        },
        "paper_internal_id": "InyYuWLWHD",
        "category": "reject",
        "embedding_score": 0.6632055640220642,
        "final_score": 0.6685844659805298
      },
      "spotlight": {
        "paper": {
          "id": "4Ud0pRqFto",
          "title": "MARS-VFL: A Unified Benchmark for Vertical Federated Learning with Realistic Evaluation",
          "abstract": "Vertical Federated Learning (VFL) has emerged as a critical privacy-preserving learning paradigm, enabling collaborative model training by leveraging distributed features across clients. However, due to privacy concerns, there are few publicly available real-world datasets for evaluating VFL methods, which poses significant challenges to related research. To bridge this gap, we propose MARS-VFL, a unified benchmark for realistic VFL evaluation. It integrates data from practical applications involving collaboration across different features, maintaining compatibility with the VFL setting. Based on this, we standardize the evaluation of VFL methods from the mainstream aspects of efficiency, robustness, and security. We conduct comprehensive experiments to assess different VFL approaches, providing references for unified evaluation. Furthermore, we are the first to unify the evaluation of robustness challenges in VFL and introduce a new method for addressing robustness challenges, establishing standard baselines for future research.",
          "keywords": [
            "Vertical Federated Learning",
            "Distrubuted System",
            "Benchmark"
          ],
          "primary_area": "AL/ML_data_processing_and_benchmarking_infrastructure",
          "TLDR": "We introduce a unified benchmark for VFL that evaluates efficiency, robustness, and security based on realistic data distributions.",
          "creation_date": "2025-05-04",
          "original_date": "2025-10-30",
          "modification_date": "2025-10-30",
          "venue": "NeurIPS 2025 Datasets and Benchmarks Track spotlight",
          "forum_link": "https://openreview.net/forum?id=4Ud0pRqFto",
          "pdf_link": "https://openreview.net/pdf?id=4Ud0pRqFto"
        },
        "paper_internal_id": "4Ud0pRqFto",
        "category": "spotlight",
        "embedding_score": 0.7040466070175171,
        "final_score": 0.9846231341362
      },
      "oral": {
        "paper": {
          "id": "aUAG1WS7J2",
          "title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
          "abstract": "Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model’s overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.",
          "keywords": [
            "Federated Learning;Federated Class-Incremental Learning; Data Replay"
          ],
          "primary_area": "general_machine_learning",
          "TLDR": "",
          "creation_date": "2025-05-09",
          "original_date": "2025-10-29",
          "modification_date": "2025-12-11",
          "venue": "NeurIPS 2025 oral",
          "forum_link": "https://openreview.net/forum?id=aUAG1WS7J2",
          "pdf_link": "https://openreview.net/pdf?id=aUAG1WS7J2"
        },
        "paper_internal_id": "aUAG1WS7J2",
        "category": "oral",
        "embedding_score": 0.7353912591934204,
        "final_score": 0.06086915358901024
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  },
  {
    "query": {
      "paper_id": "pISLZG7ktL",
      "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
      "abstract": "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy’s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\\% success rates in novel environments with unseen objects.",
      "keywords": "['Data Scaling Laws', 'Imitation Learning', 'Robotic Manipulation']",
      "decision": "Accept (Oral)",
      "year": "2025",
      "ratings": "[]",
      "confidences": "[]"
    },
    "query_internal_id": "pISLZG7ktL",
    "query_category": "oral",
    "matches": {
      "reject": {
        "paper": {
          "id": "JaRihIHbZm",
          "title": "VideoAgent: Self-Improving Video Generation",
          "abstract": "Video generation has been used to generate visual plans for controlling robotic systems. Given an image observation and a language instruction, previous work has generated video plans which are then converted to robot controls to be executed. However, a major bottleneck in leveraging video generation for control lies in the quality of the generated videos, which often suffer from hallucinatory content and unrealistic physics, resulting in low task success when control actions are extracted from the generated videos. While scaling up dataset and model size provides a partial solution, integrating external feedback is both natural and essential for grounding video generation in the real world. With this observation, we propose VideoAgent for self-improving generated video plans based on external feedback. Instead of directly executing the generated video plan, VideoAgent first refines the generated video plans using a novel procedure which we call self-conditioning consistency, utilizing feedback from a pretrained vision-language model (VLM). As the refined video plan is being executed, VideoAgent collects additional data from the environment to further improve video plan generation. Experiments in simulated robotic manipulation from MetaWorld and iTHOR show that VideoAgent drastically reduces hallucination, thereby boosting success rate of downstream manipulation tasks. We further illustrate that VideoAgent can effectively refine real-robot videos, providing an early indicator that robotics can be an effective tool in grounding video generation in the physical world.",
          "keywords": [
            "sequential decision making",
            "video generation",
            "self improvement"
          ],
          "primary_area": "reinforcement learning",
          "TLDR": "We propose VideoAgent to self improve video generation by refining video plans using external feedback, significantly reducing hallucinations and enhancing task success in simulated robotic manipulation.",
          "creation_date": "2024-09-28",
          "original_date": "2024-10-04",
          "modification_date": "2025-02-05",
          "venue": "Submitted to ICLR 2025",
          "forum_link": "https://openreview.net/forum?id=JaRihIHbZm",
          "pdf_link": "https://openreview.net/pdf?id=JaRihIHbZm"
        },
        "paper_internal_id": "JaRihIHbZm",
        "category": "reject",
        "embedding_score": 0.7291430234909058,
        "final_score": 0.129550039768219
      },
      "poster": {
        "paper": {
          "id": "1v7SRWsYve",
          "title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation",
          "abstract": "Model merging has emerged as an effective approach to combining multiple single-task models into a multitask model. This process typically involves computing a weighted average of the model parameters without additional training. Existing model-merging methods focus on improving average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during the merging process. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel and low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP efficiently identifies a Pareto set of scaling coefficients for merging multiple models, reflecting the trade-offs involved. It amortizes the substantial computational cost of evaluations needed to estimate the Pareto front by using quadratic approximation surrogate models derived from a preselected set of scaling coefficients. Experimental results on vision and natural language processing tasks demonstrate that MAP can accurately identify the Pareto front, providing practitioners with flexible solutions to balance competing task objectives. We also introduce Bayesian MAP for scenarios with a relatively low number of tasks and Nested MAP for situations with a high number of tasks, further reducing the computational cost of evaluation.",
          "keywords": [
            "model merging",
            "transfer learning",
            "multitask learning",
            "task arithmetic",
            "multi-objective optimization"
          ],
          "primary_area": "other topics in machine learning (i.e., none of the above)",
          "TLDR": "We provide a computation-efficient algorithm for finding the Pareto front representing the trade-offs during model merging caused by conflicting objectives between different tasks.",
          "creation_date": "2024-09-26",
          "original_date": "2024-10-04",
          "modification_date": "2025-05-17",
          "venue": "ICLR 2025 Poster",
          "forum_link": "https://openreview.net/forum?id=1v7SRWsYve",
          "pdf_link": "https://openreview.net/pdf?id=1v7SRWsYve"
        },
        "paper_internal_id": "1v7SRWsYve",
        "category": "poster",
        "embedding_score": 0.7237749099731445,
        "final_score": 0.2956140339374542
      },
      "spotlight": {
        "paper": {
          "id": "et5l9qPUhm",
          "title": "Strong Model Collapse",
          "abstract": "Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1 per 1000) can still lead to model collapse: larger and larger training sets do not enhance performance.  We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and neural networks for images.",
          "keywords": [
            "Model Collapse",
            "Regression",
            "High dimensional asymptotics",
            "Synthetic Data",
            "Scaling Laws"
          ],
          "primary_area": "learning theory",
          "TLDR": "We provide an exact characterization of model collapse for mixing original and AI-generated data in the regression setting; even a small (but constant) fraction of synthetic data is detrimental asymptotically. We also study the impact of model size.",
          "creation_date": "2024-09-25",
          "original_date": "2024-10-04",
          "modification_date": "2025-03-03",
          "venue": "ICLR 2025 Spotlight",
          "forum_link": "https://openreview.net/forum?id=et5l9qPUhm",
          "pdf_link": "https://openreview.net/pdf?id=et5l9qPUhm"
        },
        "paper_internal_id": "et5l9qPUhm",
        "category": "spotlight",
        "embedding_score": 0.7377175092697144,
        "final_score": 0.366423100233078
      }
    },
    "missing_categories": [],
    "pool_size": 800,
    "rerank_top_k": 120
  }
]